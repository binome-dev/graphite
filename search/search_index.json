{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Graphite Documentation","text":"<p>Graphite is an open-source framework for creating domain-specific AI assistants via composable, agentic workflows. It emphasizes loose coupling and well-defined interfaces, enabling developers to construct flexible, modular systems. Each major layer \u2013 assistant, node, tool, and workflow \u2013 has a clear role in orchestrating or executing tasks, with events serving as the single source of truth for every state change or data exchange.</p> <p>This documentation details how Graphite\u2019s event-driven architecture seamlessly supports complex business logic, from initial user requests through advanced tool integrations (e.g., LLM calls, function calls, RAG retrieval). Dedicated topics manage pub/sub operations, providing mechanisms for input, output, and human-in-the-loop interactions. Meanwhile, commands encapsulate execution logic for each tool, allowing nodes to delegate work without tight coupling.</p> <p>Four critical capabilities\u2014observability, idempotency, auditability, and restorability\u2014underpin Graphite\u2019s suitability for production AI environments. Observability is achieved via event sourcing and OpenTelemetry-based tracing, idempotency through carefully managed event stores and retry logic, auditability by logging every action and data flow, and restorability by maintaining offset-based consumption records that let workflows resume exactly where they left off.</p> <p>Overall, Graphite offers a powerful, extensible foundation for building AI solutions that scale, adapt to evolving compliance needs, and gracefully handle failures or user-driven pauses. By combining a robust workflow engine, well-structured nodes and tools, and a complete event model, Graphite enables teams to develop sophisticated conversational agents and automated pipelines with confidence.</p>"},{"location":"#what-is-graphite","title":"What is Graphite?","text":"<p>Graphite is an open-source platform that treats data as interconnected nodes and relationships, allowing you to:</p> <ul> <li>Process complex data relationships with graph-based algorithms</li> <li>Visualize data connections through interactive network diagrams</li> <li>Build analytical pipelines that leverage graph structures</li> <li>Scale efficiently with distributed processing capabilities</li> </ul> <p>Whether you're analyzing social networks, tracking data lineage, exploring knowledge graphs, or building recommendation systems, Graphite provides the tools and abstractions you need to work effectively with connected data.</p>"},{"location":"#key-features","title":"Key Features","text":"<p>Graph-Native Processing: Built from the ground up to handle graph data structures efficiently, with optimized algorithms for common graph operations like traversals, clustering, and pathfinding.</p> <p>Visual Analytics: Interactive visualization tools that help you explore and understand complex data relationships through customizable network diagrams and graph layouts.</p> <p>Flexible Data Integration: Connect to various data sources including databases, APIs, and file formats, with built-in support for common graph data formats like GraphML, GEXF, and JSON.</p> <p>Extensible Architecture: Plugin-based system that allows you to extend functionality with custom algorithms, data connectors, and visualization components.</p> <p>Performance Optimized: Efficient memory management and parallel processing capabilities designed to handle large-scale graph datasets.</p>"},{"location":"#who-should-use-graphite","title":"Who Should Use Graphite?","text":"<p>Graphite is designed for data scientists, analysts, researchers, and developers who work with interconnected data, including:</p> <ul> <li>Data Scientists building recommendation engines or fraud detection systems</li> <li>Business Analysts exploring customer journey maps or organizational networks  </li> <li>Researchers analyzing citation networks, protein interactions, or social structures</li> <li>Developers building applications that require graph-based computations</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>This documentation will guide you through:</p> <ol> <li>Installation and Setup - Get Graphite running in your environment</li> <li>Core Concepts - Understand graphs, nodes, edges, and data models</li> <li>Data Import - Load your data from various sources</li> <li>Processing and Analysis - Apply algorithms and transformations</li> <li>Visualization - Create interactive graph visualizations</li> <li>Advanced Topics - Custom plugins, performance tuning, and deployment</li> </ol> <p>Ready to dive in? Start with our Quick Start Guide to get Graphite up and running in minutes, or explore the Core Concepts to understand the fundamentals of graph-based data processing.</p>"},{"location":"#community-and-support","title":"Community and Support","text":"<p>Graphite is actively developed and maintained by the open-source community. Join us:</p> <ul> <li>GitHub: github.com/binome-dev/graphite</li> <li>Issues and Feature Requests: Use GitHub Issues for bug reports and feature requests</li> <li>Discussions: Join community discussions and get help from other users</li> <li>Contributing: Check out our contribution guidelines to help improve Graphite</li> </ul> <p>This documentation covers Graphite v0.0.x.</p>"},{"location":"getting-started/features/","title":"Features","text":"<p>The core design principles that set Graphite apart from other agent frameworks are:</p> <ol> <li> <p>A Simple 3-Layer Execution Model    Three distinct layers\u2014assistant, node, and tool\u2014manage execution, while a dedicated workflow layer oversees orchestration.</p> </li> <li> <p>Pub/Sub Event-Driven Orchestration    Communication relies on publishing and subscribing to events, ensuring a decoupled, modular flow of data throughout the system.</p> </li> <li> <p>Events as the Single Source of Truth    All operational states and transitions are recorded as events, providing a uniform way to track and replay system behavior if needed.</p> </li> </ol> <p>Combining these elements, Graphite provides a production-grade AI application framework capable of operating reliably at scale, handling failures gracefully, and maintaining user and stakeholder trust. Four essential capabilities form the backbone of this approach:</p> <ol> <li> <p>Observability    Complex AI solutions involve multiple steps, data sources, and models. Graphite\u2019s event-driven architecture, logging, and tracing make it possible to pinpoint bottlenecks or errors in real time, ensuring that each component\u2019s behavior is transparent and measurable.</p> </li> <li> <p>Idempotency    Asynchronous workflows often require retries when partial failures occur or network conditions fluctuate. Graphite\u2019s design emphasizes idempotent operations, preventing pub/sub data duplication or corruption when calls must be repeated.</p> </li> <li> <p>Auditability    By treating events as the single source of truth, Graphite automatically logs every state change and decision path. This level of detailed recordkeeping is indispensable for users working in regulated sectors or who need full traceability for debugging and compliance.</p> </li> <li> <p>Restorability    Long-running AI tasks risk substantial rework if they fail mid-execution. In Graphite, checkpoints and event-based playback enable workflows to resume from the precise point of interruption, minimizing downtime and maximizing resource efficiency.</p> </li> </ol> <p>Together, these capabilities\u2014observability, idempotency, auditability, and restorability\u2014distinguish Graphite as a framework for building robust and trustworthy AI applications. Below is a detailed breakdown of how Graphite implements each feature.</p>"},{"location":"getting-started/features/#observability","title":"Observability","text":"<p>The system leverages event sourcing to record all operations, combined with OpenTelemetry for standardized tracing. Thanks to the clearly defined three-layer execution model (assistant, node, tool) plus an orchestration workflow, each execution function is decorated to capture inputs, outputs, and any exceptions. These captures are converted into events (stored in the event store) and traces (exported to platforms like Arize or other OpenTelemetry services).</p> <p>Meanwhile, each Topic instance logs pub/sub interactions in the event store after processing. Coupled with the ExecutionContext object, this approach makes the entire data flow within the agentic workflow fully transparent. Because every node and tool has a unique name and ID, and each operation is stamped with execution context IDs, locating specific inputs and outputs is straightforward. Then given pub/sub events, the framework can build a directed data flows between nodes. Even there are circles, the data flow can form a DAG with publishing and consuming offset in each topic.</p>"},{"location":"getting-started/features/#idempotency","title":"Idempotency","text":"<p>Graphite adopts an event-driven architecture where topics function as logical message queues, storing each event exactly once in an event store. When a workflow fails, needs to be retried, or is paused (e.g., for a human-in-the-loop intervention), it can resume from the last known valid state by replaying events that were produced but not yet consumed by downstream nodes.</p> <p>Consumption events are only recorded once the entire node processing completes successfully. Until that point, the system treats partial or failed node executions as if they never happened, preventing duplicated outputs or broken states. Should a node encounter an error (e.g., an LLM connection failure, external API issue, or function exception), Graphite detects the unconsumed events upon restoration and places the associated node(s) back into the execution queue. This design ensures the node can safely retry from the same input without creating conflicting or duplicated consumption records.</p> <p>By storing each event exactly once and withholding consumption records until success, Graphite guarantees idempotent behavior. Even if a node issues multiple invocations due to an error, the event logs and consumption rules still reconstruct a single, consistent path from invocation to response. This approach produces correct outcomes on retries while maintaining a complete, conflict-free audit trail.</p>"},{"location":"getting-started/features/#auditability","title":"Auditability","text":"<p>Auditability in Graphite emerges naturally from its observability. By automatically persisting all execution events and pub/sub events in a centralized event store, the platform provides a complete historical record of every action taken. Function decorators capture each execution (including inputs, outputs, and exceptions), while Topic operations log every publish and consume operation, and effectively acting as a \u201ccache\u201d layer of orchestration events.</p> <p>Moreover, Graphite\u2019s modular design and clear separation of concerns simplify the process of examining specific components\u2014such as an LLM node and its associated tool. Each module has well-defined responsibilities, ensuring that every action is accurately documented in the event store and easily traceable. This end-to-end audit trail not only supports today\u2019s nascent AI regulations but positions Graphite to adapt to evolving compliance requirements. By preserving all relevant data in a consistent, verifiable format, Graphite provides the transparency and accountability that organizations demand from AI solutions.</p>"},{"location":"getting-started/features/#restorability","title":"Restorability","text":"<p>Restorability in Graphite builds on top of idempotency, ensuring that whenever a workflow stops\u2014due to an exception, human intervention, or any other cause\u2014it always concludes at a point where an event has been successfully published. This guarantees that upon resuming the workflow for an unfinished assistant_request_id, any node subscribed to that newly published event is reactivated, effectively restarting the process from where it left off.</p> <p>Internally, Graphite uses offset-based consumption in each topic. Whenever a node publishes an event (including self-loops or circular dependencies), the system records the publish offset in <code>PublishToTopicEvent</code> instance. When a node later consumes that event, it updates a corresponding consumption offset in topic, and store the offset in <code>ConsumeFromTopicEvent</code>. If a workflow is interrupted before consumption offsets are written, the node remains subscribed to the \u201cunconsumed\u201d event. As a result, when the workflow recovers, the engine identifies these outstanding events and places the node(s) back into the execution queue.</p> <p>This mechanism effectively transforms cyclical or looping dependencies into a directed acyclic graph. The event store, combined with offset tracking, reveals which events have been fully processed and which remain pending, letting Graphite re-trigger only the incomplete parts of the workflow. The result is a resilient system that can resume from exactly where it stopped\u2014without reprocessing entire segments or risking inconsistent states.</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide walks you through installing Graphite using pip.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<p>Prerequisites: - Python 3.7+  - pip (Python package installer)</p>"},{"location":"getting-started/installation/#installation","title":"Installation","text":"<p>Graphite can be installed with a single command using pip:</p> <pre><code>pip install grafi\n</code></pre> <p>That's it! Graphite will be installed along with all its dependencies.</p>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>After installation, verify that Graphite is installed correctly:</p> <pre><code># Check if the installation was successful\npython -c \"import grafi; print('Graphite installed successfully')\"\n</code></pre>"},{"location":"getting-started/installation/#virtual-environment-recommended","title":"Virtual Environment (Recommended)","text":"<p>For better dependency management, it's recommended to install Graphite in a virtual environment:</p> <pre><code># Create a virtual environment\npython -m venv graphite-env\n\n# Activate the virtual environment\n# On Linux/macOS:\nsource graphite-env/bin/activate\n# On Windows:\ngraphite-env\\Scripts\\activate\n\n# Install Graphite\npip install grafi\n\n# When done, deactivate the virtual environment\ndeactivate\n</code></pre>"},{"location":"getting-started/installation/#upgrading","title":"Upgrading","text":"<p>To upgrade to the latest version of Graphite:</p> <pre><code>pip install --upgrade grafi\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<p>Permission Errors: If you encounter permission errors, try installing with the <code>--user</code> flag:</p> <pre><code>pip install --user grafi\n</code></pre> <p>Dependency Conflicts: If you have dependency conflicts, consider using a virtual environment or:</p> <pre><code>pip install --force-reinstall grafi\n</code></pre> <p>Python Version Issues: Ensure you're using a supported Python version:</p> <pre><code>python --version\n</code></pre>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter installation issues:</p> <ol> <li>Check the GitHub repository for current documentation</li> <li>Look through GitHub Issues for similar problems</li> <li>Create a new issue with:</li> <li>Your operating system and version</li> <li>Python and pip versions</li> <li>Complete error messages</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once Graphite is installed, you can start using it in your Python projects:</p> <pre><code>import grafi\n# Your Graphite code here\n</code></pre> <p>Check the project documentation for usage examples and API reference.</p>"},{"location":"getting-started/quickstart/","title":"Getting Started with Graphite: The Hello, World! Assistant","text":"<p>Graphite is an event-driven AI agent framework designed for modularity, observability, and composability. This guide will walk you through building a minimal \"Hello, World\" agent using the <code>grafi</code> package. For this example we will create a simple function call assistant that generates a mock user form and wrap it in a node to be used with the Graphite framework.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Make sure the following are installed:</p> <ul> <li>Python 3.11 or 3.12 (required by the <code>grafi</code> package)</li> <li>Poetry</li> <li>Git</li> </ul> <p>\u26a0\ufe0f Important: <code>grafi</code> requires Python &gt;=3.11 and &lt;3.13. Python 3.13+ is not yet supported.</p>"},{"location":"getting-started/quickstart/#1-create-a-new-project-directory","title":"1. Create a New Project Directory","text":"<pre><code>mkdir graphite-hello-world\ncd graphite-hello-world\n</code></pre>"},{"location":"getting-started/quickstart/#2-initialize-a-poetry-project","title":"2. Initialize a Poetry Project","text":"<p>This will create the <code>pyproject.toml</code> file that Poetry needs. Be sure to specify a compatible Python version:</p> <pre><code>poetry init --name graphite-hello-world -n\n</code></pre> <p>Then open <code>pyproject.toml</code> and ensure it includes:</p> <pre><code>[tool.poetry.dependencies]\ngrafi = \"^0.0.12\"\npython = \"&gt;=3.11,&lt;3.13\"\n</code></pre> <p>Now install the dependencies:</p> <pre><code>poetry install --no-root\n</code></pre> <p>This will automatically create a virtual environment and install <code>grafi</code> with the appropriate Python version.</p> <p>\ud83d\udca1 You can also create the virtual environment with the correct Python version explicitly:</p> <p><code>bash poetry env use python3.12</code></p>"},{"location":"getting-started/quickstart/#3-create-assistant","title":"3. Create Assistant","text":"<p>In graphite an assistant is a specialized node that can handle events and perform actions based on the input it receives. We will create a simple assistant that uses OpenAI's language model to process input, make function calls, and generate responses.</p> <p>Create a file named <code>assistant.py</code> and add the code from our example directory here</p>"},{"location":"getting-started/quickstart/#class-simplefunctionllmassistant","title":"Class <code>SimpleFunctionLLMAssistant</code>","text":"<pre><code># assistant.py\nclass SimpleFunctionLLMAssistant(Assistant):\n\n    oi_span_type: OpenInferenceSpanKindValues = Field(\n        default=OpenInferenceSpanKindValues.AGENT\n    )\n    name: str = Field(default=\"SimpleFunctionLLMAssistant\")\n    type: str = Field(default=\"SimpleFunctionLLMAssistant\")\n    api_key: Optional[str] = Field(default_factory=lambda: os.getenv(\"OPENAI_API_KEY\"))\n    model: str = Field(default=\"gpt-4o-mini\")\n    output_format: OutputType\n    function: Callable\n</code></pre> <ul> <li><code>oi_span_type</code>: this field defines the OpenInference span kind for the assistant, which is set to <code>AGENT</code>, ,you will see this span kind in the OpenTelemetry traces.</li> <li><code>name</code>: the name of the assistant. This could be any string that identifies the assistant.</li> <li><code>type</code>: the type of the assistant, which is set to <code>SimpleFunctionLLMAssistant</code>.</li> <li><code>api_key</code>: the API key for OpenAI, which can be set via an environment variable or directly in the code.</li> <li><code>model</code>: the model to be used by the assistant, defaulting to <code>gpt-4o-mini</code>.</li> <li><code>output_format</code>: the format of the output, which is an instance of <code>OutputType</code>. This will be later used to define the output to be returned by the assistant. Will be mapped to a pydantic class.</li> <li><code>function</code>: a callable function that the assistant will use to process the input data. This function is a python function, can be any callable that takes input and returns output. In this case, it will be used to process the user form data.</li> </ul>"},{"location":"getting-started/quickstart/#class-builder","title":"Class <code>Builder</code>","text":"<p>Every assistant in Graphite has an inner class called builder class that allows you to construct the assistant step by step. The <code>Builder</code> class for <code>SimpleFunctionLLMAssistant</code> will allow you to set the API key, model, output format, and function before building the assistant. This class uses the builder pattern to create an instance of <code>SimpleFunctionLLMAssistant</code>. It is used to initialize the assistant with the fields that have been defined in the previous step.</p> <pre><code>class SimpleFunctionLLMAssistant(Assistant):\n   ...\n   ...\n    class Builder(Assistant.Builder):\n        \"\"\"Concrete builder for SimpleFunctionLLMAssistant.\"\"\"\n\n        _assistant: \"SimpleFunctionLLMAssistant\"\n\n        def __init__(self) -&gt; None:\n            self._assistant = self._init_assistant()\n\n        def _init_assistant(self) -&gt; \"SimpleFunctionLLMAssistant\":\n            return SimpleFunctionLLMAssistant.model_construct()\n\n        def api_key(self, api_key: str) -&gt; Self:\n            self._assistant.api_key = api_key\n            return self\n\n        def model(self, model: str) -&gt; Self:\n            self._assistant.model = model\n            return self\n\n        def output_format(self, output_format: OutputType) -&gt; Self:\n            self._assistant.output_format = output_format\n            return self\n\n        def function(self, function: Callable) -&gt; Self:\n            self._assistant.function = function\n            return self\n\n        def build(self) -&gt; \"SimpleFunctionLLMAssistant\":\n            self._assistant._construct_workflow()\n            return self._assistant\n</code></pre> <pre><code>    def _construct_workflow(self) -&gt; \"SimpleFunctionLLMAssistant\":\n        function_topic = Topic(name=\"function_call_topic\")\n\n        llm_input_node = (\n            LLMNode.Builder()\n            .name(\"OpenAIInputNode\")\n            .subscribe(SubscriptionBuilder().subscribed_to(agent_input_topic).build())\n            .command(\n                LLMResponseCommand.Builder()\n                .llm(\n                    OpenAITool.Builder()\n                    .name(\"UserInputLLM\")\n                    .api_key(self.api_key)\n                    .model(self.model)\n                    .chat_params({\"response_format\": self.output_format})\n                    .build()\n                )\n                .build()\n            )\n            .publish_to(function_topic)\n            .build()\n        )\n\n        # Create a function node\n\n        function_call_node = (\n            FunctionNode.Builder()\n            .name(\"FunctionCallNode\")\n            .subscribe(SubscriptionBuilder().subscribed_to(function_topic).build())\n            .command(\n                FunctionCommand.Builder()\n                .function_tool(FunctionTool.Builder().function(self.function).build())\n                .build()\n            )\n            .publish_to(agent_output_topic)\n            .build()\n        )\n\n        # Create a workflow and add the nodes\n        self.workflow = (\n            EventDrivenWorkflow.Builder()\n            .name(\"simple_function_call_workflow\")\n            .node(llm_input_node)\n            .node(function_call_node)\n            .build()\n        )\n\n        return self\n</code></pre>"},{"location":"getting-started/quickstart/#4-call-the-assistant","title":"4. Call the Assistant","text":"<p>Create a <code>main.py</code> that will call the assistant created previously.</p>"},{"location":"getting-started/quickstart/#setup-openai_api_key","title":"Setup <code>OPENAI_API_KEY</code>","text":"<pre><code>from grafi.common.containers.container import container\nevent_store = container.event_store\n\napi_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n</code></pre> <p>Replace your <code>OPENAI_API_KEY</code> in the environment variable or set it directly in the code for testing purposes.</p> <pre><code>export OPENAI_API_KEY=\"sk-proj-***********\"\n</code></pre>"},{"location":"getting-started/quickstart/#create-the-userform-model","title":"Create the <code>UserForm</code> model","text":"<pre><code>from pydantic import BaseModel\n\nclass UserForm(BaseModel):\n    \"\"\"\n    A simple user form model for demonstration purposes.\n    \"\"\"\n\n    first_name: str\n    last_name: str\n    location: str\n    gender: str\n\n</code></pre>"},{"location":"getting-started/quickstart/#create-the-print_user_form-function","title":"Create the <code>print_user_form</code> function","text":"<p>A function to print user form details from the messages received by the assistant.</p> <pre><code>from grafi.common.models.message import Message, Messages\ndef print_user_form(input_messages: Messages) -&gt; List[str]:\n    \"\"\"\n    Function to print user form details.\n\n    Args:\n        Messages: The input messages containing user form details.\n\n    Returns:\n        list: A list string containing the user form details.\n    \"\"\"\n\n    user_details = []\n\n    for message in input_messages:\n        if message.role == \"assistant\" and message.content:\n            try:\n                if isinstance(message.content, str):\n                    form = UserForm.model_validate_json(message.content)\n                    print(\n                        f\"User Form Details:\\nFirst Name: {form.first_name}\\nLast Name: {form.last_name}\\nLocation: {form.location}\\nGender: {form.gender}\\n\"\n                    )\n                    user_details.append(form.model_dump_json(indent=2))\n            except Exception as e:\n                raise ValueError(\n                    f\"Failed to parse user form from message content: {message.content}. Error: {e}\"\n                )\n\n    return user_details\n</code></pre>"},{"location":"getting-started/quickstart/#create-the-get_execution_context-function","title":"Create the <code>get_execution_context</code> function","text":"<p>This function will create an <code>ExecutionContext</code> for the assistant to use during execution. It generates unique IDs for conversation and execution to be tracked through the event store.</p> <pre><code>import uuid\nfrom grafi.common.models.execution_context import ExecutionContext\n\ndef get_execution_context() -&gt; ExecutionContext:\n    return ExecutionContext(\n        conversation_id=\"conversation_id\",\n        execution_id=uuid.uuid4().hex,\n        assistant_request_id=uuid.uuid4().hex,\n    )\n</code></pre>"},{"location":"getting-started/quickstart/#putting-it-all-together","title":"Putting it all together","text":"<pre><code>execution_context = get_execution_context()\n\nassistant = (\n    SimpleFunctionLLMAssistant.Builder()\n    .name(\"SimpleFunctionLLMAssistant\")\n    .api_key(api_key)\n    .function(print_user_form)\n    .output_format(UserForm)\n    .build()\n)\n\n# Test the run method\ninput_data = [\n    Message(\n        role=\"user\",\n        content=\"Generate mock user.\",\n    )\n]\n\noutput = assistant.execute(execution_context, input_data)\nprint(output)\n</code></pre>"},{"location":"getting-started/quickstart/#5-run-the-application","title":"5. Run the Application","text":"<p>Use Poetry to execute the script inside the virtual environment:</p> <pre><code>poetry run python main.py\n</code></pre> <p>You should see:</p> <pre><code>graphite-hello-world-OsMKLmDe-py3.12 \u276f poetry run python main.py\n2025-05-26 19:19:10.299 | DEBUG    | grafi.common.instrumentations.tracing:is_local_endpoint_available:27 - Endpoint check failed: [Errno -3] Temporary failure in name resolution\n2025-05-26 19:19:10.300 | DEBUG    | grafi.common.instrumentations.tracing:is_local_endpoint_available:27 - Endpoint check failed: [Errno 111] Connection refused\n2025-05-26 19:19:10.300 | DEBUG    | grafi.common.instrumentations.tracing:setup_tracing:95 - OTLP endpoint is not available. Using InMemorySpanExporter.\n2025-05-26 19:19:10.341 | INFO     | grafi.common.topics.topic:publish_data:76 - [agent_input_topic] Message published with event_id: 858d841f8b8d48498fc3c31cf9a50c24\n2025-05-26 19:19:10.341 | DEBUG    | grafi.nodes.impl.llm_node:execute:57 - Executing LLMNode with inputs: [ConsumeFromTopicEvent(event_id='11ede69c47c34222a9bcb7c81adfb469', event_version='1.0', execution_context=ExecutionContext(conversation_id='conversation_id', execution_id='467fb5006a25440c80f5801c05bfc807', assistant_request_id='c574d43fdea94280817facb9c3a597dc', user_id=''), event_type=&lt;EventType.CONSUME_FROM_TOPIC: 'ConsumeFromTopic'&gt;, timestamp=datetime.datetime(2025, 5, 26, 18, 19, 10, 341430, tzinfo=datetime.timezone.utc), topic_name='agent_input_topic', offset=0, data=[Message(name=None, message_id='16e956e2e0424362b6362f6899bff798', timestamp=1748283550341049054, content='Generate mock user.', refusal=None, annotations=None, audio=None, role='user', tool_call_id=None, tools=None, function_call=None, tool_calls=None)], consumer_name='OpenAIInputNode', consumer_type='LLMNode')]\n2025-05-26 19:19:11.840 | INFO     | grafi.common.topics.topic:publish_data:76 - [function_call_topic] Message published with event_id: b1ee77395c3e449193d1a2175133bf98\nUser Form Details:\nFirst Name: John\nLast Name: Doe\nLocation: New York, USA\nGender: Male\n\n2025-05-26 19:19:11.841 | INFO     | grafi.common.topics.output_topic:publish_data:80 - [agent_output_topic] Message published with event_id: 7398d115a1fb4a4c9e2b14a4dcf6a114\n[Message(name=None, message_id='5bc27a0909b146049e0d5ce5c66e068f', timestamp=1748283551841195922, content='[\"{\\\\n  \\\\\"first_name\\\\\": \\\\\"John\\\\\",\\\\n  \\\\\"last_name\\\\\": \\\\\"Doe\\\\\",\\\\n  \\\\\"location\\\\\": \\\\\"New York, USA\\\\\",\\\\n  \\\\\"gender\\\\\": \\\\\"Male\\\\\"\\\\n}\"]', refusal=None, annotations=None, audio=None, role='function', tool_call_id=None, tools=None, function_call=None, tool_calls=None)]\n</code></pre>"},{"location":"getting-started/quickstart/#summary","title":"Summary","text":"<p>\u2705 Initialized a Poetry project \u2705 Installed <code>grafi</code> with the correct Python version constraint \u2705 Wrote a minimal agent that handles an event \u2705 Ran the agent using a test event</p>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the Graphite GitHub Repository for full-featured examples.</li> <li>Extend your agent to respond to different event types.</li> <li>Dive into advanced features like memory, workflows, and tools.</li> </ul> <p>Happy building! \ud83d\ude80</p>"},{"location":"user-guide/architecture/","title":"Arhitecture Overview","text":"<p>Graphite is an open-source framework designed for building domain-specific AI assistants through composable agentic workflows. We provide a highly extensible platform that empowers AI engineers to create custom workflows tailored to their specific business domains.</p> <p>In this documentation, we'll explore Graphite's architecture and each component in detail, while adhering to our core design philosophy: each component should only know what it needs to know. This principle of minimal coupling enables the flexibility and modularity that makes Graphite powerful.</p>"},{"location":"user-guide/architecture/#architecture","title":"Architecture","text":"<p>Below is an overview of the Graphite architecture. In the following sections, we'll break down each component and explore them in detail.</p> <p></p>"},{"location":"user-guide/assistant/","title":"Assistant","text":"<p>In our platform, the Assistant serves as the primary interface between the user and the underlying agent system. Its core responsibility includes processing user input, constructing and managing workflows, and coordinating interactions between users and the workflow components.</p>"},{"location":"user-guide/assistant/#assistantbase-class","title":"AssistantBase Class","text":"<p>The <code>AssistantBase</code> class provides an abstract base interface defining the foundational properties required by all assistants.</p>"},{"location":"user-guide/assistant/#assistantbase-class-fields","title":"AssistantBase Class Fields","text":"Field Description <code>assistant_id</code> Unique identifier for the assistant instance. <code>name</code> Human-readable name identifying the assistant. <code>type</code> Category or type specification for the assistant. <code>oi_span_type</code> Semantic attribute from OpenInference for tracing purposes. <code>workflow</code> Associated workflow instance managed by the assistant."},{"location":"user-guide/assistant/#assistant-class","title":"Assistant Class","text":"<p>The concrete <code>Assistant</code> class extends <code>AssistantBase</code>, implementing workflow execution and managing the interactions between user inputs and the agent\u2019s workflow components.</p>"},{"location":"user-guide/assistant/#assistant-class-methods","title":"Assistant Class Methods","text":"Method Description <code>execute</code> Processes input messages synchronously through the workflow and returns sorted response messages. <code>a_execute</code> Processes input messages asynchronously through the workflow, suitable for streaming or concurrent use cases. <code>_get_consumed_events</code> Internally retrieves and processes consumed events from workflow topics. <code>to_dict</code> Serializes the assistant's workflow state and configuration into a dictionary. <code>generate_manifest</code> Generates a manifest file representing the assistant\u2019s configuration and workflow state. <p>Both <code>AssistantBase</code> and <code>Assistant</code> utilize an inner <code>Builder</code> class to facilitate structured and configurable construction of Assistant instances, enhancing clarity and ease of use.</p> <p>Developers can extend the base classes to implement specific business logic or functionality required by their unique applications. By leveraging the provided interfaces, assistants can seamlessly manage complex workflow orchestration and user interaction scenarios.</p>"},{"location":"user-guide/command/","title":"Command","text":"<p>In our platform, the Command implements the Command Pattern, effectively separating workflow orchestration (Nodes) from execution logic (Tools). Commands encapsulate the request or execution logic, allowing the orchestrator (Node) to delegate execution to the executor (Tool) without needing to know the internal details of the execution process.</p> <p>Using the Command Pattern brings several significant benefits:</p> <ul> <li>Separation of Concerns: Clearly separates orchestration logic from execution logic, making the system more modular.</li> <li>Flexibility and Extensibility: Allows for easy swapping, extension, and customization of execution logic without altering workflow structures.</li> <li>Improved Maintainability: Facilitates testing and debugging by isolating command logic within distinct units.</li> </ul> <p>The <code>Command</code> interface class itself primarily defines the interface structure and thus does not contain specific instance fields.</p> Method Description <code>execute</code> Defines synchronous execution logic to process inputs and return results; must be implemented by subclasses. <code>a_execute</code> Defines asynchronous execution logic supporting streaming or concurrent processes; must be implemented by subclasses. <code>to_dict</code> Serializes command configurations or state for persistence or debugging purposes; must be implemented by subclasses. <p>The <code>Command</code> interface class utilizes an inner <code>Builder</code> class to facilitate structured and step-by-step construction of Command instances, enhancing readability and configurability.</p> <p>Developers implement custom Commands tailored to specific logic or operational needs by inheriting from this base Command class and overriding the required methods. This approach empowers developers to flexibly craft specialized behaviors while maintaining consistency across the workflow execution environment.</p> <p>The concrete implementation of <code>Command</code> interface should be with its associated tools. Here are some examples.</p>"},{"location":"user-guide/command/#llm-response-command-and-llm-stream-command","title":"LLM Response Command and LLM Stream Command","text":"<p><code>LLMResponseCommand</code> encapsulates synchronous LLM usage within a command, allowing a node to request the LLM for a response.</p> <p>Fields:</p> Field Description <code>llm</code> An <code>LLM</code> instance (e.g., OpenAI) that provides the response <p>Key methods are:</p> Method Description <code>execute(execution_context, input_data)</code> Synchronously obtains a single response from the LLM based on the provided messages. <code>a_execute(execution_context, input_data)</code> Asynchronously streams generated messages from the LLM. <code>to_dict()</code> Serializes the command\u2019s state, including its associated LLM configuration. <p>The <code>LLMStreamResponseCommand</code> specializes <code>LLMResponseCommand</code> for stream use cases where synchronous responses must be disabled, and only relies exclusively on asynchronous streaming via a_execute.</p>"},{"location":"user-guide/command/#function-calling-command","title":"Function Calling Command","text":"<p><code>FunctionCallCommand</code> is a concrete implementation of the Command interface that allows a Node to call a <code>FunctionCallTool</code>. By assigning a <code>FunctionCallTool</code> to the command, the Node can trigger function execution without needing to know how arguments are parsed or how the function is actually invoked.</p> <p>Fields:</p> Field Description <code>function_tool</code> A <code>FunctionCallTool</code> instance that encapsulates the registered function and its execution logic. <p>Methods:</p> Method Description <code>execute(execution_context, input_data)</code> Invokes the <code>function_tool</code>'s synchronous <code>execute</code> method, returning a list of resulting <code>Message</code> objects. <code>a_execute(execution_context, input_data)</code> Calls the <code>function_tool</code>'s asynchronous <code>a_execute</code>, yielding one or more <code>Message</code> objects in an async generator. <code>get_function_specs()</code> Retrieves the function specifications (schema, name, parameters) from the underlying <code>function_tool</code>. <code>to_dict()</code> Serializes the command\u2019s current state, including the <code>function_tool</code> configuration. <p>By passing a <code>FunctionCallTool</code> to the <code>function_tool</code> field, you can seamlessly integrate function-based logic into a Node\u2019s orchestration without embedding execution details in the Node or the tool consumer. This separation keeps workflows flexible and easy to extend.</p>"},{"location":"user-guide/command/#embedding-response-command-and-rag-response-command","title":"Embedding Response Command and RAG Response Command","text":"<p><code>EmbeddingResponseCommand</code> encapsulates a <code>RetrievalTool</code> for transforming input messages into embeddings, retrieving relevant content, and returning it as a <code>Message</code>. This command is used by <code>EmbeddingRetrievalNode</code>.</p> <p><code>EmbeddingResponseCommand</code> fields:</p> Field Description <code>retrieval_tool</code> A <code>RetrievalTool</code> instance for embedding-based lookups, returning relevant data <p><code>EmbeddingResponseCommand</code> methods:</p> Method Description <code>execute(execution_context, input_data)</code> Synchronously calls <code>retrieval_tool.execute</code>, returning the resulting <code>Message</code>. <code>a_execute(execution_context, input_data)</code> Asynchronously calls <code>retrieval_tool.a_execute</code>, yielding one or more <code>Message</code> objects. <code>to_dict()</code> Serializes the command\u2019s state, including the <code>retrieval_tool</code> configuration. <p><code>RagResponseCommand</code> similarly delegates to a <code>RagTool</code> that performs retrieval-augmented generation. This command is used by <code>RagNode</code>.</p> <p><code>RagResponseCommand</code> fields:</p> Field Description <code>rag_tool</code> A <code>RagTool</code> instance for retrieval-augmented generation. <p><code>RagResponseCommand</code> methods:</p> Method Description <code>execute(execution_context, input_data)</code> Synchronously calls <code>rag_tool.execute</code>, returning a <code>Message</code> with retrieval results. <code>a_execute(execution_context, input_data)</code> Asynchronously invokes <code>rag_tool.a_execute</code>, yielding partial or complete messages from the retrieval-augmented flow. <code>to_dict()</code> Serializes the command\u2019s state, reflecting the assigned <code>RagTool</code> configuration. <p>Both commands enable a node to delegate specialized retrieval operations to their respective tools, without needing to manage the internal logic of how embeddings or RAG processes are performed.</p>"},{"location":"user-guide/conventional-rules/","title":"Conventional Rules","text":"<p>While the platform is designed for maximum flexibility, certain conventions guide how components interact. These rules ensure consistency, maintainability, and ease of integration across a range of use cases\u2014especially when handling user requests, generating outputs, and enabling powerful LLM features.</p>"},{"location":"user-guide/conventional-rules/#reserved-topics","title":"Reserved Topics","text":""},{"location":"user-guide/conventional-rules/#agent-input-topic","title":"Agent Input Topic","text":"<ul> <li>Triggering the Workflow: All new assistant requests with a fresh <code>assistant_request_id</code> begin by publishing the user input to the agent_input_topic.</li> <li>Downstream Consumption: Nodes that need to process initial requests consume from this topic, triggering the rest of the workflow.</li> </ul>"},{"location":"user-guide/conventional-rules/#agent-output-topics","title":"Agent Output Topics","text":"<ul> <li>Final Responses: All output events route to agent_output_topic, which the Assistant consumes to return data to the user or caller.</li> <li>Single Consumer: Only the Assistant should subscribe to this topic, avoiding conflicting read operations.</li> </ul>"},{"location":"user-guide/conventional-rules/#human-request-topic","title":"Human Request Topic","text":"<ul> <li>Human in the Loop: Used when user intervention is required; the system posts an <code>OutputTopicEvent</code> here, which the Assistant can consume to display prompts or questions.</li> <li>User Response: When the user replies, <code>append_user_input()</code> posts a <code>PublishToTopicEvent</code> (the user\u2019s answer). This message is then read by downstream nodes.</li> <li>Assistant Role: The Assistant only consumes <code>OutputTopicEvent</code> objects, while nodes consume both the question (<code>OutputTopicEvent</code>) and the final user reply (<code>PublishToTopicEvent</code>).</li> </ul> <p>Rationale: Structuring input and output channels ensures clarity, preventing multiple consumers from inadvertently processing final outputs and providing a clear path for user-driven requests.</p>"},{"location":"user-guide/conventional-rules/#outputtopicevent","title":"OutputTopicEvent","text":"<ul> <li>Dedicated for Assistant: If a newly received event is an <code>OutputTopicEvent</code>, the workflow\u2019s <code>on_event()</code> skips subscription checks, since only the Assistant should consume it.</li> <li>Exclusive Destination: <code>OutputTopicEvent</code> can only be published to agent_output_topic or human_request_topic, ensuring a clear boundary for user-facing outputs.</li> </ul> <p>Rationale: Limiting <code>OutputTopicEvent</code> usage avoids confusion over who should read final results, reinforcing the principle of single responsibility for returning data to the user.</p>"},{"location":"user-guide/conventional-rules/#stream-usage","title":"Stream Usage","text":"<ul> <li>Output Only: Streaming is only relevant for final outputs, letting the LLM emit partial content in real time.</li> <li>Asynchronous Requirement: Nodes, workflows, and assistants do not support synchronous streaming. Though the LLM tool may have a synchronous stream function, the system\u2019s architecture uses async flows.</li> <li>Usage Pattern: For practical examples, see <code>stream_assistant</code>; it shows how to handle partial token streams differently from normal async generators.</li> </ul> <p>Rationale: Maintaining an async-only stream approach for nodes, workflows, and assistants simplifies concurrency, reduces potential race conditions, and provides a consistent development experience.</p>"},{"location":"user-guide/conventional-rules/#llmfunctioncall","title":"LLMFunctionCall","text":"<ul> <li>Agent-Like Interactions: By calling functions, the LLM can access additional tools\u2014making the system more agent-like.</li> <li>Separation of Concerns: The LLM node focuses on generating or interpreting responses, while a separate <code>LLMFunctionCall</code> node invokes tool logic.</li> <li>Upstream Connections: Each <code>LLMFunctionCall</code> must directly connect to one or more LLM node via topic(s) when want to enable upstream LLM node tool calling feature.</li> <li>Downstream Connections: Each LLM node can directly connect to one or more <code>LLMFunctionCall</code> nodes. If the workflow builder detects an <code>LLMFunctionCall</code> node downstream, it attaches the relevant function specs to the LLM node\u2019s final output message, letting the LLM know which tools are available.</li> </ul> <p>Rationale: Decoupling LLM operations from tool invocation keeps the graph modular, fosters reusability, and ensures that an LLM can dynamically discover and call specialized tools within a single workflow.</p>"},{"location":"user-guide/event-driven-workflow/","title":"Event Driven Workflow","text":"<p>The event-driven workflow serves as the dynamic execution layer of Graphite, orchestrating node execution through a publish/subscribe (pub/sub) model. This architecture structures workflows as interconnected nodes that communicate via event queues (topic), enabling asynchronous, scalable, and flexible processing.</p> <p>The fields of the event driven workflow is:</p> Field Name Description <code>name</code> Unique identifier for the workflow instance (default: <code>\"EventDrivenWorkflow\"</code>). <code>type</code> The type identifier for the workflow, typically matching the class name (<code>\"EventDrivenWorkflow\"</code>). <code>oi_span_type</code> Semantic attribute from OpenInference used for tracing and observability. <code>nodes</code> Collection of all nodes defined within this workflow. <code>topics</code> Dictionary storing all event topics managed by the workflow. <code>topic_nodes</code> Mapping of topic names to lists of node names subscribed to each topic. <code>execution_context</code> Runtime execution context containing state and metadata for the current workflow execution. <code>node_queue</code> Queue of nodes that are ready to execute, typically triggered by event availability or workflow state. <p>The following table summarizes key functions within the EventDrivenWorkflow class, highlighting their role in managing workflow execution, event handling, and node orchestration:</p> Function Name Description <code>_publish_events</code> Publishes events to designated workflow topics after a node completes execution. <code>execute</code> Processes nodes from the execution queue sequentially, executing each node as inputs become available until the queue is exhausted. <code>a_execute</code> Asynchronous version of <code>execute</code>; initializes the workflow and processes nodes asynchronously, handling streaming nodes differently if required. <code>get_node_input</code> Collects and returns input events consumed by a node based on its subscribed topics. <code>on_event</code> Responds to newly published events, evaluates node readiness, and queues nodes for execution if their execution criteria are met. <code>initial_workflow</code> Initializes the workflow context, either restoring an unfinished workflow state or setting initial event data for new executions. <p>The following diagram illustrates the execution flow of a Node within an event-driven workflow:</p> <pre><code>graph TD\n    Start[Start Workflow Execution] --&gt; Restore[Restore Workflow State]\n    Restore[Restore Workflow State] --&gt; EventListener[Event Listener: `on_event`]\n    EventListener --&gt; CheckCriteria[Evaluate Execution Criteria]\n    CheckCriteria --&gt;|Conditions Met| QueueNode[Add Node to Execution Queue]\n    QueueNode --&gt; ExecuteNode[Execute Next Node in Queue]\n    ExecuteNode --&gt; EventListener\n    EventListener --&gt;|No Pending Nodes| Output[Assistant Output]\n\n</code></pre>"},{"location":"user-guide/event-driven-workflow/#workflow-initialization","title":"Workflow Initialization","text":"<p>When a workflow instance is initialized, it constructs a graph (not necessary a DAG!) based on each node\u2019s subscribed (input) and published (output) topics. Additionally, it attaches function call specifications to the relevant LLM caller node, allowing the LLM to recognize and invoke available tools.</p> <pre><code>graph LR\n    A[Start: Initialize Workflow] --&gt; B[Build Graph with Dependencies]\n    B --&gt; C[Setup Topic Subscriptions]\n    C --&gt; D[LLM Recognizes Tools]\n    D --&gt; E[Validation Check]\n</code></pre>"},{"location":"user-guide/event-driven-workflow/#node-execution-process","title":"Node Execution Process","text":"<p>The pub/sub model governs node execution. A node remains idle until all required messages from its subscribed topics become available. Once the conditions are met, the workflow add the node to <code>execution_queue</code>. When node executed and publishes the result to designated topics, it triggers the <code>on_event()</code> function, which performs the following steps:</p> <ol> <li>Identifies all nodes subscribed to the newly published topic.</li> <li>Checks execution criteria for each subscribed node.</li> <li>Queues nodes that meet the criteria for execution.</li> </ol>"},{"location":"user-guide/event-driven-workflow/#input-preparation-execution-logic","title":"Input Preparation &amp; Execution Logic","text":"<p>Before executing a node, the workflow collects all relevant events from its subscribed topics and passes them as input to ensure accurate processing. To support flexible and complex execution logic, the system provides both AND and OR operators:</p> <ul> <li>AND operator: A node executes only when all required conditions are met.</li> <li>OR operator: A node executes when at least one of the required conditions is met.</li> </ul> <p>By combining AND and OR operators, users can construct advanced logic tailored to their specific requirements. However, when using OR logic, it is important to consider potential inconsistencies in execution. Since a node is appended to the <code>execution_queue</code> as soon as one OR condition is satisfied, messages from other subscribed topics in the OR condition may or may not appear by the time the workflow prepares the node\u2019s input. This can lead to inconsistencies in data availability, affecting the reliability of execution. Careful design and validation are recommended when implementing OR-based logic to ensure predictable workflow behavior.</p>"},{"location":"user-guide/event-driven-workflow/#restoring-unfinished-workflows","title":"Restoring Unfinished Workflows","text":"<p>A key advantage of this event-driven architecture is its ability to restore and resume unfinished workflows. When <code>initial_workflow()</code> is called, the system:</p> <ol> <li>Checks if an unfinished workflow exists for the given assistant_request_id.</li> <li>If no unfinished workflow exists, it publishes the input to agent_input_topic, initiating a new workflow execution.</li> <li>If an unfinished workflow exists, it restores the workflow to the last valid state, allowing execution to continue without restarting.</li> </ol> <p>This restoration mechanism is particularly beneficial for human-in-the-loop scenarios, where workflows may pause while awaiting user input. From our perspective, a human-in-the-loop interaction is essentially a paused workflow until a response is received. Beyond human interaction, the ability to resume from failure points is also cost-efficient, as it prevents unnecessary re-execution of previous steps\u2014reducing LLM call costs and optimizing overall performance.</p>"},{"location":"user-guide/executor/","title":"Executor","text":"<p>Executor decorator record the executor action events, such as invoke, respond, and failed. Each time execute function has been called, the decorate will save the events to the event store. Also it will create tracer and push the tracer to the platform such as phoenix or Arize.</p>"},{"location":"user-guide/models/","title":"Models","text":"<p>In the Graphite, varies models provide the fundamental data structures that underpin the event-driven workflow. Message represents the content exchanged between users, assistants, and language models, enabling consistent communication and processing. Event captures the various actions and state changes in the system, from workflow initiation to final outputs. Meanwhile, Topic defines the named channels where events are published and consumed, establishing a structured mechanism for coordinating data flow across the platform.</p>"},{"location":"user-guide/models/#message","title":"Message","text":"<p><code>Message</code> extends OpenAI\u2019s <code>ChatCompletionMessage</code>, serving as a standardized data structure for both incoming and outgoing content in the event-driven workflow. Each <code>Message</code> instance retains essential metadata such as timestamps, unique identifiers, and optional tool references, facilitating robust and traceable communication between users, assistants, and LLM tools.</p>"},{"location":"user-guide/models/#fields","title":"Fields","text":"Field Description <code>name</code> An optional name indicating the source or identifier for the message (e.g., function name). <code>message_id</code> A unique identifier for the message, defaulting to a generated UUID. <code>timestamp</code> The time in nanoseconds when the message was created, allowing strict chronological ordering. <code>role</code> Specifies the speaker\u2019s role (<code>system</code>, <code>user</code>, <code>assistant</code>, or <code>tool</code>). <code>tool_call_id</code> Associates the message with a particular tool invocation if relevant. <code>tools</code> An optional list of OpenAI's <code>ChatCompletionToolParam</code> for referencing available tool calls."},{"location":"user-guide/models/#usage-example","title":"Usage Example","text":"<pre><code>from grafi.common.models.message import Message\n\n# Creating a user message\nuser_message = Message(\n    role=\"user\",\n    content=\"What is the capital of France?\"\n)\n\n# Creating an assistant message\nassistant_message = Message(\n    role=\"assistant\",\n    content=\"The capital of France is Paris.\"\n)\n</code></pre> <p>In both cases, the <code>Message</code> class provides a consistent structure for storing conversation state, bridging the gap between OpenAI\u2019s chat messages and the system\u2019s event-driven architecture.</p>"},{"location":"user-guide/models/#event","title":"Event","text":"<p><code>Event</code> is the foundational data model in the event driven architecture, capturing the common fields and logic shared by all event types. Each subclass of <code>Event</code> (e.g., Node events, Topic events) extends this base with specialized data. The core <code>Event</code> model also offers a standard interface for serialization (<code>to_dict</code>) and deserialization (<code>from_dict</code>), promoting consistency across the platform.</p> <p>The <code>Event</code> fields are:</p> Field Description <code>event_id</code> Unique identifier for the event, defaulting to a generated UUID. <code>execution_context</code> A reference to the workflow\u2019s current state, including assistant request details and other metadata. <code>event_type</code> An <code>EventType</code> enum value describing the kind of event (e.g., NodeInvoke, ToolRespond). <code>timestamp</code> The UTC timestamp of event creation, used for ordering and auditing. <p>The benefits are:</p> <ul> <li>Consistency: All events adhere to the same schema for IDs, context, and timestamps.</li> <li>Extensibility: Subclasses can introduce additional fields while still retaining base serialization logic.</li> <li>Traceability: The shared timestamp and <code>execution_context</code> fields provide a reliable audit trail.</li> </ul> <p>By leveraging this Event model, the system enforces uniform data handling for everything from node invocations to assistant responses, simplifying debugging and logging throughout the workflow lifecycle.</p>"},{"location":"user-guide/models/#component-activity-event","title":"Component activity event","text":"<p>In the Graphite\u2019s layered architecture, each principal component (Assistant, Node, Tool, and Workflow) can invoke, respond, or fail during execution. And there are events associate with each actions, such as invoke  event, respond event and failed event. For nodes specifically, these actions are tracked as three distinct event types:</p> <ol> <li>NodeInvokeEvent: The node is invoked with input data.</li> <li>NodeRespondEvent: The node completes execution and returns output data.</li> <li>NodeFailedEvent: The node encounters an error during execution.</li> </ol> <p>These events capture the inputs, outputs, timestamps, and other metadata essential for observing and debugging node behavior.</p> <p>Here is the <code>Node</code> base event <code>NodeEvent</code>:</p> Field Description <code>node_id</code> Unique identifier for the node. Defaults to a generated UUID. <code>node_name</code> Human-readable name of the node. <code>node_type</code> Describes the functional category of the node (e.g., \"LLMNode\"). <code>subscribed_topics</code> The list of event topics to which this node is subscribed. <code>publish_to_topics</code> The list of event topics where the node publishes output. <code>execution_context</code> Workflow metadata, including request details and IDs. <code>event_type</code> The specific event variant: <code>NODE_INVOKE</code>, <code>NODE_RESPOND</code>, or <code>NODE_FAILED</code>. <code>timestamp</code> The UTC timestamp when the event was generated. <p>and the <code>Node</code> base event methods</p> Method Description <code>node_event_dict()</code> Returns a dictionary merging base event data (<code>event_dict()</code>) with node-specific fields (e.g., ID, topics). <code>node_event_base()</code> Class method that reconstructs node-specific fields (like <code>node_id</code> and <code>node_name</code>) from a dictionary. <code>event_dict()</code> Inherited from <code>Event</code>; provides flattening of <code>execution_context</code> and standard event metadata. <code>event_base()</code> Inherited from <code>Event</code>; extracts <code>event_id</code>, <code>event_type</code>, and <code>timestamp</code> from a serialized event. <code>to_dict()</code> / <code>from_dict()</code> Implemented in subclasses, each adjusts data serialization or deserialization for the event\u2019s unique fields. <p><code>NodeInvokeEvent</code> extended from  <code>NodeEvent</code>, with additional field:</p> Field Description <code>input_data</code> A list of <code>ConsumeFromTopicEvent</code> representing the node\u2019s consumed messages upon invocation. <p><code>NodeInvokeEvent</code> implemented the serialise and deserialise methods <code>to_dict()</code> and <code>from_dict(data)</code>.  </p> Method Description <code>to_dict()</code> Returns the merged dictionary from <code>node_event_dict()</code> plus the serialized list of input events (<code>input_data</code>). <code>from_dict(data)</code> Class method that calls <code>node_event_base</code> for the base event fields, then rebuilds <code>input_data</code> by deserializing each consumed event dict. <p><code>NodeRespondEvent</code> extended from <code>NodeEvent</code>, with two additional fields:</p> Field Description <code>input_data</code> A list of <code>ConsumeFromTopicEvent</code> messages that the node consumed. <code>output_data</code> The resulting message(s) (<code>Message</code> or list of <code>Message</code>) produced by the node\u2019s execution. <p><code>NodeRespondEvent</code> implemented the serialise and deserialise methods <code>to_dict()</code> and <code>from_dict(data)</code>.</p> Method Description <code>to_dict()</code> Calls <code>node_event_dict()</code> and includes JSON-serialized <code>output_data</code>. <code>from_dict()</code> Deserializes <code>input_data</code> and <code>output_data</code>; uses <code>node_event_base</code> for common fields. <p><code>NodeFailedEvent</code> extended from  <code>NodeEvent</code>, with additional field:</p> Field Description <code>input_data</code> A list of <code>ConsumeFromTopicEvent</code> messages that led to this error condition. <code>error</code> Contains information about the error encountered (stack trace, message, or custom error object). <p><code>NodeFailedEvent</code> implemented the serialise and deserialise methods <code>to_dict()</code> and <code>from_dict(data)</code>.</p> Method Description <code>to_dict()</code> Uses <code>node_event_dict()</code> and adds an <code>error</code> field. <code>from_dict()</code> Builds the event from <code>node_event_base</code>, restoring <code>input_data</code> and capturing <code>error</code> details. <p>Collectively, these Node Activity Events form a consistent pattern for tracking node lifecycle across invoke, respond, and fail states. The same concept applies to other components in the system (e.g., Assistant, Tool, Workflow), each featuring its respective invoke, respond, and failed events. This design ensures clear traceability and systematic error handling within the event-driven workflow architecture.</p>"},{"location":"user-guide/models/#publish-and-subscribe-event","title":"Publish and Subscribe Event","text":"<p>Publish and subscribe events capture data published to or consumed from specific channels - topics - in the system. They enable Nodes to communicate asynchronously by sending and receiving messages on named topics. The platform distinguishes three main types:</p> <ol> <li>PublishToTopicEvent: Emitted when data is published to a topic.</li> <li>ConsumeFromTopicEvent: Occurs when a consumer retrieves data from a topic.</li> <li>OutputTopicEvent: A special publish event intended for final user-facing outputs, typically consumed by an Assistant.</li> </ol> <p><code>TopicEvent</code> is the base event, it extends from <code>Event</code> class, and added following fields</p> Field Description <code>topic_name</code> Identifies the topic to which this event pertains (e.g., \"agent_input\", \"agent_output\"). <code>offset</code> A numeric indicator of the event\u2019s position in the topic stream. <code>data</code> The message(s) (or generator of messages) being transferred. <code>event_id</code> Inherited from <code>Event</code>; unique identifier for this event. <code>event_type</code> Inherited from <code>Event</code>; marks it as a topic event variant (e.g., <code>PUBLISH_TO_TOPIC</code>). <code>timestamp</code> Inherited from <code>Event</code>; records the time the event was created (UTC). <code>execution_context</code> Inherited from <code>Event</code>; includes metadata such as <code>assistant_request_id</code> for tracing. <p><code>TopicEvent</code> has following methods:</p> Method Description <code>topic_event_dict()</code> Combines base event data (<code>event_dict()</code>) with topic-specific fields and JSON-serialized <code>data</code>. <code>topic_event_base(dict)</code> Class method that deserializes topic data (including <code>Message</code> objects) and merges with base event fields. <code>event_dict()</code> From the <code>Event</code> class; flattens <code>execution_context</code> and includes standard metadata (event ID, type, etc.). <code>event_base(dict)</code> From the <code>Event</code> class; extracts <code>event_id</code>, <code>event_type</code>, and <code>timestamp</code>. <code>to_dict() / from_dict()</code> Implemented in each subclass, customizing how <code>data</code> or additional fields are serialized. <p><code>PublishToTopicEvent</code> extends <code>TopicEvent</code> with following additional fields</p> Field Description <code>consumed_event_ids</code> A list of event IDs indicating which prior events (e.g., consumed messages) led to this publish. <code>publisher_name</code> The name of the component (Node, Assistant, etc.) publishing the data. <code>publisher_type</code> The type/category of the publisher (e.g., \"Node\", \"Assistant\"). <p><code>PublishToTopicEvent</code> implemented the following methods</p> Method Description <code>to_dict()</code> Adds <code>consumed_event_ids</code>, <code>publisher_name</code>, and <code>publisher_type</code> to the standard topic event dict. <code>from_dict(dict)</code> Recreates the event by merging base topic fields with the additional publisher-related fields. <p><code>ConsumeFromTopicEvent</code> extends <code>TopicEvent</code> with following additional fields</p> Field Description <code>consumer_name</code> The name of the component consuming the data (Node, Assistant, etc.). <code>consumer_type</code> The category or type of the consumer (e.g., \"Node\", \"Assistant\"). <p><code>ConsumeFromTopicEvent</code> implemented the following methods</p> Method Description <code>to_dict()</code> Adds consumer-specific fields (<code>consumer_name</code>, <code>consumer_type</code>) to the base topic event data. <code>from_dict(dict)</code> Restores the consume event by parsing both base topic fields and the consumer-related fields. <p><code>OutputTopicEvent</code> is a special form of <code>PublishToTopicEvent</code> used exclusively for final outputs. Typically consumed by an Assistant to relay data back to the user.</p> <p><code>OutputTopicEvent</code>'s additional details are</p> <ul> <li>EventType is fixed to <code>OUTPUT_TOPIC</code>.</li> <li><code>data</code> can be a single <code>Message</code>, multiple <code>Message</code> objects, or a generator of messages. Currently, serialization is pending further implementation.</li> </ul> <p><code>OutputTopicEvent</code> implemented the following methods</p> Method Description <code>to_dict()</code> Extends <code>PublishToTopicEvent.to_dict()</code>, placeholder for future data serialization logic. <code>from_dict(dict)</code> Placeholder for data deserialization from a dictionary, to be implemented. <p>These topic-based events enable decoupled communication within the system. PublishToTopicEvent moves data onto a topic, ConsumeFromTopicEvent retrieves it, and OutputTopicEvent designates final user-facing outputs. By standardizing how messages flow through topics, the platform ensures reliability, traceability, and straightforward integration among nodes, assistants, and tools.</p>"},{"location":"user-guide/node/","title":"Node","text":"<p>A Node is a discrete component in a graph-based agent system that operates under an event-driven model. Its primary role is to represent its position within a workflow graph, manage event subscriptions, and designate topics for publishing. In addition, it delegates execution to a Command object, adhering to the Command Pattern. Each Node comprises the following elements:</p> <ul> <li>Unique Identity</li> <li>Distinguished by a unique node_id, name, and type.</li> <li>The name must be unique within a given workflow.</li> <li>Subscribed Topics</li> <li>Stores the event topics to which the node subscribes, typically originating from upstream publishers.</li> <li>Subscriptions can reference explicit topic names or apply custom subscription strategies.</li> <li>Publish-To Topics</li> <li>Stores the event topics designated for downstream nodes to subscribe to, facilitating event routing.</li> <li>Command for Execution</li> <li>Encapsulates execution logic through a Command object.</li> <li>Allows integration of new or specialized commands without modifying the node\u2019s existing structure.</li> </ul> <p>Node includes the <code>can_execute()</code> method, which determines whether the Node is ready for execution based on the availability of events in subscribed topics.</p> <p>The following table describes each field within the Node class, highlighting its purpose and usage in the workflow:</p> Field Description <code>node_id</code> A unique identifier for the node instance. <code>name</code> A unique name identifying the node within the workflow. <code>type</code> Defines the category or type of node, indicating its function. <code>command</code> The command object encapsulating node execution logic. <code>oi_span_type</code> Semantic attribute from OpenInference for tracing purposes. <code>subscribed_expressions</code> List of DSL-based subscription expressions used by the node. <code>publish_to</code> List of designated topics the node publishes events to. <code>_subscribed_topics</code> Internal mapping of subscribed topic names to Topic instances. <p>The following table summarizes the methods available in the Node class, highlighting their purpose and intended usage:</p> Method Description <code>execute</code> Executes the node's main operation synchronously. Must be overridden by subclasses. <code>a_execute</code> Asynchronous version of <code>execute</code>, supporting streaming or asynchronous operations. Must be overridden. <code>get_command_input</code> Combines and formats input events appropriately for the node\u2019s execution. Must be overridden. <code>can_execute</code> Evaluates subscription conditions to determine whether the node is ready to execute based on new events. <code>to_dict</code> Serializes node attributes to a dictionary, suitable for persistence or transmission. <p>Developers can customize Nodes to meet specific business requirements. Below are several common examples demonstrating Nodes frequently adapted for various use cases.</p>"},{"location":"user-guide/node/#llmnode","title":"LLMNode","text":"<p>The <code>LLMNode</code> class represents a specialized Node within an event-driven workflow, specifically designed for interactions with Language Models (LLMs). The <code>LLMNode</code> aggregates historical messages from all ancestor nodes, serializes them based on dependencies and timestamps, and provides these messages as context for the Language Model to process.</p>"},{"location":"user-guide/node/#llmnode-class-fields","title":"LLMNode Class Fields","text":"Field Description <code>node_id</code> Unique identifier for the LLMNode instance. <code>name</code> Human-readable name identifying the node (<code>\"LLMNode\"</code> by default). <code>type</code> Specifies the node type (<code>\"LLMNode\"</code>). <code>oi_span_type</code> Semantic attribute from OpenInference indicating tracing semantics. <code>command</code> Command object encapsulating logic for LLM execution. <code>subscribed_expressions</code> List of subscription expressions defining topic dependencies. <code>publish_to</code> Topics designated for events generated by the node. <code>function_specs</code> Specifications of functions provided to enhance LLM capabilities."},{"location":"user-guide/node/#llmnode-class-methods","title":"LLMNode Class Methods","text":"Method Description <code>execute</code> Synchronous method executing the node\u2019s operation; must be overridden in subclasses. <code>a_execute</code> Asynchronous method executing node logic, supporting streaming from LLMs; must be overridden. <code>get_command_input</code> Retrieves, orders, and serializes input messages from ancestor nodes, preparing data for LLM processing. <code>add_function_spec</code> Adds custom function specifications to enhance interactions with the LLM."},{"location":"user-guide/node/#llmfunctioncallnode","title":"LLMFunctionCallNode","text":"<p>The <code>LLMFunctionCallNode</code> class represents a specialized Node within an event-driven workflow designed specifically to handle function calls made by a Language Model (LLM). It is responsible for generating function specifications (<code>function_specs</code>) utilized by upstream LLMNodes. Additionally, the <code>LLMFunctionCallNode</code> processes only messages from parent nodes that contain unprocessed tool call requests.</p>"},{"location":"user-guide/node/#llmfunctioncallnode-class-fields","title":"LLMFunctionCallNode Class Fields","text":"Field Description <code>node_id</code> Unique identifier for the LLMFunctionCallNode instance. <code>name</code> Human-readable identifier for the node (<code>\"LLMFunctionCallNode\"</code> by default). <code>type</code> Specifies the type of node (<code>\"LLMFunctionCallNode\"</code>). <code>oi_span_type</code> Semantic attribute from OpenInference indicating tracing semantics (<code>CHAIN</code>). <code>command</code> Command object encapsulating function call execution logic for LLM interactions."},{"location":"user-guide/node/#llmfunctioncallnode-class-methods","title":"LLMFunctionCallNode Class Methods","text":"Method Description <code>execute</code> Processes synchronous function calls extracted from the LLM response and returns function call results. <code>a_execute</code> Asynchronously executes function calls derived from LLM responses, supporting concurrent operations. <code>get_function_specs</code> Retrieves function specifications that the upstream LLMNodes can use for function call interactions. <code>get_command_input</code> Filters and retrieves input messages containing unprocessed tool call requests from parent nodes. <code>to_dict</code> Serializes the node\u2019s current configuration and state for persistence or debugging."},{"location":"user-guide/node/#execution-flow","title":"Execution Flow","text":"<p>The <code>LLMFunctionCallNode</code> performs the following actions during execution:</p> <ul> <li>Identifies unprocessed tool call messages from parent nodes.</li> <li>Executes function calls based on these identified messages using the provided command.</li> </ul> <p>This structure ensures streamlined and efficient processing of LLM-initiated function calls within complex event-driven workflows.</p>"},{"location":"user-guide/node/#ragnode-and-embeddingretrievalnode","title":"RagNode and EmbeddingRetrievalNode","text":"<p>The <code>EmbeddingRetrievalNode</code> and <code>RagNode</code> classes represent specialized Nodes tailored for specific use cases, such as embedding retrieval and Retrieval-Augmented Generation (RAG). These nodes demonstrate how developers can craft custom nodes and logic tailored to particular requirements.</p>"},{"location":"user-guide/node/#embeddingretrievalnode-class-fields","title":"EmbeddingRetrievalNode Class Fields","text":"Field Description <code>node_id</code> Unique identifier for the EmbeddingRetrievalNode instance. <code>name</code> Human-readable name identifying the node (<code>\"EmbeddingRetrievalNode\"</code>). <code>type</code> Node type specification (<code>\"EmbeddingRetrievalNode\"</code>). <code>oi_span_type</code> Semantic attribute for OpenInference tracing (<code>RETRIEVER</code>). <code>command</code> Command object containing embedding retrieval logic."},{"location":"user-guide/node/#embeddingretrievalnode-class-methods","title":"EmbeddingRetrievalNode Class Methods","text":"Method Description <code>execute</code> Performs synchronous embedding retrieval operations and returns results as messages. <code>a_execute</code> Executes embedding retrieval asynchronously, yielding results in a stream of messages. <code>get_command_input</code> Retrieves the latest message from input events for embedding retrieval queries. <code>to_dict</code> Serializes the node\u2019s configuration and state for persistence or debugging purposes."},{"location":"user-guide/node/#ragnode-class-fields","title":"RagNode Class Fields","text":"Field Description <code>node_id</code> Unique identifier for the RagNode instance. <code>name</code> Human-readable identifier for the node (<code>\"RagNode\"</code>). <code>type</code> Specifies node type (<code>\"RagNode\"</code>). <code>oi_span_type</code> Semantic attribute for OpenInference tracing (<code>RETRIEVER</code>). <code>command</code> Command object encapsulating RAG-specific execution logic."},{"location":"user-guide/node/#ragnode-class-methods","title":"RagNode Class Methods","text":"Method Description <code>execute</code> Performs synchronous RAG operations and returns generated responses as messages. <code>a_execute</code> Executes RAG asynchronously, yielding generated responses in a message stream. <code>get_command_input</code> Retrieves the latest message from input events to serve as input for RAG processing. <code>to_dict</code> Serializes the node\u2019s configuration and current state for persistence or debugging. <p>These specialized Nodes serve as practical examples, guiding developers to extend or customize Nodes for specific, application-driven use cases within event-driven workflows.</p>"},{"location":"user-guide/topics/","title":"Topics","text":"<p><code>TopicBase</code> and <code>Topic</code> represent logical message queues in the event-driven workflow. They temporarily store messages in a First-In-First-Out (FIFO) fashion and track how many messages each consumer has read using an offset system. This allows components\u2014like Nodes, Assistants, or Tools\u2014to communicate asynchronously by publishing and consuming messages.</p>"},{"location":"user-guide/topics/#topicbase","title":"TopicBase","text":"<p><code>TopicBase</code> provides the core interface and data structures for managing published events, consumption offsets, and conditions used to filter which messages are accepted.</p> <p>Fields:</p> Field Description <code>name</code> The topic\u2019s human-readable name. <code>condition</code> A function deciding if incoming messages should be published to this topic. Defaults to True. <code>publish_event_handler</code> An optional callback that runs after a successful publish. <code>topic_events</code> A list of <code>TopicEvent</code> objects representing messages accepted by the topic. <code>consumption_offsets</code> Maps consumer identifiers to the index of the last message they consumed. <p>Methods:</p> Method Description <code>publish_data(...)</code> Publishes data to the topic if it meets the <code>condition</code>. Must be implemented in subclasses. <code>can_consume(consumer_name)</code> Checks if a consumer has unread messages in this topic. <code>consume(consumer_name)</code> Retrieves the unread messages for the consumer, updates its offset, and returns the relevant events. <code>reset()</code> Clears <code>topic_events</code> and <code>consumption_offsets</code>, effectively reverting the topic to its initial state. <code>restore_topic(topic_event)</code> Rebuilds the topic\u2019s state from a <code>TopicEvent</code>, adding to <code>topic_events</code> or adjusting consumption offsets. <code>to_dict()</code> Serializes basic fields like <code>name</code> and <code>condition</code>. <code>serialize_callable()</code> Helper that extracts details about the <code>condition</code> function (e.g., lambda source code or function name). <p><code>TopicBase</code> also includes a builder pattern that simplifies creating and customizing topics (e.g., adding a <code>condition</code>). Subclasses extend <code>publish_data</code>, <code>can_consume</code>, and <code>consume</code> to store and retrieve messages in a more concrete manner.</p>"},{"location":"user-guide/topics/#topic","title":"Topic","text":"<p><code>Topic</code> is a direct subclass of <code>TopicBase</code> that implements the required methods for a working FIFO message queue. Components publish via <code>publish_data</code>, and consumers read new messages via <code>consume</code>, each consumer having an independent offset.</p> <p><code>Topic</code> shares all fields from <code>TopicBase</code> and does not introduce additional fields beyond its default name.</p> <p>Methods:</p> Method Description <code>publish_data(...)</code> Creates a <code>PublishToTopicEvent</code> if the <code>condition</code> is met and calls <code>publish_event_handler</code> to handle event. <code>can_consume(consumer_name)</code> Checks if <code>consumer_name</code>\u2019s offset is behind <code>len(topic_events)</code>, meaning there are new, unread messages. <code>consume(consumer_name)</code> Retrieves unconsumed messages, updates the consumer\u2019s offset, and returns the new events. <p>A typical workflow involves creating a <code>Topic</code> instance (or more specialized subclass), optionally setting a <code>condition</code> to filter messages, and then connecting nodes or assistants that publish or consume messages. Whenever data is published, <code>Topic</code> increments the offset and stores the new event. When a consumer checks <code>can_consume</code>, the topic compares its offset with the total published messages to determine if any remain unread.</p> <p>This design ensures that each consumer reads messages in the correct order, preserving FIFO behavior while enabling asynchronous, distributed interactions across the event-driven workflow.</p>"},{"location":"user-guide/topics/#output-topic","title":"Output Topic","text":"<p><code>OutputTopic</code> is a specialized subclass of <code>Topic</code> designed for user-facing events. When data is published to an <code>OutputTopic</code>, it uses <code>OutputTopicEvent</code> rather than a standard <code>PublishToTopicEvent</code>, indicating that these messages should ultimately be returned to the user.</p> <p>Fields:</p> Field Description <code>name</code> Defaults to <code>AGENT_OUTPUT_TOPIC</code>, representing the system\u2019s standard output channel. <code>publish_event_handler</code> An optional callback that executes whenever an <code>OutputTopicEvent</code> is successfully published. <code>topic_events</code> A list of <code>OutputTopicEvent</code> objects, maintaining the published output messages in FIFO order. <code>consumption_offsets</code> Maps consumer identifiers (e.g., assistant names) to the last read event offset, ensuring each reads in order. <p>Methods:</p> Method Description <code>publish_data(...)</code> Creates an <code>OutputTopicEvent</code> with the given messages if the <code>condition</code> is met. Append event to topic, then call handler. <code>_publish(event)</code> Inherited from <code>TopicBase</code>; assigns an offset and appends the event to <code>topic_events</code> if allowed by <code>condition</code>. <p>Use Case:</p> <p>Typically, an assistant consumer will subscribe to the OutputTopic to retrieve user-facing results. By separating output into a dedicated topic, the system can more easily track final responses, funneling them back to the user through consistent workflows.</p>"},{"location":"user-guide/topics/#human-request-topic","title":"Human Request Topic","text":"<p><code>HumanRequestTopic</code> is a specialized extension of <code>Topic</code> dedicated to handling requests that require human intervention or input. When the workflow needs user input, it publishes an <code>OutputTopicEvent</code> to <code>HumanRequestTopic</code>. On the user\u2019s response, that input is appended back to the same topic, keeping the entire request-response cycle self-contained.</p> <p>Fields:</p> Field Description <code>name</code> Defaults to <code>HUMAN_REQUEST_TOPIC</code>, indicating it\u2019s the main channel for human-driven requests. <code>publish_to_human_event_handler</code> A callback triggered after successfully publishing an <code>OutputTopicEvent</code> for user-facing interactions. <code>topic_events</code> A list of <code>TopicEvent</code> (or <code>OutputTopicEvent</code>) objects, preserving a history of user requests and appended responses. <code>consumption_offsets</code> Maps consumer identifiers to the offset of the last read event, enabling a FIFO workflow for multiple consumers. <p>Methods:</p> Method Description <code>publish_data(...)</code> Publishes data to the topic as an <code>OutputTopicEvent</code> and add to topic if <code>condition</code> is met. Then invoke <code>publish_to_human_event_handler</code> <code>can_append_user_input(consumer_name, event)</code> Check if can add the user input event given its parent <code>PublishToTopicEvent</code>. <code>append_user_input(user_input_event, data)</code> Appends actual user responses using a standard <code>PublishToTopicEvent</code>, ensuring they become available for downstream nodes. <p>Usage:</p> <ol> <li>Publishing Requests: When a node or another component needs user input, it calls <code>publish_data(...)</code> on <code>HumanRequestTopic</code>, generating an <code>OutputTopicEvent</code>. This signals the assistant to display or relay a query to the user.</li> <li>Appending User Input: After the user responds, the assistant (or another client) calls <code>append_user_input(...)</code>, creating a <code>PublishToTopicEvent</code> that effectively stores the user\u2019s messages in the same topic.</li> <li>Downstream Consumption: Any node subscribed to the <code>HumanRequestTopic</code> can consume new messages as they appear, enabling further automated logic once the user\u2019s response is available.</li> </ol> <p>Rational:</p> <p>By splitting user interaction into distinct publish and append steps, the system provides a clear interface for capturing requests and responses, all under a single, specialized topic designed for human-driven workflows.</p>"},{"location":"user-guide/topics/#topic-expression","title":"Topic Expression","text":"<p>Topic Expression provides a mini DSL (Domain-Specific Language) for building complex subscription logic based on multiple topics. By combining topic references using logical operators (AND, OR), you can specify whether a node should wait for messages in all required topics (<code>AND</code>) or at least one of several possible topics (<code>OR</code>). This approach offers a flexible way to manage event-driven subscriptions.</p> <p>Models</p> <p><code>LogicalOp</code></p> Enum Value Description <code>AND</code> Both sides must be satisfied for expression <code>OR</code> At least one side must be satisfied <p><code>SubExpr</code> (Base Class)</p> Class Description <code>SubExpr</code> Abstract base class for any subscription expression. <p><code>TopicExpr</code> (extended from <code>SubExpr</code>)</p> Field Description <code>topic</code> A <code>TopicBase</code> object representing a single topic in the subscription tree. <p><code>TopicExpr</code> states that a subscriber is interested in a single topic. If new, unread messages exist in that topic, the expression evaluates to <code>True</code>.</p> <p><code>CombinedExpr</code> (extended from <code>SubExpr</code>)</p> Field Description <code>op</code> A <code>LogicalOp</code> indicating <code>AND</code> or <code>OR</code>. <code>left</code> Another <code>SubExpr</code> node. <code>right</code> Another <code>SubExpr</code> node. <p><code>CombinedExpr</code> composes two sub-expressions with a logical operator, enabling complex nested conditions.</p> <p>Methods</p> Method Description <code>evaluate_subscription(expr, topics_with_new_msgs)</code> Checks whether a subscription expression (<code>expr</code>) is fulfilled by the given list of topics that have new messages. Returns <code>True</code> if the condition is met (based on <code>AND</code>/<code>OR</code> logic). <code>extract_topics(expr)</code> Recursively collects all <code>TopicBase</code> objects from the DSL expression tree, letting the system know which topics a node depends on. <p>Key Points:</p> <ol> <li>Flexibility: You can nest multiple expressions to create complex logic. For instance, <code>(TopicA AND (TopicB OR TopicC))</code>.</li> <li>Maintainability: By separating subscription logic into DSL expressions, the system remains clear and easy to debug.</li> <li>Integration: Each <code>TopicExpr</code> references an actual <code>TopicBase</code>, ensuring that the DSL and the underlying queue system stay in sync.</li> </ol>"},{"location":"user-guide/topics/#subscription-builder","title":"Subscription Builder","text":"<p><code>SubscriptionBuilder</code> streamlines the process of creating complex topic subscription expressions, allowing you to chain logical operations (<code>AND</code>, <code>OR</code>) and define whether a node requires messages from multiple topics or at least one. This builder pattern provides a concise DSL for specifying these conditions without manually constructing <code>TopicExpr</code> and <code>CombinedExpr</code> objects.</p> <p>Fields:</p> Field Description <code>root_expr</code> The current root of the subscription expression tree (<code>SubExpr</code>), built incrementally by chaining. <code>pending_op</code> A <code>LogicalOp</code> (AND/OR) that awaits completion of the next <code>subscribed_to(...)</code> call. <p>Methods:</p> Method Description <code>subscribed_to(topic: TopicBase)</code> Adds a new <code>TopicExpr</code> node referencing <code>topic</code>. If <code>pending_op</code> is set, combines it with the existing <code>root_expr</code> via a <code>CombinedExpr</code>. <code>and_()</code> Sets <code>pending_op</code> to <code>LogicalOp.AND</code>, indicating the next topic reference should form an AND relationship. <code>or_()</code> Sets <code>pending_op</code> to <code>LogicalOp.OR</code>, indicating the next topic reference should form an OR relationship. <code>build()</code> Finalizes the builder, returning the constructed <code>SubExpr</code>. <p>Usage Example:</p> <pre><code>\n# Suppose you have two Topic objects: topicA and topicB\n# Build an expression: (topicA AND topicB)\nsubscription_expr = (\n    SubscriptionBuilder()\n    .subscribed_to(topicA)\n    .and_()\n    .subscribed_to(topicB)\n    .build()\n)\n\n# The resulting expression can be assigned to a node, which then requires new messages from both topics.\nnode_builder.subscribed_to(subscription_expr)\n</code></pre> <p>Key Points:</p> <ol> <li>Chained Syntax: The builder pattern enables a straightforward DSL-like syntax: <code>.subscribed_to(topicA).and_().subscribed_to(topicB).build()</code>.</li> <li>Operator Checks: If <code>and_()</code> or <code>or_()</code> is called without a subsequent <code>subscribed_to(...)</code>, or vice versa, a <code>ValueError</code> is raised.</li> <li>Integration: Once created, the resulting <code>SubExpr</code> can be evaluated against incoming messages with <code>evaluate_subscription()</code> or used for introspection with <code>extract_topics()</code>. This provides flexible, powerful subscription logic for nodes in an event-driven system.</li> </ol>"},{"location":"user-guide/topics/#reserved-topics","title":"Reserved Topics","text":"<p>These topics are reserved for essential system operations in the event-driven workflow, ensuring consistent handling of inputs, outputs, and user-interactive events.</p> <ol> <li>Input Topic<ul> <li><code>agent_input_topic</code>: Receives user or external inputs, starting the workflow by providing initial messages or commands for further processing.</li> </ul> </li> <li>Output Topics<ul> <li><code>agent_stream_output_topic</code>: Streams partial or incremental responses during long-running or asynchronous operations. Typically used for real-time updates.</li> <li><code>agent_output_topic</code>: Publishes final agent responses that are ready to be returned to the user or external systems. The <code>Assistant</code> is the only consumer of this topic.</li> </ul> </li> <li>Human Request Topic<ul> <li><code>human_request_topic</code>: A special topic for user involvement. When the system needs additional information or confirmation from humans, it posts requests here; once the user responds, messages are appended to the same topic and become available for downstream processing.</li> </ul> </li> </ol> <p>Using these reserved topics helps maintain a clear, consistent architecture for input processing, output streaming, final responses, and human-driven request handling. They are key building blocks for standardizing communication across the workflow.</p>"},{"location":"user-guide/topics/#event-graph","title":"Event Graph","text":"<p><code>EventGraph</code> organizes events (particularly <code>ConsumeFromTopicEvent</code> and <code>PublishToTopicEvent</code>) into a directed graph structure. It traces how messages flow from published events through consumed ones, enabling advanced operations like retrieving a sorted sequence of all ancestor messages. This is especially important for Large Language Model (LLM) interactions, where the full conversation history (including intermediate nodes) must be serialized in a coherent, chronological order.</p>"},{"location":"user-guide/topics/#fields","title":"Fields","text":"Field Description <code>nodes</code> A dictionary mapping event IDs to <code>EventGraphNode</code> objects. <code>root_nodes</code> A list of <code>EventGraphNode</code> objects representing the starting points (e.g., directly consumed events)."},{"location":"user-guide/topics/#methods","title":"Methods","text":"Method Description <code>_add_event(event)</code> Creates a new <code>EventGraphNode</code> for a given <code>Event</code> if it does not already exist. <code>build_graph(consume_events, topic_events)</code> Constructs the event graph from a list of consume events and a dictionary of topic events. It links each consume event to its corresponding publish event, building upstream/downstream refs. <code>get_root_event_nodes()</code> Returns the root nodes, i.e., the events that begin sub-graphs (often direct consume events). <code>get_topology_sorted_events()</code> Performs a custom topological sort, ordering events by reverse timestamp within each dependency layer, and then reversing the result for ascending chronological output. <code>to_dict()</code> Serializes the entire graph, including each node\u2019s event and references. <code>from_dict(...)</code> Deserializes the graph from a dictionary, recreating each <code>EventGraphNode</code>."},{"location":"user-guide/topics/#rationale-for-topological-and-timestamp-sorting","title":"Rationale for Topological and Timestamp Sorting","text":"<p>When feeding conversation or workflow history to an LLM, it\u2019s crucial to maintain logical and temporal ordering of all ancestor events. By combining topological ordering with timestamp-based sorting, the <code>EventGraph</code> ensures:</p> <ol> <li>Correct Causality: Dependencies (publish -&gt; consume) appear before reliant events.</li> <li>Chronological Consistency: Events with similar dependency levels are ordered by their actual creation time.</li> <li>Complete Context: The LLM receives a fully serialized token sequence of all ancestor interactions, enabling more coherent responses.</li> </ol> <p>By leveraging the <code>EventGraph</code> class, developers can reliably trace the chain of message publications and consumptions, producing a robust representation of the workflow\u2019s complete ancestry\u2014critical for advanced LLM tasks or debugging complex distributed processes.</p>"},{"location":"user-guide/tools/function-call/","title":"Function Calls","text":""},{"location":"user-guide/tools/function-call/#functioncalltool","title":"FunctionCallTool","text":"<p><code>FunctionCallTool</code> is designed to allow Language Models (LLMs) to invoke specific Python functions directly through JSON-formatted calls. When a message from the LLM references a particular function name along with arguments, <code>FunctionCallTool</code> checks if it has a function matching that name and, if so, invokes it.</p> <p>This design greatly reduces the complexity of integrating advanced logic: the LLM simply issues a request to invoke a function, and the tool handles the invocation details behind the scenes.</p>"},{"location":"user-guide/tools/function-call/#fields","title":"Fields","text":"Field Description <code>name</code> Descriptive identifier (defaults to <code>\"FunctionCallTool\"</code>). <code>type</code> Tool type (defaults to <code>\"FunctionCallTool\"</code>). <code>function_specs</code> Captures metadata describing the registered function, such as parameter definitions. <code>function</code> The actual callable that <code>FunctionCallTool</code> invokes when a function call matches <code>function_specs</code>. <code>oi_span_type</code> Semantic tracing attribute (<code>TOOL</code>) for observability."},{"location":"user-guide/tools/function-call/#methods","title":"Methods","text":"Method Description <code>function</code> (Builder) Builder method to register a function. Automatically applies <code>@llm_function</code> if not already decorated. <code>register_function</code> Assigns a function to this tool, generating function specs if missing. <code>get_function_specs</code> Retrieves detailed metadata about the function (including parameter info), enabling structured LLM-based function calls. <code>execute</code> Evaluates whether incoming messages match the registered function\u2019s name and, if so, calls it with the JSON arguments. <code>a_execute</code> Asynchronous equivalent to <code>execute</code>, allowing concurrency if the function is a coroutine. <code>to_message</code> Converts execution results into a <code>Message</code> object, preserving context like the <code>tool_call_id</code>. <code>to_dict</code> Serializes the <code>FunctionCallTool</code> instance, listing function specifications for debugging or persistence."},{"location":"user-guide/tools/function-call/#how-it-works","title":"How It Works","text":"<ol> <li>Function Registration: A Python function is wrapped or decorated using <code>@llm_function</code>. This generates a schema (<code>function_specs</code>) describing its name, arguments, and docstring.</li> <li>Invocation: When a message arrives specifying a function call, <code>FunctionCallTool</code> checks whether it corresponds to the registered function\u2019s name.</li> <li>JSON Parsing: The arguments are parsed from the <code>tool_call</code> field. If they match, the tool dispatches the function call with the given parameters.</li> <li>Response: After execution, the returned data is converted into a new <code>Message</code>, allowing the workflow to process the function\u2019s output seamlessly.</li> </ol>"},{"location":"user-guide/tools/function-call/#usage-and-customization","title":"Usage and Customization","text":"<ul> <li>Builder Pattern: Use the builder\u2019s <code>.function(...)</code> method to assign the function you want to expose. This ensures your function is properly decorated if not already.</li> <li>Flexible: By simply swapping out the underlying callable, you can quickly adapt to new or updated logic without modifying the rest of the workflow.</li> <li>Observability: Because <code>FunctionCallTool</code> implements the <code>Tool</code> interface and integrates with the event-driven architecture, all executions can be monitored and logged.</li> </ul> <p>With <code>FunctionCallTool</code>, you can integrate specialized Python functions into an LLM-driven workflow with minimal extra overhead. As your system grows and evolves, it provides a clean way to add or modify functionality while retaining a uniform interaction pattern with the LLM.</p>"},{"location":"user-guide/tools/function-call/#agent-calling-tool","title":"Agent Calling Tool","text":"<p><code>AgentCallingTool</code> extends the <code>FunctionCallTool</code> concept to enable multi-agent systems, allowing an LLM to call another agent by name, pass relevant arguments (as a message prompt), and return the agent\u2019s response as part of the workflow.</p> <p>Fields:</p> Field Description <code>name</code> Descriptive identifier, defaults to <code>\"AgentCallingTool\"</code>. <code>type</code> Tool type indicator, defaults to <code>\"AgentCallingTool\"</code>. <code>agent_name</code> Name of the agent to call; also used as the tool\u2019s name. <code>agent_description</code> High-level explanation of what the agent does, used to generate function specs. <code>argument_description</code> Describes the argument required (e.g., <code>prompt</code>) for the agent call. <code>agent_call</code> A callable that takes <code>(execution_context, Message)</code> and returns a dictionary (e.g., <code>{\"content\": ...}</code>). <code>oi_span_type</code> OpenInference semantic attribute (<code>TOOL</code>), enabling observability and traceability. <p>Methods:</p> Method Description <code>get_function_specs</code> Returns the function specification (name, description, parameters) for the agent call. <code>execute</code> Synchronously processes incoming tool calls that match <code>agent_name</code>, passing the <code>prompt</code> to the <code>agent_call</code> callable and returning a list of <code>Message</code> objects. <code>a_execute</code> Asynchronous variant of <code>execute</code>; yields messages in an async generator for real-time or concurrent agent calls. <code>to_message</code> Creates a <code>Message</code> object from the agent\u2019s response, linking the output to <code>tool_call_id</code>. <code>to_dict</code> Serializes all relevant fields, including agent metadata and the assigned callable, for debugging or persistence. <p>Here is the workflow example:</p> <ol> <li>Tool Registration: An <code>AgentCallingTool</code> is constructed with details about the agent (<code>agent_name</code>, <code>agent_description</code>, etc.) and the callable (<code>agent_call</code>).</li> <li>Agent Invocation: When an LLM includes a tool call referencing this agent\u2019s name, <code>execute</code> or <code>a_execute</code> receives the <code>prompt</code> and calls the agent.</li> <li>Response Conversion: The agent\u2019s return value is formed into a new <code>Message</code>, which the workflow can then process or forward.</li> </ol> <p>The usage and customization are:</p> <ul> <li>Multi-Agent Systems: By configuring multiple <code>AgentCallingTool</code> instances, you can facilitate dynamic exchanges among multiple agents, each specializing in a different task.</li> <li>Runtime Flexibility: Changing or updating the underlying <code>agent_call</code> logic requires no changes to the rest of the workflow.</li> <li>Parameter Schemas: <code>argument_description</code> ensures the LLM knows which arguments are required and how they should be formatted.</li> </ul> <p>By integrating <code>AgentCallingTool</code> into your event-driven workflow, you can build sophisticated multi-agent systems where each agent can be invoked seamlessly via structured function calls. This approach maintains a clear separation between the LLM\u2019s orchestration and the agents\u2019 execution details.</p>"},{"location":"user-guide/tools/function-call/#example-weather-mock-tool","title":"Example - Weather Mock Tool","text":"<p>A simple mock implementation of a weather service tool that inherits from <code>FunctionCallTool</code>. This class provides a straightforward way to use <code>FunctionCallTool</code>. It is easy to use - just instantiate and call the method. And implements the <code>FunctionCallTool</code> interface for seamless integration. Uses <code>@llm_function</code> decorator for automatic registering function.</p> <p><code>@llm_function</code> is a decorator that enables your Python functions to be seamlessly called by a Language Model (LLM). By inspecting type hints, parsing docstrings, and inferring parameter definitions, this decorator automatically constructs a <code>FunctionSpec</code> object that describes your function\u2019s name, parameters (including default values and descriptions), and return type. It then attaches this metadata to the decorated function, making it discoverable and callable within an LLM-driven workflow.</p> <p>In practical terms, <code>@llm_function</code> allows an LLM to dynamically invoke your function with structured, JSON-based arguments. As a result, you can integrate arbitrary Python functions into your dialogue or workflow system without manually encoding parameter details, ensuring consistent and accurate function calls.</p> <pre><code>class WeatherMock(FunctionCallTool):\n\n    @llm_function\n    async def get_weather_mock(self, postcode: str):\n        \"\"\"\n        Function to get weather information for a given postcode.\n\n        Args:\n            postcode (str): The postcode for which to retrieve weather information.\n\n        Returns:\n            str: A string containing a weather report for the given postcode.\n        \"\"\"\n        return f\"The weather of {postcode} is bad now.\"\n</code></pre>"},{"location":"user-guide/tools/function-call/#example-tavily-search-tool","title":"Example - Tavily Search Tool","text":"<p>TavilyTool extends FunctionCallTool to provide web search capabilities through the Tavily API. In general, when the tool will be reused and needs more complex construction, you can create a class with a builder pattern and apply <code>@llm_function</code> to the function that will be called by the LLM. By adding the <code>@llm_function</code> decorator to <code>web_search_using_tavily</code>, you can integrate web search logic into an LLM-driven workflow with minimal extra configuration.</p> <p>TavilyTool fields:</p> Field Description <code>name</code> Descriptive identifier for the tool (default: <code>\"TavilyTool\"</code>). <code>type</code> Tool type indicator (default: <code>\"TavilyTool\"</code>). <code>client</code> Instance of the <code>TavilyClient</code> used for performing search queries. <code>search_depth</code> Defines the search mode (either <code>\"basic\"</code> or <code>\"advanced\"</code>) for Tavily. <code>max_tokens</code> Limits the total size (in tokens) of the returned JSON string, preventing overly large responses. <p><code>web_search_using_tavily</code> is decorated with <code>@llm_function</code>, so it can be invoked by an LLM using structured arguments. It calls the Tavily API with the specified query, search depth, and maximum results, then returns a JSON string containing relevant matches. The method also checks for maximum token usage before appending items to the output.</p> <p>Usage example:</p> <ol> <li>Instantiate the builder:</li> </ol> <pre><code>tavily_tool = (\n    TavilyTool.Builder()\n    .api_key(\"YOUR_API_KEY\")\n    .search_depth(\"advanced\")\n    .max_tokens(6000)\n    .build()\n)\n</code></pre> <ol> <li>A node in your workflow references <code>TavilyTool</code> by name and calls <code>web_search_using_tavily</code> when requested by the LLM.</li> <li>The LLM sends a JSON function call containing <code>query</code> and <code>max_results</code>; TavilyTool executes the query and returns JSON-based results.</li> </ol> <p>You can customize TavilyTool by extending <code>web_search_using_tavily</code> with additional parameters or logic. This approach maintains a clean, unified interface for integrating search capabilities into an event-driven or node-based workflow.</p>"},{"location":"user-guide/tools/function-call/#customized-tools","title":"Customized Tools","text":"<p>When your requirements exceed what <code>FunctionCallTool</code> can provide, you can implement a custom tool within the framework, ensuring your specialized logic and configuration remain fully integrated into the event-driven workflow.</p> <p>Here are two examples</p>"},{"location":"user-guide/tools/function-call/#retrievaltool","title":"RetrievalTool","text":"<p><code>RetrievalTool</code> defines a base interface for embedding-based lookups in an event-driven workflow. It inherits from <code>Tool</code> and introduces an <code>embedding_model</code> field for custom embedding generation. By default, <code>RetrievalTool</code> provides a builder pattern so you can assign an embedding model before instantiation. When the required functionality surpasses this base retrieval capability, you can extend or subclass <code>RetrievalTool</code> for more specialized use cases.</p> <p>The <code>ChromadbRetrievalTool</code> is a concrete subclass of <code>RetrievalTool</code>, tailored for queries against a ChromaDB collection. It uses an <code>OpenAIEmbedding</code> model (or any suitable <code>OpenAIEmbedding</code> subclass) to transform input text into vector embeddings, which are then passed to the ChromaDB collection for similarity matching. During <code>execute</code> or <code>a_execute</code>, the tool retrieves the most relevant documents by comparing the user\u2019s query embedding against stored embeddings in ChromaDB. The resulting matches are serialized into a <code>Message</code> object, making the data seamlessly available to the rest of the workflow. Because it inherits from <code>RetrievalTool</code>, you can still configure or replace the embedding model as needed.</p> <p>RetrievalTool fields:</p> Field Description <code>name</code> Tool name (default: <code>\"RetrievalTool\"</code>). <code>type</code> Type identifier (default: <code>\"RetrievalTool\"</code>). <code>embedding_model</code> Any embedding model (e.g., OpenAIEmbedding) used to encode text for retrieval. <code>oi_span_type</code> Specifies an OpenInference span type (<code>RETRIEVER</code>), useful for tracing. <p>ChromadbRetrievalTool fields:</p> Field Description <code>name</code> Tool name (default: <code>\"ChromadbRetrievalTool\"</code>). <code>type</code> Type identifier (default: <code>\"ChromadbRetrievalTool\"</code>). <code>collection</code> A ChromaDB <code>Collection</code> for storing and querying document embeddings. <code>embedding_model</code> An instance of <code>OpenAIEmbedding</code> used to generate embeddings from user queries. <code>n_results</code> Maximum number of results to return when querying ChromaDB. <code>oi_span_type</code> Specifies an OpenInference span type (<code>RETRIEVER</code>), useful for tracing. <p>Typical usage involves creating an instance of either tool via its builder, providing any required models or indexes. When an input <code>Message</code> arrives, the tool encodes the message text using the configured embedding model, queries the retrieval backend (generic or ChromaDB), and returns a <code>Message</code> with the matched results. As part of an event-driven workflow, these matches can then be consumed by subsequent nodes or logic.</p>"},{"location":"user-guide/tools/function-call/#ragtool","title":"RagTool","text":"<p><code>RagTool</code> is used for <code>RagNode</code>, providing a specialized <code>Tool</code> for Retrieval-Augmented Generation (RAG) use cases. It integrates with <code>llama_index</code> via a <code>BaseIndex</code> instance, allowing your workflow to query stored data or documents and incorporate those results into a context-aware response. Ideal for knowledge-intensive tasks, <code>RagTool</code> seamlessly translates user queries into an index lookup, returning relevant information as a <code>Message</code>.</p> <p>Fields:</p> Field Description <code>name</code> Identifier for the tool (default: <code>\"RagTool\"</code>). <code>type</code> Type of the tool (default: <code>\"RagTool\"</code>). <code>index</code> A <code>BaseIndex</code> instance from llama_index for retrieving relevant data. <code>oi_span_type</code> An OpenInference semantic attribute indicating the retriever type (<code>RETRIEVER</code>). <p>Execution flow:</p> <ol> <li><code>execute</code> or <code>a_execute</code> transforms incoming messages into queries against the assigned <code>BaseIndex</code>. For synchronous calls, <code>execute</code> returns results immediately; <code>a_execute</code> uses asynchronous logic.</li> <li><code>as_query_engine()</code> fetches the relevant documents from the index.</li> <li><code>to_message</code> converts the query result into a <code>Message</code>, enabling the rest of the workflow to consume the retrieved information.</li> </ol> <p>Usage example:</p> <pre><code>rag_tool = (\n    RagTool.Builder()\n    .index(your_llama_index)  # Where your_llama_index is an instance of BaseIndex\n    .build()\n)\n\n# In your workflow, supply `rag_tool` with a user query message.\n# The tool will query `your_llama_index` and return a Message with the result.\n</code></pre> <p>Methods:</p> Method Description <code>execute</code> Synchronously queries the index using the input message\u2019s <code>content</code> as a query. <code>a_execute</code> Asynchronous version of <code>execute</code>; returns the result in an async generator. <code>to_message</code> Converts the response from the query engine to a <code>Message</code> object, enabling uniform workflow consumption. <code>to_dict</code> Provides a dictionary representation of the tool, including its fields and the index class name. <p>With <code>RagTool</code>, you can incorporate advanced document retrieval capabilities into your node-based workflows, providing context-rich responses sourced from external knowledge bases while maintaining a clean separation between data storage and LLM-driven logic.</p>"},{"location":"user-guide/tools/ollama/","title":"Ollama","text":"<p>Similar to OpenAI tool, <code>OllamaTool</code> is an implementation of the <code>LLM</code> interface designed to interface with Ollama\u2019s language model API. It supports synchronous and asynchronous execution patterns, converting workflow <code>Message</code> objects into an Ollama-compatible format and translating API responses back into the workflow.</p> <p>Fields:</p> Field Description <code>name</code> Descriptive identifier for the tool (defaults to <code>\"OllamaTool\"</code>). <code>type</code> Tool type indicator (defaults to <code>\"OllamaTool\"</code>). <code>api_url</code> URL of the Ollama API endpoint (defaults to <code>\"http://localhost:11434\"</code>). <code>model</code> Ollama model name (defaults to <code>\"qwen3\"</code>). <p>Methods:</p> Method Description <code>prepare_api_input</code> Adapts the list of <code>Message</code> objects to match Ollama\u2019s expected input format, including function calls if present. <code>execute</code> Synchronously calls the Ollama API, returning a <code>Message</code> with the resulting content or function calls. <code>a_execute</code> Asynchronously calls the Ollama API, yielding a <code>Message</code> in an async generator for real-time processing. <code>to_message</code> Converts Ollama\u2019s raw API response into a <code>Message</code>, supporting function call data when present. <code>to_dict</code> Provides a dictionary representation of the <code>OllamaTool</code> configuration. <p>This tool can be configured with its internal <code>Builder</code> class, allowing customization of fields such as the <code>api_url</code> or <code>model</code> before constructing an instance. By integrating <code>OllamaTool</code> into the workflow, developers can leverage local or remote Ollama services without altering the overarching event-driven logic. Messages from the workflow are passed to Ollama, and responses are returned in a consistent format, preserving a clear separation between orchestration and execution logic.</p>"},{"location":"user-guide/tools/openai/","title":"OpenAI","text":"<p><code>OpenAITool</code> is a concrete implementation of the <code>LLM</code> interface, integrating directly with OpenAI\u2019s language model APIs. It supports synchronous and asynchronous interactions, as well as streaming responses for real-time experience.</p> <p>The OpenAI tool fields are</p> Field Description <code>name</code> Name of the tool (inherited from <code>LLM</code>, defaults to <code>\"OpenAITool\"</code>). <code>type</code> Type indicator for this tool (inherited from <code>LLM</code>, defaults to <code>\"OpenAITool\"</code>). <code>api_key</code> API key required to authenticate with OpenAI\u2019s services. <code>model</code> Model name used for OpenAI API calls (defaults to <code>\"gpt-4o-mini\"</code>). <code>chat_params</code> Additional optional chat completion parameters <p>The OpenAI tool methods are</p> Method Description <code>prepare_api_input</code> Adapts the list of <code>Message</code> objects to match the input schema expected by OpenAI\u2019s API, optionally extracting function tools from the latest message. <code>execute</code> Synchronous method that calls the OpenAI API using the prepared input, returning a single <code>Message</code> as the response. <code>a_execute</code> Asynchronous version of <code>execute</code>, returning responses using an async generator for concurrent or streaming workflows. <code>stream</code> Deprecated synchronous streaming method that yields partial token results as they become available. <code>a_stream</code> Asynchronous streaming method that yields partial token results, useful for real-time applications. <code>to_stream_message</code> Converts partial response chunks (<code>ChatCompletionChunk</code>) from OpenAI\u2019s streaming API into a <code>Message</code> object. <code>to_message</code> Converts a fully realized response (<code>ChatCompletion</code>) from OpenAI\u2019s API into a single <code>Message</code> object. <code>to_dict</code> Serializes <code>OpenAITool</code> configuration, hiding the <code>api_key</code> for security. <p>It will take 3 main steps to finish a request. They are</p> <ol> <li>Prepare Input: Using <code>prepare_api_input</code>, the tool converts incoming messages and any associated function specifications into the OpenAI-compatible format.</li> <li>Execute or Stream: Depending on whether you call <code>execute</code>/<code>a_execute</code> or <code>stream</code>/<code>a_stream</code>, the tool invokes OpenAI\u2019s API to generate a response, optionally streaming tokens.</li> <li>Response Conversion: Partial or complete responses are converted into <code>Message</code> objects via <code>to_stream_message</code> or <code>to_message</code>, enabling uniform handling across the workflow.</li> </ol> <p>When create a openai tool, consider following</p> <ul> <li>Builder Pattern: Use the <code>Builder</code> class to specify the API key and model before building an <code>OpenAITool</code> instance.</li> <li>Model Customization: Configure the <code>model</code> field (e.g., <code>\"gpt-4\"</code>, <code>\"gpt-4o-mini\"</code>) to target specific OpenAI model endpoints.</li> <li>Key Management: Provide the <code>api_key</code> either as an environment variable (<code>OPENAI_API_KEY</code>) or explicitly through the builder.</li> <li>Streaming: For real-time or large-scale tasks, leverage <code>a_stream</code> to handle partial responses incrementally.</li> </ul> <p>By integrating <code>OpenAITool</code> into your node-based workflows, you can seamlessly introduce advanced language model capabilities powered by OpenAI, maintaining consistency and modularity throughout the system.</p>"},{"location":"user-guide/tools/tools/","title":"Tools Core","text":"<p>In our platform, Tools represent the execution components within a workflow. A Tool is essentially a function designed to transform input data into output based on specified rules or logic. Tools can encompass interactions with Language Models (LLMs), external API calls, or purely deterministic functions. Crucially, Tools operate independently of the workflow context\u2014they are unaware of the invoking node or their position within the workflow graph. Each Tool strictly adheres to a defined schema, processing a list of <code>Message</code> objects as input and returning a list of <code>Message</code> objects as output.</p> <p>The following table describes each field within the Tool interface class</p> Field Description <code>tool_id</code> Unique identifier assigned to each Tool instance. <code>name</code> Human-readable identifier for the Tool. <code>type</code> Specifies the category or nature of the Tool. <code>oi_span_type</code> Semantic attribute from OpenInference used for tracing and observability. <p>The following table describes each method within the Tool interface class</p> Method Description <code>execute</code> Synchronously processes input messages according to the Tool's logic and returns a result message. <code>a_execute</code> Asynchronously processes input messages, typically used for streaming or concurrent operations. <code>to_message</code> Converts the Tool's raw response into a standardized <code>Message</code> object. <code>to_dict</code> Serializes the Tool instance into a dictionary format for persistence or debugging. <p>Developers can implement custom Tools tailored to specific business logic or operational requirements. By following the clearly defined interface, new Tools can seamlessly integrate into existing workflows, enhancing modularity, extensibility, and maintainability of the overall system.</p> <p>Here we introduce some build in tools with corresponding command implementation.</p>"},{"location":"user-guide/tools/tools/#llmtool-interface-and-openaitool-implementation","title":"LLMTool Interface and OpenAITool Implementation","text":"<p>The LLM class is a specialized <code>Tool</code> designed to interface with Language Model (LLM) services such as OpenAI, Claude, or other third-party providers. It provides both synchronous and asynchronous streaming options, making it suitable for various real-time or batch processing scenarios. By adhering to the base <code>Tool</code> interface, it remains compatible with the broader event-driven workflow and command pattern used throughout the system.</p>"},{"location":"user-guide/tools/tools/#fields","title":"Fields","text":"Field Description <code>tool_id</code> Unique identifier for the LLM tool instance (inherited from <code>Tool</code>). <code>name</code> Human-readable name identifying the LLM tool (inherited from <code>Tool</code>). <code>type</code> Specifies the type of the tool (inherited from <code>Tool</code>). <code>system_message</code> An optional system or instructional message to guide the LLM\u2019s behavior. <code>oi_span_type</code> Semantic attribute from OpenInference used for tracing, specifically set to <code>LLM</code>."},{"location":"user-guide/tools/tools/#methods","title":"Methods","text":"Method Description <code>execute</code> Implements the core logic for synchronous requests (inherited from <code>Tool</code>). Must be overridden by subclasses. <code>a_execute</code> Implements the core logic for asynchronous requests (inherited from <code>Tool</code>). Must be overridden. <code>stream</code> Provides synchronous streaming functionality, yielding LLM responses as they become available. <code>a_stream</code> Provides asynchronous streaming functionality, useful for real-time or larger-scale deployments. <code>prepare_api_input</code> Prepares input data (list of messages) to match the expected format of the LLM API. <code>to_dict</code> Serializes the LLM tool\u2019s current configuration into a dictionary format."},{"location":"user-guide/tools/tools/#usage-and-customization","title":"Usage and Customization","text":"<ul> <li>Subclasses: To implement a concrete LLM tool, create a subclass of <code>LLM</code> and override <code>execute</code>, <code>a_execute</code>, <code>stream</code>, and <code>a_stream</code> methods. This allows for integration with various LLM providers (e.g., OpenAI, Claude) while following a consistent interface.</li> <li>System Message: You can specify a <code>system_message</code> to influence the tone or purpose of the LLM\u2019s responses. This is particularly useful for role-based messaging systems or specialized tasks.</li> <li>API Input Preparation: Use <code>prepare_api_input</code> to adapt your workflow messages into the required schema for each LLM provider\u2019s endpoint, making integration with new or changing APIs more flexible.</li> </ul> <p>By adhering to the <code>Tool</code> interface and focusing on LLM operations, the <code>LLM</code> class bridges message-based workflows with language model services, ensuring a clean separation of concerns and streamlined integration into the rest of the system.</p>"},{"location":"user-guide/tools/tools/#streaming","title":"Streaming","text":"<p>We added two stream interface for LLM, adding the user friendly interaction with the agent. However, when we use the stream in assistant, we only use a_stream for async capability.</p>"}]}