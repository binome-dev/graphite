{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>Graphite is an open-source framework for creating domain-specific AI assistants via composable, agentic workflows. It emphasizes loose coupling and well-defined interfaces, enabling developers to construct flexible, modular systems. Each major layer \u2013 assistant, node, tool, and workflow \u2013 has a clear role in orchestrating or executing tasks, with events serving as the single source of truth for every state change or data exchange.</p> <p>This documentation details how Graphite's event-driven architecture seamlessly supports complex business logic, from initial user requests through advanced tool integrations (e.g., LLM calls, function calls, MCP servers, and external APIs). Dedicated topics manage pub/sub operations, providing mechanisms for input, output, and human-in-the-loop interactions. Meanwhile, commands encapsulate invoke logic for each tool, allowing nodes to delegate work without tight coupling.</p> <p>Four critical capabilities\u2014observability, idempotency, auditability, and restorability\u2014underpin Graphite\u2019s suitability for production AI environments. Observability is achieved via event sourcing and OpenTelemetry-based tracing, idempotency through carefully managed event stores and retry logic, auditability by logging every action and data flow, and restorability by maintaining offset-based consumption records that let workflows resume exactly where they left off.</p> <p>Overall, Graphite offers a powerful, extensible foundation for building AI solutions that scale, adapt to evolving compliance needs, and gracefully handle failures or user-driven pauses. By combining a robust workflow engine, well-structured nodes and tools, and a complete event model, Graphite enables teams to develop sophisticated conversational agents and automated pipelines with confidence.</p>"},{"location":"#key-features","title":"Key Features","text":"<p>Event-Driven Architecture: Built on a pub/sub pattern where Topics manage message flow between Nodes, enabling loose coupling and flexible workflow composition.</p> <p>Modular Workflow Components: Construct AI agents using composable layers - Assistants orchestrate Workflows, Workflows coordinate Nodes, and Nodes execute Tools.</p> <p>Multiple LLM Integrations: Out-of-the-box support for OpenAI, Claude, Gemini, Ollama, DeepSeek, and OpenRouter, with a consistent interface across all providers.</p> <p>Function Calling Support: Seamlessly integrate custom Python functions with LLMs through the FunctionCallTool, enabling agents to interact with external APIs and services.</p> <p>MCP Server Integration: Connect to Model Context Protocol servers for dynamic tool discovery and external data source access.</p> <p>Production-Ready Features: Built-in observability via OpenTelemetry, event sourcing for auditability, idempotent operations, and workflow restorability for fault tolerance.</p>"},{"location":"#who-should-use-graphite","title":"Who Should Use Graphite?","text":"<p>Graphite is designed for developers and teams building AI-powered applications, including:</p> <ul> <li>AI Engineers building conversational agents with complex reasoning capabilities</li> <li>Backend Developers integrating LLM functionality into production systems</li> <li>MLOps Teams deploying observable, auditable AI workflows</li> <li>Researchers prototyping multi-step AI agents with tool use</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>This documentation will guide you through:</p> <ol> <li>Installation and Setup - Get Graphite running in your environment</li> <li>Core Concepts - Understand the architecture: Assistants, Workflows, Nodes, and Tools</li> <li>Building Workflows - Create event-driven AI pipelines</li> <li>Tool Integration - Add LLMs, function calls, and MCP servers</li> <li>Observability - Configure tracing with Arize and Phoenix</li> <li>Advanced Topics - Event stores, workflow recovery, and custom tools</li> </ol> <p>Ready to dive in? Start with our Quick Start Guide to build your first AI agent, or explore the Architecture to understand how Graphite components work together.</p>"},{"location":"#community-and-support","title":"Community and Support","text":"<p>Graphite is actively developed and maintained by the open-source community. Join us:</p> <ul> <li>GitHub: github.com/binome-dev/graphite</li> <li>Issues and Feature Requests: Use GitHub Issues for bug reports and feature requests</li> <li>Discussions: Join community discussions and get help from other users</li> <li>Contributing: Check out our contribution guidelines to help improve Graphite</li> </ul>"},{"location":"getting-started/features/","title":"Features","text":"<p>The core design principles that set Graphite apart from other agent frameworks are:</p> <ol> <li> <p>A Simple 3-Layer Invoke Model    Three distinct layers\u2014assistant, node, and tool\u2014manage invoke, while a dedicated workflow layer oversees orchestration.</p> </li> <li> <p>Pub/Sub Event-Driven Orchestration    Communication relies on publishing and subscribing to events, ensuring a decoupled, modular flow of data throughout the system.</p> </li> <li> <p>Events as the Single Source of Truth    All operational states and transitions are recorded as events, providing a uniform way to track and replay system behavior if needed.</p> </li> </ol> <p>Combining these elements, Graphite provides a production-grade AI application framework capable of operating reliably at scale, handling failures gracefully, and maintaining user and stakeholder trust. Four essential capabilities form the backbone of this approach:</p> <ol> <li> <p>Observability    Complex AI solutions involve multiple steps, data sources, and models. Graphite\u2019s event-driven architecture, logging, and tracing make it possible to pinpoint bottlenecks or errors in real time, ensuring that each component\u2019s behavior is transparent and measurable.</p> </li> <li> <p>Idempotency    Asynchronous workflows often require retries when partial failures occur or network conditions fluctuate. Graphite\u2019s design emphasizes idempotent operations, preventing pub/sub data duplication or corruption when calls must be repeated.</p> </li> <li> <p>Auditability    By treating events as the single source of truth, Graphite automatically logs every state change and decision path. This level of detailed recordkeeping is indispensable for users working in regulated sectors or who need full traceability for debugging and compliance.</p> </li> <li> <p>Restorability    Long-running AI tasks risk substantial rework if they fail mid-invoke. In Graphite, checkpoints and event-based playback enable workflows to resume from the precise point of interruption, minimizing downtime and maximizing resource efficiency.</p> </li> </ol> <p>Together, these capabilities\u2014observability, idempotency, auditability, and restorability\u2014distinguish Graphite as a framework for building robust and trustworthy AI applications. Below is a detailed breakdown of how Graphite implements each feature.</p>"},{"location":"getting-started/features/#observability","title":"Observability","text":"<p>The system leverages event sourcing to record all operations, combined with OpenTelemetry for standardized tracing. Thanks to the clearly defined three-layer invoke model (assistant, node, tool) plus an orchestration workflow, each invoke function is decorated to capture inputs, outputs, and any exceptions. These captures are converted into events (stored in the event store) and traces (exported to platforms like Arize or other OpenTelemetry services).</p> <p>Meanwhile, each Topic instance logs pub/sub interactions in the event store after processing. Coupled with the InvokeContext object, this approach makes the entire data flow within the agentic workflow fully transparent. Because every node and tool has a unique name and ID, and each operation is stamped with invoke context IDs, locating specific inputs and outputs is straightforward. Then given pub/sub events, the framework can build directed data flows between nodes. Even when there are circles, the data flow can form a DAG with publishing and consuming offset in each topic.</p>"},{"location":"getting-started/features/#idempotency","title":"Idempotency","text":"<p>Graphite adopts an event-driven architecture where topics function as logical message queues, storing each event exactly once in an event store. When a workflow fails, needs to be retried, or is paused (e.g., for a human-in-the-loop intervention), it can resume from the last known valid state by replaying events that were produced but not yet consumed by downstream nodes.</p> <p>Consumption events are only recorded once the entire node processing completes successfully. Until that point, the system treats partial or failed node invokes as if they never happened, preventing duplicated outputs or broken states. Should a node encounter an error (e.g., an LLM connection failure, external API issue, or function exception), Graphite detects the unconsumed events upon restoration and places the associated node(s) back into the invoke queue. This design ensures the node can safely retry from the same input without creating conflicting or duplicated consumption records.</p> <p>By storing each event exactly once and withholding consumption records until success, Graphite guarantees idempotent behavior. Even if a node issues multiple invocations due to an error, the event logs and consumption rules still reconstruct a single, consistent path from invocation to response. This approach produces correct outcomes on retries while maintaining a complete, conflict-free audit trail.</p>"},{"location":"getting-started/features/#auditability","title":"Auditability","text":"<p>Auditability in Graphite emerges naturally from its observability. By automatically persisting all invoke events and pub/sub events in a centralized event store, the platform provides a complete historical record of every action taken. Function decorators capture each invoke (including inputs, outputs, and exceptions), while Topic operations log every publish and consume operation, and effectively acting as a \u201ccache\u201d layer of orchestration events.</p> <p>Moreover, Graphite\u2019s modular design and clear separation of concerns simplify the process of examining specific components\u2014such as an LLM node and its associated tool. Each module has well-defined responsibilities, ensuring that every action is accurately documented in the event store and easily traceable. This end-to-end audit trail not only supports today\u2019s nascent AI regulations but positions Graphite to adapt to evolving compliance requirements. By preserving all relevant data in a consistent, verifiable format, Graphite provides the transparency and accountability that organizations demand from AI solutions.</p>"},{"location":"getting-started/features/#restorability","title":"Restorability","text":"<p>Restorability in Graphite builds on top of idempotency, ensuring that whenever a workflow stops\u2014due to an exception, human intervention, or any other cause\u2014it always concludes at a point where an event has been successfully published. This guarantees that upon resuming the workflow for an unfinished assistant_request_id, any node subscribed to that newly published event is reactivated, effectively restarting the process from where it left off.</p> <p>Internally, Graphite uses offset-based consumption in each topic. Whenever a node publishes an event (including self-loops or circular dependencies), the system records the publish offset in <code>PublishToTopicEvent</code> instance. When a node later consumes that event, it updates a corresponding consumption offset in topic, and store the offset in <code>ConsumeFromTopicEvent</code>. If a workflow is interrupted before consumption offsets are written, the node remains subscribed to the \u201cunconsumed\u201d event. As a result, when the workflow recovers, the engine identifies these outstanding events and places the node(s) back into the invoke queue.</p> <p>This mechanism effectively transforms cyclical or looping dependencies into a directed acyclic graph. The event store, combined with offset tracking, reveals which events have been fully processed and which remain pending, letting Graphite re-trigger only the incomplete parts of the workflow. The result is a resilient system that can resume from exactly where it stopped\u2014without reprocessing entire segments or risking inconsistent states.</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide walks you through installing Graphite using uv.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<p>Prerequisites:</p> <ul> <li>Python &gt;=3.11</li> <li>uv</li> </ul>"},{"location":"getting-started/installation/#installation","title":"Installation","text":"<p>Graphite can be installed with a single command using uv:</p> <pre>\n<code>uv add grafi</code></pre> <p>That's it! Graphite will be installed along with all its dependencies.</p>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>After installation, verify that Graphite is installed correctly:</p> <pre>\n<code># Check if the installation was successful\npython -c \"import grafi; print('Graphite installed successfully')\"</code></pre>"},{"location":"getting-started/installation/#virtual-environment-recommended","title":"Virtual Environment (Recommended)","text":"<p>For better dependency management, it's recommended to install Graphite in a virtual environment:</p> <pre>\n<code># Create a virtual environment\nuv venv\n\n# Activate the virtual environment\n# On Linux/macOS\nsource .venv/bin/activate\n# On Windows:\n.venv\\Scripts\\activate\n\n# Install Graphite\nuv  add grafi\n\n# When done, deactivate the virtual environment\ndeactivate</code></pre>"},{"location":"getting-started/installation/#upgrading","title":"Upgrading","text":"<p>To upgrade to the latest version of Graphite:</p> <pre>\n<code>uv add --upgrade grafi</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<p>Dependency Conflicts: If you have dependency conflicts, consider using a virtual environment or:</p> <pre>\n<code>uv add --force-reinstall grafi</code></pre> <p>Python Version Issues: Ensure you're using a supported Python version:</p> <pre>\n<code>python --version</code></pre>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter installation issues:</p> <ol> <li>Check the GitHub repository for current documentation</li> <li>Look through GitHub Issues for similar problems</li> <li>Create a new issue with:</li> <li>Your operating system and version</li> <li>Python and uv versions</li> <li>Complete error messages</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once Graphite is installed, you can start using it in your Python projects:</p> <pre>\n<code>import grafi\n# Your Graphite code here</code></pre> <p>Check the project documentation for usage examples and API reference.</p>"},{"location":"getting-started/quickstart/","title":"Getting Started with Graphite: The Hello, World! Assistant","text":"<p>Graphite is a powerful event-driven AI agent framework built for modularity, observability, and seamless composition of AI workflows. This comprehensive guide will walk you through creating your first ReAct (Reasoning and Acting) agent using the <code>grafi</code> package. In this tutorial, we'll build a function-calling assistant that demonstrates how to integrate language models with google search function within the Graphite framework, showcasing the core concepts of event-driven AI agent development.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Make sure the following are installed:</p> <ul> <li>Python &gt;=3.11 (required by the <code>grafi</code> package)</li> <li>uv</li> <li>Git</li> </ul> <p>\u26a0\ufe0f Important: <code>grafi</code> requires Python &gt;= 3.11. Other python version is not yet supported.</p>"},{"location":"getting-started/quickstart/#create-a-new-project","title":"Create a New Project","text":"<pre>\n<code>mkdir graphite-react\ncd graphite-react\n</code></pre> <p>This will create the <code>pyproject.toml</code> file that uv needs.</p> <pre>\n<code>uv init --name graphite-react</code></pre> <p>Be sure to specify a compatible Python version,  open <code>pyproject.toml</code> and ensure it includes:</p> <pre><code>[project]\nname = \"graphite-react\"\ndependencies = [\n    \"grafi&gt;=0.0.21\",\n]\nrequires-python = \"&gt;=3.11\"\n</code></pre> <p>Now install the dependencies:</p> <pre>\n<code>uv sync</code></pre> <p>This will automatically create a virtual environment and install <code>grafi</code> with the appropriate Python version.</p> <p>\ud83d\udca1 You can also specify the Python version explicitly:</p> <p><pre>\n<code>uv python pin python3.12</code></pre></p> <p>You also need the following two dependencies for this guide.</p> <pre>\n<code>uv add googlesearch-python pycountry</code></pre>"},{"location":"getting-started/quickstart/#use-built-in-react-agent","title":"Use Built-in ReAct Agent","text":"<p>In graphite an agent is a specialized assistant that can handle events and perform actions based on the input it receives. We will create a ReAct agent that uses OpenAI's language model to process input, make function calls, and generate responses.</p> <p>Create a file named <code>react_agent_app.py</code> and create a built-in react-agent:</p> <pre><code># react_agent_app.py\nimport asyncio\n\nfrom grafi.agents.react_agent import create_react_agent\n\n\nasync def main():\n    print(\"ReAct Agent Chat Interface\")\n    print(\"Type your questions and press Enter. Type '/bye' to exit.\")\n    print(\"-\" * 50)\n\n    react_agent = create_react_agent()\n\n    while True:\n        user_input = input(\"\\nYou: \").strip()\n\n        if user_input.lower() == \"/bye\":\n            print(\"Goodbye!\")\n            break\n\n        if not user_input:\n            continue\n\n        try:\n            # Get synchronized response from agent\n            output = await react_agent.run(user_input)\n            print(f\"\\nAgent: {output}\")\n\n        except Exception as e:\n            print(f\"Error: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>And finally export an <code>OPENAI_API_KEY</code> and <code>TAVILY_API_KEY</code> key as an environment variable:</p> <pre><code>export OPENAI_API_KEY=\"sk-proj-******\"\nexport TAVILY_API_KEY=\"tvly-******\"\n</code></pre>"},{"location":"getting-started/quickstart/#run-the-application","title":"Run the Application","text":"<p>Use uv to invoke the script inside the virtual environment:</p> <pre>\n<code>uv run python react_agent_app.py</code></pre> <p>You should see following in the terminal</p> <pre><code>ReAct Agent Chat Interface\nType your questions and press Enter. Type '/bye' to exit.\n--------------------------------------------------\n\nYou:\n</code></pre> <p>then you can add your questions, and exit by typing <code>/bye</code></p> <pre><code>ReAct Agent Chat Interface\nType your questions and press Enter. Type '/bye' to exit.\n--------------------------------------------------\n\nYou: What year was the United Kingdom Founded?\n\n&lt;... logs&gt;\n\nAgent: The United Kingdom (UK) was officially formed in 1707 with the Acts of Union,\nwhich united the Kingdom of England and the Kingdom of Scotland into a single\nentity known as the Kingdom of Great Britain. Later, in 1801, another\nAct of Union added the Kingdom of Ireland, leading to the formation of the\nUnited Kingdom of Great Britain and Ireland. After the majority of Ireland gained\nindependence in 1922, the name was changed to the United Kingdom of\nGreat Britain and Northern Ireland.\n\n\nYou: /bye\nGoodbye!\n</code></pre>"},{"location":"getting-started/quickstart/#summary","title":"Summary","text":"<p>\u2705 Initialized a uv project</p> <p>\u2705 Installed <code>grafi</code> with the correct Python version constraint</p> <p>\u2705 Wrote a minimal agent that handles an event</p> <p>\u2705 Ran the agent with a question</p>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the Graphite GitHub Repository for full-featured examples.</li> <li>Extend your agent to respond to different event types.</li> <li>Dive into advanced features like memory, workflows, and tools.</li> </ul> <p>Happy building! \ud83d\ude80</p>"},{"location":"guide/configuring-event-store/","title":"Configuring and Using Graphite's Event Store","text":"<p>Event stores are a powerful feature in the Graphite AI framework that enable persistent storage and retrieval of conversation events. This guide will walk you through setting up PostgreSQL as your event store backend and demonstrate how to leverage event storage for debugging, analytics, and conversation management.</p>"},{"location":"guide/configuring-event-store/#what-is-an-event-store","title":"What is an Event Store?","text":"<p>An event store in Graphite captures and persists  pub/sub events that occur during workflow execution, including: - User messages and system responses - Node executions and tool invocations - Pub/Sub events from Topics and Workflows - Error events and debugging information</p> <p>This provides valuable insights into your AI application's behavior and enables features like conversation history, debugging, and analytics.</p>"},{"location":"guide/configuring-event-store/#prerequisites","title":"Prerequisites","text":"<p>Before getting started, ensure you have: - Docker and Docker Compose installed - Python environment with Graphite AI framework installed - OpenAI API key configured - Basic understanding of PostgreSQL and event-driven architectures</p>"},{"location":"guide/configuring-event-store/#setting-up-postgresql-with-docker","title":"Setting Up PostgreSQL with Docker","text":"<p>First, let's set up a PostgreSQL database using Docker Compose. This provides a clean, isolated environment for your event store.</p>"},{"location":"guide/configuring-event-store/#docker-compose-configuration","title":"Docker Compose Configuration","text":"<p>create <code>.pgdata</code> directory that will act as the volume for your database</p> <pre>\n<code>mkdir .pgdata</code></pre> <p>Create a <code>docker-compose.yaml</code> file in your project root:</p> <pre><code>services:\n  postgres:\n    image: postgres:15\n    container_name: postgres15\n    environment:\n      POSTGRES_DB: grafi_test_db\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: postgres\n    volumes:\n      - ./.pgdata:/var/lib/postgresql/data\n    ports:\n      - \"5432:5432\"\n</code></pre>"},{"location":"guide/configuring-event-store/#starting-the-database","title":"Starting the Database","text":"<p>Launch your PostgreSQL container:</p> <pre>\n<code>docker compose up -d</code></pre> <p>The <code>-d</code> flag runs the container in detached mode, allowing it to run in the background.</p>"},{"location":"guide/configuring-event-store/#configuring-the-event-store","title":"Configuring the Event Store","text":"<p>Now let's integrate the PostgreSQL event store with your Graphite workflow. You need to have followed the quickstart  to create a <code>uv</code> project with grafite installed.</p> <p>You will have to add <code>sqlalchemy</code> and <code>psycopg2</code> dependencies to it since we will be using postgres as the event store.</p> <pre>\n<code>uv add sqlalchemy psycopg2</code></pre>"},{"location":"guide/configuring-event-store/#context-and-message-setup","title":"Context and Message Setup","text":"<p>Generate unique identifiers for tracking this conversation and request, configure OpenAI credentials and settings and then create the invoke context with all necessary tracking IDs.</p> <pre><code>import os\nimport uuid\n\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\nif not api_key:\n    raise ValueError(\"OPENAI_API_KEY environment variable is required\")\n\nmodel = os.getenv(\"OPENAI_MODEL\", \"gpt-4o\")\nsystem_message = os.getenv(\"OPENAI_SYSTEM_MESSAGE\", \"You are a helpful assistant.\")\n\ninvoke_context = InvokeContext(\n    user_id=uuid.uuid4().hex,\n    conversation_id=uuid.uuid4().hex,\n    invoke_id=uuid.uuid4().hex,\n    assistant_request_id=uuid.uuid4().hex\n)\nuser_input = \"What is the capital of the United Kingdom\"\n\nmessage = Message(\n    role=\"user\",\n    content=user_input\n)\n</code></pre> <p>Nothing fancy here, just initialization of context and setting up of variables.</p>"},{"location":"guide/configuring-event-store/#event-store-initialization","title":"Event Store Initialization","text":"<p>We create a PostgreSQL event store instance with the connection URL matching your Docker container configuration, we then register the event store with Graphite's dependency injection container to obtain a reference to the registered event store for later use.</p> <pre><code>from grafi.common.event_stores.event_store_postgres import EventStorePostgres\nfrom grafi.common.containers.container import container\n\npostgres_event_store = EventStorePostgres(\n    # must match what is in docker-compose.yaml\n    db_url=\"postgresql://postgres:postgres@localhost:5432/grafi_test_db\",\n)\n\ncontainer.register_event_store(postgres_event_store)\nevent_store = container.event_store\n</code></pre>"},{"location":"guide/configuring-event-store/#running-the-agent-and-retrieving-events","title":"Running the agent and Retrieving Events","text":"<p>Graphite has a built in react agent that can be used out of the box for your needs, we will use it for the simple use case of passing in input and retrieving the output from OpenAI.</p> <pre><code>import asyncio\nfrom grafi.agents.react_agent import create_react_agent\n\nasync def run_agent():\n\n    react_agent = create_react_agent()\n\n    result = await react_agent.run(user_input, invoke_context)\n\n    print(\"Output from React Agent:\", result)\n\n\n    events = event_store.get_conversation_events(conversation_id)\n\n    print(f\"Events for conversation {conversation_id}:\")\n\n    print(f\"Events: {events} \")\n\nasyncio.run(run_agent())\n</code></pre> <p>The key difference is that with the event store configured, all events will be automatically persisted.</p> <p>Once you have built the workflow and invoked it we can get all the events from the previously configured <code>event_store</code> and print it out.</p>"},{"location":"guide/configuring-event-store/#complete-example","title":"Complete Example","text":"<p>Here's the complete <code>event_store.py</code> file that demonstrates the full integration:</p> <pre><code># event_store.py\n\nimport os\nimport uuid\n\nfrom grafi.agents.react_agent import create_react_agent\nfrom grafi.common.containers.container import container\nfrom grafi.common.event_stores.event_store_postgres import EventStorePostgres\nfrom grafi.common.models.invoke_context import InvokeContext\nfrom grafi.common.models.message import Message\n\npostgres_event_store = EventStorePostgres(\n    db_url=\"postgresql://postgres:postgres@localhost:5432/grafi_test_db\",\n)\n\n\ncontainer.register_event_store(postgres_event_store)\n\nevent_store = container.event_store\n\n# Generate consistent IDs for the conversation\nconversation_id = uuid.uuid4().hex\nuser_id = uuid.uuid4().hex\ninvoke_id = uuid.uuid4().hex\nassistant_request_id = uuid.uuid4().hex\n\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"OPENAI_API_KEY environment variable is required\")\n\nmodel = os.getenv(\"OPENAI_MODEL\", \"gpt-4o\")\nsystem_message = os.getenv(\"OPENAI_SYSTEM_MESSAGE\", \"You are a helpful assistant.\")\n\nuser_input = \"What is the capital of the United Kingdom\"\n\n# Create invoke context with consistent IDs\ninvoke_context = InvokeContext(\n    user_id=user_id,\n    conversation_id=conversation_id,\n    invoke_id=invoke_id,\n    assistant_request_id=assistant_request_id,\n)\n\nasync def run_agent():\n\n    react_agent = create_react_agent()\n\n    result = await react_agent.run(user_input, invoke_context)\n\n    print(\"Output from React Agent:\", result)\n\n\n    events = event_store.get_conversation_events(conversation_id)\n\n    print(f\"Events for conversation {conversation_id}:\")\n\n    print(f\"Events: {events} \")\n\nasyncio.run(run_agent())\n</code></pre>"},{"location":"guide/configuring-event-store/#running-the-example","title":"Running the Example","text":"<ol> <li> <p>Start the PostgreSQL container:    <pre><code>docker-compose up -d\n</code></pre></p> </li> <li> <p>Set up environment variables:    <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\nexport OPENAI_MODEL=\"gpt-4o\"  # Optional\nexport OPENAI_SYSTEM_MESSAGE=\"You are a helpful assistant.\"  # Optional\n</code></pre></p> </li> <li> <p>Execute the script:    <pre><code>python event_store.py\n</code></pre></p> </li> <li> <p>Expected output:</p> </li> </ol> <pre>\n<code>\n2025-07-12 17:38:55.566 | WARNING  | grafi.tools.function_calls.function_call_tool:__init_subclass__:86 - GoogleSearchTool: no method decorated with @llm_function found.\n2025-07-12 17:38:55.895 | DEBUG    | grafi.common.instrumentations.tracing:is_local_endpoint_available:30 - Endpoint check failed: [Errno 111] Connection refused\n2025-07-12 17:38:55.895 | DEBUG    | grafi.common.instrumentations.tracing:setup_tracing:117 - OTLP endpoint is not available. Using InMemorySpanExporter.\n2025-07-12 17:38:55.905 | INFO     | grafi.topics.topic:publish_data:64 - [agent_input_topic] Message published with event_id: 1da4f45008264dc98fd63dc154dcaa6e\n2025-07-12 17:38:55.912 | DEBUG    | grafi.nodes.node:invoke:49 - Executing Node with inputs: [ConsumeFromTopicEvent(event_id='8dd13f159fbc40b29ca030501f4d2859', event_version='1.0', invoke_context=InvokeContext(conversation_id='9051491ca7a84b71a3f3e9d790b79e4f', invoke_id='6e56da25ca824bf4a8d454a731b6f336', assistant_request_id='0e0317d25c9b40e4a76a130bbbb7bc43', user_id='9e396be8f8c442bc9269a31a9b3f69a2'), event_type=, timestamp=datetime.datetime(2025, 7, 12, 16, 38, 55, 909090, tzinfo=datetime.timezone.utc), name='agent_input_topic', offset=0, data=[Message(name=None, message_id='ae2638fa5f3047fbbfb43dbd1545e6d2', timestamp=1752338335885634346, content='What is the capital of the United Kingdom', refusal=None, annotations=None, audio=None, role='user', tool_call_id=None, tools=None, function_call=None, tool_calls=None, is_streaming=False)], consumer_name='OpenAIInputNode', consumer_type='OpenAIInputNode')]\n2025-07-12 17:38:57.400 | INFO     | grafi.topics.topic:publish_data:69 - [function_call_topic] Message NOT published (condition not met)\n2025-07-12 17:38:57.400 | INFO     | grafi.topics.output_topic:publish_data:91 - [agent_output_topic] Message published with event_id: d83d3339fbd94d6e983fc51965177891\nOutput from React Agent:  The capital of the United Kingdom is London.\nEvents for conversation:  9051491ca7a84b71a3f3e9d790b79e4f:\nEvents: [AssistantInvokeEvent(event_id='7d97059ad3404c74be582ed4d01e365e', event_version='1.0', invoke_context=InvokeContext(conversation_id='9051491ca7a84b71a3f3e9d790b79e4f', invoke_id='6e56da25ca824bf4a8d454a731b6f336', assistant_request_id='0e0317d25c9b40e4a76a130bbbb7bc43', user_id='9e396be8f8c442bc9269a31a9b3f69a2'), event_type=, timestamp=datetime.datetime(2025, 7, 12, 16, 38, 55, 885710), assistant_id='de51912ecc6c44ce803a0cd5eb358ba5', assistant_name='ReActAgent', assistant_type='ReActAgent', input_data=[Message(name=None, message_id='ae2638fa5f3047fbbfb43dbd1545e6d2', timestamp=1752338335885634346, content='What is the capital of the United Kingdom', refusal=None, annotations=No```\n<p>```</p>\n<p>You can view the events in your postgres database as well now</p>\n<p></p>"},{"location":"guide/configuring-event-store/#key-benefits","title":"Key Benefits","text":""},{"location":"guide/configuring-event-store/#debugging-and-monitoring","title":"Debugging and Monitoring","text":"<ul>\n<li>Event Tracing: Track the complete flow of events through your workflow</li>\n<li>Error Analysis: Identify where failures occur and why</li>\n<li>Performance Monitoring: Analyze timing and resource usage</li>\n</ul>"},{"location":"guide/configuring-event-store/#conversation-management","title":"Conversation Management","text":"<ul>\n<li>History Retrieval: Access complete conversation history for context</li>\n<li>Session Continuity: Maintain history across multiple interactions</li>\n<li>User Analytics: Understand user behavior patterns</li>\n</ul>"},{"location":"guide/configuring-event-store/#compliance-and-auditing","title":"Compliance and Auditing","text":"<ul>\n<li>Audit Trail: Complete record of all interactions</li>\n<li>Data Governance: Centralized event storage for compliance</li>\n<li>Troubleshooting: Detailed logs for issue resolution</li>\n</ul>"},{"location":"guide/configuring-event-store/#best-practices","title":"Best Practices","text":"<ol>\n<li>Use Meaningful IDs: Generate consistent, traceable identifiers for conversations and users</li>\n<li>Environment Configuration: Keep database credentials in environment variables</li>\n<li>Connection Pooling: Use connection pooling for production deployments</li>\n<li>Event Cleanup: Implement retention policies for old events</li>\n<li>Monitoring: Set up alerts for event store health and performance</li>\n</ol>"},{"location":"guide/configuring-event-store/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/configuring-event-store/#common-issues","title":"Common Issues","text":"<p>Database Connection Errors:\n- Ensure PostgreSQL container is running: <code>docker compose ps</code>\n- Check connection string matches container configuration\n- Verify port 5432 is available</p>\n<p>Event Store Registration:\n- Ensure event store is registered before workflow execution\n- Check container dependencies are properly configured</p>\n<p>Performance Issues:\n- Monitor database disk usage in <code>.pgdata</code> directory\n- Consider indexing strategies for large event volumes\n- Implement event archiving for long-term storage</p>"},{"location":"guide/configuring-event-store/#next-steps","title":"Next Steps","text":"<p>With event storage configured, you can:\n- Build conversation history features\n- Implement advanced analytics and reporting\n- Create debugging and monitoring dashboards\n- Develop event-driven triggers and automation</p>\n<p>The event store provides the foundation for building sophisticated, production-ready AI applications with full observability and auditability.</p>"},{"location":"guide/configuring-integration-with-arize/","title":"Arize &amp; Phoenix Integration Guide","text":"<p>This document provides comprehensive guidance on integrating Graphite with Arize and Phoenix for distributed tracing and observability.</p>"},{"location":"guide/configuring-integration-with-arize/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Installation</li> <li>Tracing Options</li> <li>Configuration</li> <li>Usage Examples</li> <li>Best Practices</li> <li>Troubleshooting</li> </ul>"},{"location":"guide/configuring-integration-with-arize/#overview","title":"Overview","text":"<p>Graphite integrates with OpenTelemetry to provide distributed tracing through multiple backends:</p> <ul> <li>Arize: Production-grade monitoring and observability platform for AI applications</li> <li>Phoenix: Local/remote tracing solution ideal for development and debugging</li> <li>Auto: Automatic detection of available tracing endpoints</li> <li>In-Memory: Testing mode without external dependencies</li> </ul> <p>The integration is built on top of OpenTelemetry and automatically instruments: - OpenAI API calls - LLM interactions - Tool executions - Workflow orchestration - Node operations</p>"},{"location":"guide/configuring-integration-with-arize/#installation","title":"Installation","text":""},{"location":"guide/configuring-integration-with-arize/#core-dependencies","title":"Core Dependencies","text":"<p>Grafi includes the following observability dependencies by default:</p> <pre><code>dependencies = [\n    \"openinference-instrumentation-openai&gt;=0.1.30\",\n    \"arize-otel&gt;=0.10.0\",\n    \"arize-phoenix-otel&gt;=0.13.1\",\n]\n</code></pre> <p>These are automatically installed when you install Grafi:</p> <pre><code># Using pip\npip install grafi\n\n# Using poetry\npoetry add grafi\n\n# Using uv\nuv pip install grafi\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#optional-development-dependencies","title":"Optional Development Dependencies","text":"<p>For local Phoenix tracing during development:</p> <pre><code>pip install arize-phoenix\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#configuration","title":"Configuration","text":""},{"location":"guide/configuring-integration-with-arize/#docker-compose","title":"Docker Compose","text":"<p>To run Phoenix you can run it on your local machine via docker compose</p> <pre><code>version: '3.8'\n\nservices:\n\n  phoenix:\n    image: arizephoenix/phoenix:latest\n    ports:\n      - \"6006:6006\"\n      - \"4317:4317\"\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#environment-variables","title":"Environment Variables","text":""},{"location":"guide/configuring-integration-with-arize/#arize-configuration","title":"Arize Configuration","text":"<p>Set these environment variables when using Arize:</p> <pre><code># Required for Arize\nexport ARIZE_API_KEY=\"your-arize-api-key\"\nexport ARIZE_SPACE_ID=\"your-space-id\"\nexport ARIZE_PROJECT_NAME=\"your-project-name\"\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#phoenix-configuration","title":"Phoenix Configuration","text":"<p>Set these environment variables to override default Phoenix settings:</p> <pre><code># Optional - defaults to localhost:4317\nexport PHOENIX_ENDPOINT=\"localhost\" # if using docker compose and ports are forwarded\nexport PHOENIX_PORT=\"4317\" # This will override port settings in setup_tracing()\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#setup-function-parameters","title":"Setup Function Parameters","text":"<p>The <code>setup_tracing()</code> function accepts the following parameters:</p> <pre><code>def setup_tracing(\n    tracing_options: TracingOptions = TracingOptions.AUTO,\n    collector_endpoint: str = \"localhost\",\n    collector_port: int = 4317,\n    project_name: str = \"grafi-trace\",\n) -&gt; Tracer:\n</code></pre> <ul> <li>tracing_options: Backend to use (ARIZE, PHOENIX, AUTO, IN_MEMORY)</li> <li>collector_endpoint: Hostname of the collector (default: \"localhost\")</li> <li>collector_port: Port number of the collector (default: 4317)</li> <li>project_name: Name for the tracing project (default: \"grafi-trace\")</li> </ul>"},{"location":"guide/configuring-integration-with-arize/#tracing-options","title":"Tracing Options","text":"<p>Grafi provides four tracing backend options through the <code>TracingOptions</code> enum:</p>"},{"location":"guide/configuring-integration-with-arize/#1-arize-production-monitoring","title":"1. ARIZE - Production Monitoring","text":"<p>Use Arize for production environments with enterprise-grade observability:</p> <pre><code>from grafi.common.instrumentations.tracing import TracingOptions, setup_tracing\n\ntracing = setup_tracing(\n    tracing_options=TracingOptions.ARIZE,\n    collector_endpoint=\"https://otlp.arize.com/v1\",\n    project_name=\"my-dev-project\",\n)\n</code></pre> <p>When to use: - Production deployments - Need for team collaboration and sharing - Require advanced analytics and monitoring - Enterprise compliance requirements</p>"},{"location":"guide/configuring-integration-with-arize/#2-phoenix-localremote-development","title":"2. PHOENIX - Local/Remote Development","text":"<p>Use Phoenix for development and debugging:</p> <pre><code>from grafi.common.instrumentations.tracing import TracingOptions, setup_tracing\n\ntracer = setup_tracing(\n    tracing_options=TracingOptions.PHOENIX,\n    collector_endpoint=\"localhost\",\n    collector_port=4317,\n    project_name=\"my-dev-project\"\n)\n</code></pre> <p>When to use: - Local development and debugging - Quick iteration and testing - Learning and experimentation - Running Phoenix locally or on a remote server</p>"},{"location":"guide/configuring-integration-with-arize/#3-auto-automatic-detection","title":"3. AUTO - Automatic Detection","text":"<p>Let Grafi automatically detect available tracing endpoints:</p> <pre><code>from grafi.common.instrumentations.tracing import TracingOptions, setup_tracing\n\ntracer = setup_tracing(\n    tracing_options=TracingOptions.AUTO,\n    collector_endpoint=\"localhost\",\n    collector_port=4317\n)\n</code></pre> <p>Detection priority: 1. Default collector endpoint (if available) 2. Phoenix endpoint from environment variables 3. Falls back to in-memory tracing</p> <p>When to use: - Development environments with optional Phoenix - CI/CD pipelines - Flexible deployment scenarios</p>"},{"location":"guide/configuring-integration-with-arize/#4-in_memory-testing","title":"4. IN_MEMORY - Testing","text":"<p>Use in-memory tracing for tests and offline work:</p> <pre><code>tracer = setup_tracing(tracing_options=TracingOptions.IN_MEMORY)\n</code></pre> <p>When to use: - Unit and integration tests - CI/CD without external dependencies - Offline development - Minimal overhead scenarios</p>"},{"location":"guide/configuring-integration-with-arize/#usage-examples","title":"Usage Examples","text":""},{"location":"guide/configuring-integration-with-arize/#example-1-basic-setup-with-auto-detection","title":"Example 1: Basic Setup with AUTO Detection","text":"<pre><code>from grafi.common.containers.container import container\nfrom grafi.common.instrumentations.tracing import TracingOptions, setup_tracing\n\n# Register the tracer with auto-detection\ntracer = setup_tracing(tracing_options=TracingOptions.AUTO)\ncontainer.register_tracer(tracer)\n\n# Your assistant code here\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#example-2-production-setup-with-arize","title":"Example 2: Production Setup with Arize","text":"<pre><code>from grafi.common.containers.container import container\nfrom grafi.common.instrumentations.tracing import TracingOptions, setup_tracing\n\n# Ensure environment variables are set\n# ARIZE_API_KEY, ARIZE_SPACE_ID, ARIZE_PROJECT_NAME\n\ntracer = setup_tracing(\n    tracing_options=TracingOptions.ARIZE,\n    collector_endpoint=\"https://otlp.arize.com/v1\",\n    project_name=\"production-assistant\"\n)\ncontainer.register_tracer(tracer)\n\n# Your assistant code here\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#example-3-development-with-local-phoenix","title":"Example 3: Development with Local Phoenix","text":"<p>First, start Phoenix locally:</p> <pre><code># Install Phoenix if not already installed\npip install arize-phoenix\n\n# Start Phoenix server\ndocker compose up\n</code></pre> <p>Then in your code:</p> <pre><code>from grafi.common.containers.container import container\nfrom grafi.common.instrumentations.tracing import TracingOptions, setup_tracing\n\ntracer = setup_tracing(\n    tracing_options=TracingOptions.PHOENIX,\n    collector_endpoint=\"localhost\",\n    collector_port=4317,\n    project_name=\"my-dev-assistant\"\n)\ncontainer.register_tracer(tracer)\n\n# Your assistant code here\n</code></pre> <p>Visit <code>http://localhost:6006</code> to view the Phoenix UI.</p>"},{"location":"guide/configuring-integration-with-arize/#example-4-testing-with-in-memory-tracing","title":"Example 4: Testing with In-Memory Tracing","text":"<pre><code>from grafi.common.containers.container import container\nfrom grafi.common.instrumentations.tracing import TracingOptions, setup_tracing\n\n# Use in-memory tracing for tests\ntracer = setup_tracing(tracing_options=TracingOptions.IN_MEMORY)\ncontainer.register_tracer(tracer)\n\n# Your test code here\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#example-5-remote-phoenix-instance","title":"Example 5: Remote Phoenix Instance","text":"<pre><code>from grafi.common.containers.container import container\nfrom grafi.common.instrumentations.tracing import TracingOptions, setup_tracing\n\ntracer = setup_tracing(\n    # If you've set up the ENV Variables then some arguments can be skipped\n    tracing_options=TracingOptions.PHOENIX,\n    project_name=\"shared-dev-project\"\n)\ncontainer.register_tracer(tracer)\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#example-6-complete-assistant-with-tracing","title":"Example 6: Complete Assistant with Tracing","text":"<pre><code>import os\nimport uuid\nimport asyncio\nfrom grafi.common.containers.container import container\nfrom grafi.common.events.topic_events.publish_to_topic_event import PublishToTopicEvent\nfrom grafi.common.instrumentations.tracing import TracingOptions, setup_tracing\nfrom grafi.common.models.async_result import async_func_wrapper\nfrom grafi.common.models.invoke_context import InvokeContext\nfrom grafi.common.models.message import Message\nfrom grafi.assistants.assistant_base import AssistantBase\n\n# Setup tracing\ntracer = setup_tracing(tracing_options=TracingOptions.AUTO)\ncontainer.register_tracer(tracer)\n\n# Get event store\nevent_store = container.event_store\n\n# Create your assistant\nasync def main():\n    assistant = (\n        # YourAssistant is an instance of type grafi.assistants.assistant\n        # https://github.com/binome-dev/graphite/blob/main/grafi/assistants/assistant.py\n        YourAssistant.builder()\n        .name(\"MyAssistant\")\n        .api_key(os.getenv(\"OPENAI_API_KEY\"))\n        .build()\n    )\n\n    # Create invoke context\n    invoke_context = InvokeContext(\n        conversation_id=\"conversation_id\",\n        invoke_id=uuid.uuid4().hex,\n        assistant_request_id=uuid.uuid4().hex,\n    )\n\n    # Invoke assistant\n    input_data = PublishToTopicEvent(\n        invoke_context=invoke_context,\n        data=[Message(content=\"Hello!\", role=\"user\")]\n    )\n\n    output = await async_func_wrapper(\n        assistant.invoke(input_data, is_sequential=True)\n    )\n    print(output)\n\nasyncio.run(main())\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#best-practices","title":"Best Practices","text":""},{"location":"guide/configuring-integration-with-arize/#1-environment-specific-configuration","title":"1. Environment-Specific Configuration","text":"<p>Use different tracing backends for different environments:</p> <pre><code>import os\nfrom grafi.common.instrumentations.tracing import TracingOptions, setup_tracing\n\nenv = os.getenv(\"ENVIRONMENT\", \"development\")\n\nif env == \"production\":\n    tracing_option = TracingOptions.ARIZE\n    endpoint = \"https://otlp.arize.com/v1\"\nelif env == \"staging\":\n    tracing_option = TracingOptions.PHOENIX\n    endpoint = \"staging-phoenix.example.com\"\nelif env == \"development\":\n    tracing_option = TracingOptions.AUTO\n    endpoint = \"localhost\"\nelse:  # testing\n    tracing_option = TracingOptions.IN_MEMORY\n    endpoint = \"localhost\"\n\ntracer = setup_tracing(\n    tracing_options=tracing_option,\n    collector_endpoint=endpoint,\n    project_name=f\"{env}-assistant\"\n)\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#2-early-initialization","title":"2. Early Initialization","text":"<p>Set up tracing early in your application lifecycle, before creating assistants:</p> <pre><code># Good: Setup tracing first\ntracer = setup_tracing(tracing_options=TracingOptions.AUTO)\ncontainer.register_tracer(tracer)\n\n# Then create assistants\nassistant = MyAssistant.builder().build()\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#3-project-naming-conventions","title":"3. Project Naming Conventions","text":"<p>Use descriptive project names to organize traces:</p> <pre><code>tracer = setup_tracing(\n    tracing_options=TracingOptions.PHOENIX,\n    project_name=f\"{app_name}-{environment}-{version}\"\n)\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#4-secure-credential-management","title":"4. Secure Credential Management","text":"<p>Never hardcode API keys. Use environment variables or secret management:</p> <pre><code>import os\n\n# Good: Use environment variables\nos.environ[\"ARIZE_API_KEY\"] = os.getenv(\"ARIZE_API_KEY\")\n\n# Bad: Never hardcode\n# os.environ[\"ARIZE_API_KEY\"] = \"hardcoded-key\"\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#5-graceful-degradation-with-auto-mode","title":"5. Graceful Degradation with AUTO Mode","text":"<p>Use AUTO mode to gracefully degrade when tracing endpoints are unavailable:</p> <pre><code># Will automatically fall back to in-memory if no endpoint available\ntracer = setup_tracing(tracing_options=TracingOptions.AUTO)\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#6-testing-isolation","title":"6. Testing Isolation","text":"<p>Use IN_MEMORY mode in tests to avoid external dependencies:</p> <pre><code>import pytest\nfrom grafi.common.instrumentations.tracing import TracingOptions, setup_tracing\n\n@pytest.fixture(autouse=True)\ndef setup_test_tracing():\n    tracer = setup_tracing(tracing_options=TracingOptions.IN_MEMORY)\n    container.register_tracer(tracer)\n    yield\n    # Cleanup if needed\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/configuring-integration-with-arize/#issue-phoenix-endpoint-is-not-available","title":"Issue: \"Phoenix endpoint is not available\"","text":"<p>Symptom: ValueError when using PHOENIX tracing option</p> <p>Solution: 1. Ensure Phoenix is running:    <pre><code>\u279c docker compose up\n nc -zv localhost 4317\n\n Connection to localhost (::1) 4317 port [tcp/*] succeeded!\n\n nc -zv localhost 6006\n\n Connection to localhost (::1) 6006 port [tcp/x11-6] succeeded!\n</code></pre></p> <ol> <li>Check the endpoint and port are correct:</li> </ol> <pre><code>tracer = setup_tracing(\n    tracing_options=TracingOptions.PHOENIX,\n    collector_endpoint=\"localhost\",\n    collector_port=4317\n)\n</code></pre> <ol> <li>Use AUTO mode for graceful fallback:</li> </ol> <pre><code>tracer = setup_tracing(tracing_options=TracingOptions.AUTO)\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#issue-arize-traces-not-appearing","title":"Issue: Arize traces not appearing","text":"<p>Symptom: No traces visible in Arize dashboard</p> <p>Solution: 1. Verify environment variables are set:    <pre><code>import os\nprint(os.getenv(\"ARIZE_API_KEY\"))\nprint(os.getenv(\"ARIZE_SPACE_ID\"))\nprint(os.getenv(\"ARIZE_PROJECT_NAME\"))\n</code></pre></p> <ol> <li> <p>Check the collector endpoint:    <pre><code>tracer = setup_tracing(\n    tracing_options=TracingOptions.ARIZE,\n    collector_endpoint=\"https://otlp.arize.com/v1\"\n)\n</code></pre></p> </li> <li> <p>Verify API key has proper permissions</p> </li> </ol>"},{"location":"guide/configuring-integration-with-arize/#issue-connection-timeout-with-phoenix","title":"Issue: Connection timeout with Phoenix","text":"<p>Symptom: Slow startup or timeout errors</p> <p>Solution: 1. The endpoint check has a 0.1s timeout, which is normal 2. Use AUTO mode to automatically fall back:    <pre><code>tracer = setup_tracing(tracing_options=TracingOptions.AUTO)\n</code></pre></p> <ol> <li>For PHOENIX mode, ensure the endpoint is reachable:    <pre><code>nc -zv localhost 4317\n</code></pre></li> </ol>"},{"location":"guide/configuring-integration-with-arize/#issue-openai-instrumentation-not-working","title":"Issue: OpenAI instrumentation not working","text":"<p>Symptom: OpenAI calls not showing in traces</p> <p>Solution: 1. Ensure OpenAI is instrumented (done automatically by setup_tracing) 2. Verify tracer is registered before creating assistants:    <pre><code>container.register_tracer(tracer)  # Must be before assistant creation\n</code></pre></p>"},{"location":"guide/configuring-integration-with-arize/#issue-traces-showing-in-wrong-project","title":"Issue: Traces showing in wrong project","text":"<p>Symptom: Traces appear in unexpected project</p> <p>Solution: Specify project name explicitly: <pre><code>tracer = setup_tracing(\n    tracing_options=TracingOptions.PHOENIX,\n    project_name=\"my-specific-project\"\n)\n</code></pre></p>"},{"location":"guide/configuring-integration-with-arize/#debug-logging","title":"Debug Logging","text":"<p>Enable debug logging to troubleshoot tracing issues:</p> <pre><code>from loguru import logger\nimport sys\n\nlogger.remove()\nlogger.add(sys.stderr, level=\"DEBUG\")\n\n# Now setup tracing\ntracer = setup_tracing(tracing_options=TracingOptions.AUTO)\n</code></pre>"},{"location":"guide/configuring-integration-with-arize/#additional-resources","title":"Additional Resources","text":""},{"location":"guide/configuring-integration-with-arize/#arize-resources","title":"Arize Resources","text":"<ul> <li>Arize Platform Documentation</li> <li>Arize OpenTelemetry Integration</li> <li>Arize Python SDK</li> </ul>"},{"location":"guide/configuring-integration-with-arize/#phoenix-resources","title":"Phoenix Resources","text":"<ul> <li>Phoenix Documentation</li> <li>Phoenix GitHub Repository</li> <li>OpenInference Specification</li> </ul>"},{"location":"guide/configuring-integration-with-arize/#grafi-resources","title":"Grafi Resources","text":"<ul> <li>Graphite Documentation</li> <li>Event-Driven Workflows</li> <li>Graphite GitHub Repository</li> </ul>"},{"location":"guide/configuring-integration-with-arize/#support","title":"Support","text":"<p>For issues related to:</p> <ul> <li>Graphite tracing integration: Open an issue on the Grafi repository</li> <li>Arize platform: Contact Arize support or consult their documentation</li> <li>Phoenix: Check the Phoenix GitHub issues or documentation</li> </ul>"},{"location":"guide/connecting-to-an-mcp-server/","title":"Calling MCP Servers with Graphite AI Framework","text":"<p>The Graphite AI framework provides a powerful, event-driven approach to building AI agents and workflows. In this tutorial, we'll walk through a complete example that demonstrates how to call MCP Tools for your event driven workflows</p>"},{"location":"guide/connecting-to-an-mcp-server/#overview","title":"Overview","text":"<p>This tutorial will show you how to:</p> <ul> <li>Create a simple MCP Server over HTTP</li> <li>Create an Event Driven Workflow that calls MCP Server</li> <li>Parse the output from the MCP Server</li> </ul>"},{"location":"guide/connecting-to-an-mcp-server/#prerequisites","title":"Prerequisites","text":"<p>Before getting started, make sure you have:</p> <ul> <li>Python environment with Graphite AI framework installed</li> <li>OpenAI API key</li> <li>Basic understanding of Python and AI concepts</li> <li>Basic understanding of what MCP Servers are</li> <li>Understand Graphite Assistants</li> </ul>"},{"location":"guide/connecting-to-an-mcp-server/#code-walkthrough","title":"Code Walkthrough","text":"<p>Let's examine the complete code and break it down line by line:</p>"},{"location":"guide/connecting-to-an-mcp-server/#environment-configuration","title":"Environment Configuration","text":"<p>Configure your code to read <code>OPENAI_API_KEY</code> from your environment. You can modify the default values if you prefer not to set environment variables, although it is recommended to set <code>OPENAI_API_KEY</code> as an environment variable for security.</p> <pre><code>export OPENAI_API_KEY=\"sk-proj-*****\"\n</code></pre>"},{"location":"guide/connecting-to-an-mcp-server/#mcp-server","title":"MCP Server","text":"<p>MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.</p> <p>For our use case we will simulate that our internal databases have more accurate information that whatever the model is trained on. If you check the latest data on stocks that the model has been trained on.</p> <p></p> <p>For this guide, we will use the <code>FastMCP</code> library to create an MCP Server with tools to get the latest data for stocks. We will use the <code>ALPHA VANTAGE</code> api to get the latest data but make the AI queries go through our MCP Server instead of directly though Open AI.</p> <pre><code># server.py\nimport json\nimport os\nfrom typing import Any\n\nimport requests\nfrom dotenv import load_dotenv\nfrom fastmcp import Context, FastMCP\nfrom loguru import logger\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass AlphaVantageSettings(BaseSettings):\n\n    model_config: SettingsConfigDict = SettingsConfigDict(\n        env_prefix=\"ALPHA_VANTAGE_\", env_file=\".env\"\n    )\n\n    api_key: str = \"\"\n    mcp_port: int = 8081\n    mcp_path: str = \"/mcp\"\n\n\ndef json_serializer(data: Any) -&gt; str:\n    \"\"\"Serialize data to JSON format.\"\"\"\n    return json.dumps(data, indent=2)\n\n\nmcp = FastMCP(\"Stocks MCP Server\", tool_serializer=json_serializer,  on_duplicate_resources=\"error\")\n\n\nload_dotenv()\nalpha_vantage_settings = AlphaVantageSettings()  # type: ignore\n\nif alpha_vantage_settings.api_key == \"\":\n    logger.error(\n        \"ALPHA_VANTAGE_API_KEY is not set. Please set it in the .env file or as an env variable\"\n    )\n    exit(1)\n\n\n@mcp.tool()\nasync def daily(ticker: str, ctx: Context) -&gt; dict:\n    \"\"\"Query daily stock data for a given company given its ticker symbol from Alpha Vantage\"\"\"\n    logger.info(f\"Started Daily Stock Data Tool for Request ID: {ctx.request_id}\")\n    url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&amp;symbol={ticker}&amp;apikey={alpha_vantage_settings.api_key}\"\n    r = requests.get(url)\n    data = r.json()\n    return data\n\n\n@mcp.tool()\nasync def company_overview(ticker: str, ctx: Context) -&gt; dict:\n    \"\"\"Query specific company by ticker symbol and gets company overview data from Alpha Vantage\"\"\"\n    logger.info(f\"Started Company Overview Tool for Request ID: {ctx.request_id}\")\n    url = f\"https://www.alphavantage.co/query?function=OVERVIEW&amp;symbol={ticker}&amp;apikey={alpha_vantage_settings.api_key}\"\n    r = requests.get(url)\n    data = r.json()\n    return data\n\n\nif __name__ == \"__main__\":\n    logger.info(\"Started Server for Stocks MCP Server\")\n    mcp.run(\n        transport=\"http\",\n        host=\"0.0.0.0\",\n        port=alpha_vantage_settings.mcp_port,\n        path=alpha_vantage_settings.mcp_path,\n    )\n</code></pre> <p>Some of the environment variables can be overriden but for now, runnig it as is will start an mcp server on port <code>8081</code></p> <pre><code>\u276f python server.py\n INFO     | __main__:&lt;module&gt;:87 - Started Server for Stocks MCP Server\n\n\n\u256d\u2500 FastMCP 2.0 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                            \u2502\n\u2502        _ __ ___ ______           __  __  _____________    ____    ____     \u2502\n\u2502       _ __ ___ / ____/___ ______/ /_/  |/  / ____/ __ \\  |___ \\  / __ \\    \u2502\n\u2502      _ __ ___ / /_  / __ `/ ___/ __/ /|_/ / /   / /_/ /  ___/ / / / / /    \u2502\n\u2502     _ __ ___ / __/ / /_/ (__  ) /_/ /  / / /___/ ____/  /  __/_/ /_/ /     \u2502\n\u2502    _ __ ___ /_/    \\__,_/____/\\__/_/  /_/\\____/_/      /_____(_)____/      \u2502\n\u2502                                                                            \u2502\n\u2502                                                                            \u2502\n\u2502                                                                            \u2502\n\u2502    \ud83d\udda5\ufe0f  Server name:     Stocks MCP Server                                   \u2502\n\u2502    \ud83d\udce6 Transport:       Streamable-HTTP                                     \u2502\n\u2502    \ud83d\udd17 Server URL:      http://0.0.0.0:8081/mcp                             \u2502\n\u2502                                                                            \u2502\n\u2502    \ud83d\udcda Docs:            https://gofastmcp.com                               \u2502\n\u2502    \ud83d\ude80 Deploy:          https://fastmcp.cloud                               \u2502\n\u2502                                                                            \u2502\n\u2502    \ud83c\udfce\ufe0f  FastMCP version: 2.10.5                                              \u2502\n\u2502    \ud83e\udd1d MCP version:     1.11.0                                              \u2502\n\u2502                                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\nINFO     Starting MCP server 'Stocks MCP Server' with transport 'http' on        server.py:1448\n         http://0.0.0.0:8081/mcp\nINFO:     Started server process [115107]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8081 (Press CTRL+C to quit)\n</code></pre>"},{"location":"guide/connecting-to-an-mcp-server/#graphite-assistant","title":"Graphite Assistant","text":"<p>Just like before we create an assistant that will act as an interface to be called upon to run a workflow that queries mcp server for relevant stock data.</p> <pre><code># assistant.py\nimport os\nfrom typing import Optional\nfrom typing import Self\n\nfrom openinference.semconv.trace import OpenInferenceSpanKindValues\nfrom pydantic import Field\n\nfrom grafi.assistants.assistant import Assistant\nfrom grafi.assistants.assistant_base import AssistantBaseBuilder\nfrom grafi.topics.input_topic import InputTopic\nfrom grafi.topics.output_topic import OutputTopic\nfrom grafi.topics.subscription_builder import SubscriptionBuilder\nfrom grafi.topics.topic import Topic\nfrom grafi.nodes.node import Node\nfrom grafi.tools.function_calls.function_call_tool import FunctionCallTool\nfrom grafi.tools.llms.impl.openai_tool import OpenAITool\nfrom grafi.workflows.impl.event_driven_workflow import EventDrivenWorkflow\n\n\nclass StockAssistant(Assistant):\n    \"\"\"\n    A simple assistant class that uses OpenAI's language model to process input,\n    make function calls, and generate responses.\n\n    This class sets up a workflow with three nodes: an input LLM node, a function call node,\n    and an output LLM node. It provides a method to run input through this workflow.\n\n    Attributes:\n        name (str): The name of the assistant.\n        api_key (str): The API key for OpenAI. If not provided, it tries to use the OPENAI_API_KEY environment variable.\n        model (str): The name of the OpenAI model to use.\n        event_store (EventStore): An instance of EventStore to record events during the assistant's operation.\n        function (Callable): The function to be called by the assistant.\n    \"\"\"\n\n    oi_span_type: OpenInferenceSpanKindValues = Field(\n        default=OpenInferenceSpanKindValues.AGENT\n    )\n    name: str = Field(default=\"StockAssistant\")\n    type: str = Field(default=\"StockAssistant\")\n    api_key: Optional[str] = Field(default_factory=lambda: os.getenv(\"OPENAI_API_KEY\"))\n    model: str = Field(default=\"gpt-4o-mini\")\n    function_call_llm_system_message: Optional[str] = Field(default=None)\n    summary_llm_system_message: Optional[str] = Field(default=None)\n    function_tool: FunctionCallTool\n\n    @classmethod\n    def builder(cls) -&gt; \"StockAssistantBuilder\":\n        \"\"\"Return a builder for StockAssistant.\"\"\"\n        return StockAssistantBuilder(cls)\n\n    def _construct_workflow(self) -&gt; \"StockAssistant\":\n\n        mcp_tool_call_topic = Topic(\n            name=\"mcp_tool_call_topic\",\n            condition=lambda event: event.data[-1].tool_calls\n            is not None,  # Checking if the\n        )\n\n        assistant_output_topic = OutputTopic(name=\"assistant_output_topic\",\n                    condition = (\n            lambda event: event.data[-1].content is not None\n            and isinstance(event.data[-1].content, str)\n            and event.data[-1].content.strip() != \"\"\n        ))\n\n        assistant_root_topic = InputTopic(name=\"assistant_root_topic\")\n\n        llm_input_node = (\n            Node.builder()\n            .name(\"OpenAINode\")\n            .type(\"LLMNode\")\n            .subscribe(SubscriptionBuilder().subscribed_to(assistant_root_topic).build())\n            .tool(\n                OpenAITool.builder()\n                .name(\"OpenAIQueryStockOverview\")\n                .api_key(self.api_key)\n                .model(self.model)\n                .system_message(self.function_call_llm_system_message)\n                .build()\n            )\n            .publish_to(mcp_tool_call_topic)\n            .publish_to(assistant_output_topic)\n            .build()\n        )\n\n        # Create a function call node\n\n        mcp_server_response_topic = Topic(name=\"mcp_server_response_topic\")\n\n\n        mcp_node = (\n            Node.builder()\n            .name(\"MCPServerQuery\")\n            .type(\"MCPNode\")\n            .subscribe(SubscriptionBuilder().subscribed_to(mcp_tool_call_topic).build())\n            .tool(self.function_tool)\n            .publish_to(mcp_server_response_topic)\n            .build()\n        )\n\n        # Create an output LLM node\n        summary_node = (\n            Node.builder()\n            .name(\"SummaryNode\")\n            .type(\"LLMNode\")\n            .subscribe(\n                SubscriptionBuilder().subscribed_to(mcp_server_response_topic).build()\n            )\n            .tool(\n                OpenAITool.builder()\n                .name(\"OpenAIQuerySummary\")\n                .api_key(self.api_key)\n                .model(self.model)\n                .system_message(self.summary_llm_system_message)\n                .build()\n            )\n            .publish_to(assistant_output_topic)\n            .build()\n        )\n\n        # Create a workflow and add the nodes\n        self.workflow = (\n            EventDrivenWorkflow.builder()\n            .name(\"simple_function_call_workflow\")\n            .node(llm_input_node)\n            .node(mcp_node)\n            .node(summary_node)\n            .build()\n        )\n\n        return self\n\n\nclass StockAssistantBuilder(\n    AssistantBaseBuilder[StockAssistant]\n):\n    \"\"\"\n    Concrete builder for StockAssistant.\n    This builder allows setting the API key, model, system messages, and function tool.\n    \"\"\"\n\n    def api_key(self, api_key: str) -&gt; Self:\n        self.kwargs[\"api_key\"] = api_key\n        return self\n\n    def model(self, model: str) -&gt; Self:\n        self.kwargs[\"model\"] = model\n        return self\n\n    def function_call_llm_system_message(\n        self, function_call_llm_system_message: str\n    ) -&gt; Self:\n        self.kwargs[\n            \"function_call_llm_system_message\"\n        ] = function_call_llm_system_message\n        return self\n\n    def summary_llm_system_message(self, summary_llm_system_message: str) -&gt; Self:\n        self.kwargs[\"summary_llm_system_message\"] = summary_llm_system_message\n        return self\n\n    def function_tool(self, function_tool: FunctionCallTool) -&gt; Self:\n        self.kwargs[\"function_tool\"] = function_tool\n        return self\n</code></pre> <p>Graphite is natively asynchronous, but you can choose to run synchronous coroutines as well. For this case we are making a fully asynchronous workflow by overriding the <code>async def run()</code> method of the <code>Assistant</code> class. In order to run this create a <code>main.py</code> that will instantiate the assistant and execute it asynchronously.</p> <pre><code>#main.py\nimport asyncio\nimport os\nimport uuid\nfrom typing import Dict\n\nfrom grafi.common.containers.container import container\nfrom grafi.common.events.topic_events.publish_to_topic_event import PublishToTopicEvent\nfrom grafi.common.models.invoke_context import InvokeContext\nfrom grafi.common.models.mcp_connections import StreamableHttpConnection\nfrom grafi.common.models.message import Message\nfrom grafi.tools.function_calls.impl.mcp_tool import MCPTool\n\nfrom assistant import StockAssistant\n\nevent_store = container.event_store\n\nasync def create_assistant():\n    api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n    model = \"gpt-4o\"\n    function_call_llm_system_message = \"You are a helpful assistant that can call functions to retrieve data from an mcp server\"\n\n    mcp_config: Dict[str, StreamableHttpConnection] = {\n        \"stock-mcp-server\": StreamableHttpConnection(\n            url=\"http://localhost:8081/mcp/\", transport=\"streamable-http\"\n        )\n    }\n\n    assistant =( StockAssistant.builder()\n        .name(\"MCPAssistant\")\n        .model(model)\n        .api_key(api_key)\n        .function_call_llm_system_message(function_call_llm_system_message)\n        .function_tool(await MCPTool.builder().connections(mcp_config).build())\n        .summary_llm_system_message(\n            \"You are a helpful assistant that provides a summary of the company overview. return all output as json formatted string\")\n        .build()\n    )\n    return assistant\n\n\nasync def main():\n    assistant = await create_assistant()\n\n    invoke_context = InvokeContext(\n        conversation_id=uuid.uuid4().hex,\n        invoke_id=uuid.uuid4().hex,\n        assistant_request_id=uuid.uuid4().hex,\n    )\n\n    question = \"What is the overview of the company Tesla?\"\n    input_data = [Message(role=\"user\", content=question)]\n\n    publish_event = PublishToTopicEvent(\n        invoke_context=execution_context, data=input_messages\n    )\n\n    async for response in assistant.invoke(publish_event):\n        print(\"Assistant output:\")\n        for output in response:\n            print(output.content)\n\n    return assistant\n\nassistant = asyncio.run(main())\n</code></pre> <p>Runnig this code with</p> <pre>\n<code>python main.py</code></pre> <p>You'll see that the logs on your MCP server have been triggered by the call.</p> <pre>\n<pre><code>...\nINFO:     192.168.48.1:39408 - \"POST /mcp/ HTTP/1.1\" 202 Accepted\nINFO:     192.168.48.1:39422 - \"GET /mcp/ HTTP/1.1\" 200 OK\nINFO:     192.168.48.1:39434 - \"POST /mcp/ HTTP/1.1\" 200 OK\nINFO      __main__:company_overview:70 - Started Company Overview Tool for Request ID: 1\nINFO:     192.168.48.1:39450 - \"POST /mcp/ HTTP/1.1\" 200 OK\nINFO:     192.168.48.1:39456 - \"DELETE /mcp/ HTTP/1.1\" 200 OK\n...\n</code></pre>\n</pre> <p>And the logs on the <code>main.py</code> from the Assistant</p> <pre>\n<code>Assistant output:\n<pre><code>{\n  \"Symbol\": \"TSLA\",\n  \"AssetType\": \"Common Stock\",\n  \"Name\": \"Tesla Inc\",\n  \"Description\": \"Tesla, Inc. is an American electric vehicle and clean energy company based in Palo Alto, California. Tesla's current products include electric cars, battery energy storage from home to grid-scale, solar panels and solar roof tiles, as well as other related products and services. In 2020, Tesla had the highest sales in the plug-in and battery electric passenger car segments, capturing 16% of the plug-in market (which includes plug-in hybrids) and 23% of the battery-electric (purely electric) market. Through its subsidiary Tesla Energy, the company develops and is a major installer of solar photovoltaic energy generation systems in the United States. Tesla Energy is also one of the largest global suppliers of battery energy storage systems, with 3 GWh of battery storage supplied in 2020.\",\n  \"CIK\": \"1318605\",\n  \"Exchange\": \"NASDAQ\",\n  \"Currency\": \"USD\",\n  \"Country\": \"USA\",\n  \"Sector\": \"MANUFACTURING\",\n  \"Industry\": \"MOTOR VEHICLES &amp; PASSENGER CAR BODIES\",\n  \"Address\": \"3500 DEER CREEK RD, PALO ALTO, CA, US\",\n  \"OfficialSite\": \"https://www.tesla.com\",\n  \"FiscalYearEnd\": \"December\",\n  \"LatestQuarter\": \"2025-03-31\",\n  \"MarketCapitalization\": \"1061789434000\",\n  \"EBITDA\": \"12550000000\",\n  \"PERatio\": \"188.37\",\n  \"PEGRatio\": \"5.77\",\n  \"BookValue\": \"23.18\",\n  \"DividendPerShare\": \"None\",\n  \"DividendYield\": \"None\",\n  \"EPS\": \"1.75\",\n  \"RevenuePerShareTTM\": \"29.87\",\n  \"ProfitMargin\": \"0.0638\",\n  \"OperatingMarginTTM\": \"0.0255\",\n  \"ReturnOnAssetsTTM\": \"0.0372\",\n  \"ReturnOnEquityTTM\": \"0.0877\",\n  \"RevenueTTM\": \"95724003000\",\n  \"GrossProfitTTM\": \"16907000000\",\n  \"DilutedEPSTTM\": \"1.75\",\n  \"QuarterlyEarningsGrowthYOY\": \"-0.707\",\n  \"QuarterlyRevenueGrowthYOY\": \"-0.092\",\n  \"AnalystTargetPrice\": \"306.07\",\n  \"AnalystRatingStrongBuy\": \"7\",\n  \"AnalystRatingBuy\": \"16\",\n  \"AnalystRatingHold\": \"14\",\n  \"AnalystRatingSell\": \"8\",\n  \"AnalystRatingStrongSell\": \"3\",\n  \"TrailingPE\": \"188.37\",\n  \"ForwardPE\": \"169.49\",\n  \"PriceToSalesRatioTTM\": \"11.09\",\n  \"PriceToBookRatio\": \"14.22\",\n  \"EVToRevenue\": \"10.84\",\n  \"EVToEBITDA\": \"76.18\",\n  \"Beta\": \"2.461\",\n  \"52WeekHigh\": \"488.54\",\n  \"52WeekLow\": \"182\",\n  \"50DayMovingAverage\": \"323.94\",\n  \"200DayMovingAverage\": \"317.85\",\n  \"SharesOutstanding\": \"3220960000\",\n  \"SharesFloat\": \"2803295000\",\n  \"PercentInsiders\": \"12.889\",\n  \"PercentInstitutions\": \"50.134\",\n  \"DividendDate\": \"None\",\n  \"ExDividendDate\": \"None\"\n}\n</code></pre>\n </code>\n</pre> <p>You can then proceed to deserialize this into a python object, but this is beyond the scope of this guide.</p>"},{"location":"guide/connecting-to-an-mcp-server/#understanding-the-flow","title":"Understanding The Flow","text":"<p>Graphite will create the workflow graph by linking subscribers to publishers, and configuring all function tools, if we were to create a graph of the workflow it would generate the following.</p> <p></p> <p>As you can see, starting with the root node which we've called <code>assistant_root_topic</code> which must be an instance of <code>InputTopic</code> and acts as an entrypoint to the workflow.</p> <p><pre><code>assistant_root_topic = InputTopic(name=\"assistant_root_topic\")\n</code></pre> and subsequentially the final node has to be an instance of <code>OutputTopic</code></p> <pre><code>assistant_output_topic = OutputTopic(name=\"assistant_output_topic\",\n                    condition = (\n            lambda event: event.data[-1].content is not None\n            and isinstance(event.data[-1].content, str)\n            and event.data[-1].content.strip() != \"\"\n        ))\n</code></pre> <p>Topics can have conditions attached to them to trigger them, in the case of <code>InputTopic</code> it has no condition, so anything that is invoked on it will push through the workflow, but the final topic of type <code>OutputTopic</code> is filtering that there is an actual response and a non empty string before returning output. You can configure this to your heart's desire to tune your workflow as you wish.</p> <p>When you call the assistant's <code>invoke()</code> method from <code>main.py</code></p> <pre><code>async for output in assistant.invoke(invoke_context, input_data):\n</code></pre> <p>The <code>input_data</code> gets published on the assistants <code>InputTopic</code> during workflow initialization. This is done within the framework but we are outlying it here so you can follow the flow of data.</p> <pre><code>for input_topic in input_topics:\n    event = input_topic.publish_data(\n        invoke_context=invoke_context,\n        publisher_name=self.name,\n        publisher_type=self.type,\n        data=input,\n        consumed_events=[],\n    )\n</code></pre> <p>This means that the <code>input_data</code> is published to all <code>InputTopic</code> declared within the workflow,  during workflow creation, and subsequentially will act as input for the first node in the workflow, in our case the <code>assistant_root_topic</code>. The flow of all topics and nodes is that they have an input and an output, a topic's output is a node's input and a node's output becomes the topic's input.</p>"},{"location":"guide/connecting-to-an-mcp-server/#how-mcp-tool-selection-works","title":"How MCP Tool Selection Works","text":"<p>The key insight is that the AI model itself doesn't automatically decide when to query an MCP server vs. use its training data. Instead, this works through a structured tool/function calling mechanism.</p>"},{"location":"guide/connecting-to-an-mcp-server/#tool-registration","title":"Tool Registration","text":"<p>When you set up an MCP server, you register specific tools/functions with descriptions of what they do. For getting a company overview, you'd register a function like <code>company_overview()</code> with a description.</p> <pre><code>@mcp.tool()\nasync def company_overview(ticker: str, ctx: Context) -&gt; dict:\n    \"\"\"Query specific company by ticker symbol and gets company overview data from Alpha Vantage\"\"\"\n</code></pre>"},{"location":"guide/connecting-to-an-mcp-server/#model-reasoning","title":"Model Reasoning","text":"<p>When you ask \"What's Tesla's price?\", the model reads the available tool descriptions and reasons about whether it should use its training data or call an external tool. The model plans around the fact that \"each MCP call is an external operation that might be much slower than the AI's internal inference\" - it weighs speed vs. accuracy/recency. Tool Selection: The model makes this decision based on:</p> <ul> <li>The specificity of your question</li> <li>Whether current/real-time data is needed</li> <li>The available tool descriptions</li> <li>Context clues in your prompt</li> </ul>"},{"location":"guide/connecting-to-an-mcp-server/#context","title":"Context","text":"<p>This is why it's important to decorate and use descriptors in your mcp tool definition so that the model can reason and guide the query to a function call, like in our example.</p> <pre><code>@mcp.tool()\nasync def company_overview(ticker: str, ctx: Context) -&gt; dict:\n    \"\"\"Query specific company by ticker symbol and gets company overview data from Alpha Vantage\"\"\"\n    logger.info(f\"Started Company Overview Tool for Request ID: {ctx.request_id}\")\n    url = f\"https://www.alphavantage.co/query?function=OVERVIEW&amp;symbol={ticker}&amp;apikey={alpha_vantage_settings.api_key}\"\n    r = requests.get(url)\n    data = r.json()\n    return data\n</code></pre> <p>To ensure the model queries your MCP server instead of using training data, you can:</p> <ul> <li>Be explicit in your prompt</li> <li>Design good tool descriptions: Make your MCP tool description clearly indicate it provides real-time, current data (in our case <code>company_overview</code> has a descrption and a declaration of arguments)</li> <li>Use system prompts: Instruct the model to prefer external tools for specific types of queries, in our case <code>You are a helpful assistant that can call functions to retrieve data from an mcp server</code> (emphasis on the mcp server)</li> <li>Context setting: Make it clear that up-to-date information is needed</li> </ul> <p>The  more detailed you are in the \"\"\" Description \"\"\" the more likely the model will reason that it needs to use the MCP server for routing the request. In the example we've built the question was <code>What is the overview of the company Tesla?</code> which uses the keywords <code>company</code> and <code>overview</code> for the MCP Server tool description and function tool name <code>company_overview</code> and the model reasons that it should forward to this instead of it's internal training data.</p>"},{"location":"guide/connecting-to-an-mcp-server/#output-data","title":"Output Data","text":"<p>If the query to OpenAI is succesful then the response object for it will not include a string in the <code>content</code> field, but it will rather set the <code>tool_calls</code> field, indicating that it needs to use an MCP server and it returns with the correct fields as arguments to the mcp tool call.</p> <p><pre><code>output_data:\n    - name: null\n      message_id: 91863c5fdf1a4918b08bf351e444e2d7\n      timestamp: 1753035546159204600\n      content: null\n      refusal: null\n      annotations: []\n      audio: null\n      role: assistant\n      tool_call_id: null\n      tools: null\n      function_call: null\n      tool_calls:\n        - id: call_d44v3RuJMdMCxXPvaxBwG6Lo\n          function:\n            arguments: '{\"ticker\":\"TSLA\"}'\n            name: company_overview\n          type: function\n      is_streaming: false\n</code></pre> You can view this using our internal tool called <code>grafi-dev</code> which we will demonstrate in future guides.</p> <p>This is the reason why when we declare the topic <code>mcp_tool_call_topic</code> we add a condition that it can only publish to it if the <code>tool_calls</code> field is set on the response from OpenAI</p> <pre><code> mcp_tool_call_topic = Topic(\n    name=\"mcp_tool_call_topic\",\n    condition=lambda event: event.data[-1].tool_calls\n    is not None,\n)\n</code></pre>"},{"location":"guide/connecting-to-an-mcp-server/#conclusion","title":"Conclusion","text":"<p>You now have the capability to route to MCP server of your choosing and under whichever condition you want. In our case since we are only doing company overviews, whenever we ask.</p> <pre><code>question = \"What is the overview of the company Tesla?\"\n\nAssistant output:\n```json\n{\n  \"Symbol\": \"TSLA\",\n  \"AssetType\": \"Common Stock\",\n  \"Name\": \"Tesla Inc\",\n  \"Description\": \"Tesla, Inc. is an American electric vehicle and clean energy company based in Palo Alto, California. Tesla's current products include electric cars, battery energy storage from home to grid-scale, solar panels and solar roof tiles, as well as other related products and services. In 2020, Tesla had the highest sales in the plug-in and battery electric passenger car segments, capturing 16% of the plug-in market (which includes plug-in hybrids) and 23% of the battery-electric (purely electric) market. Through its subsidiary Tesla Energy, the company develops and is a major installer of solar photovoltaic energy generation systems in the United States. Tesla Energy is also one of the largest global suppliers of battery energy storage systems, with 3 GWh of battery storage supplied in 2020.\",\n  ...\n}\n</code></pre> <p>It will be routed to our MCP Server, but if we ask it about something not related to company overviews it will respond with OpenAI training data.</p> <pre><code>question = \"When was the last crusade?\"\n\nAssistant output:\nThe last major crusade is often considered to be the Ninth Crusade, which took place in 1271\u20131272. It was led by Prince Edward of England (later King Edward I) and was part of the larger series of religious wars known as the Crusades, aimed primarily at capturing and holding the Holy Land against Muslim forces. After this, the Crusading movement began to wane, although minor crusades and related military campaigns continued in various forms in later years.\n</code></pre>"},{"location":"guide/connecting-to-an-mcp-server/#next-steps","title":"Next Steps","text":"<p>With this foundation, you can extend your implementation by:</p> <ul> <li>Adding more MCP servers for different data sources</li> <li>Implementing caching strategies for frequently accessed data</li> <li>Creating specialized assistants for different domains</li> <li>Building more complex workflows with multiple tool chains</li> </ul> <p>The combination of Graphite's event-driven architecture with MCP servers provides a powerful foundation for building sophisticated AI applications that can access and reason with real-time data from multiple sources.</p> <p>Remember: the key to successful MCP integration lies in detailed tool descriptions that help the AI model make intelligent routing decisions between its training data and external sources.</p>"},{"location":"guide/creating-a-simple-workflow/","title":"Building Your First AI Agent with Graphite AI Framework","text":"<p>The Graphite AI framework provides a powerful, event-driven approach to building AI agents and workflows. In this tutorial, we'll walk through a complete example that demonstrates how to create a simple AI assistant using OpenAI's GPT models.</p>"},{"location":"guide/creating-a-simple-workflow/#overview","title":"Overview","text":"<p>This tutorial will show you how to: - Set up environment variables for API configuration - Create an AI node using OpenAI's tools - Build an event-driven workflow - Handle user input and process responses</p>"},{"location":"guide/creating-a-simple-workflow/#prerequisites","title":"Prerequisites","text":"<p>Before getting started, make sure you have: - Python environment with Graphite AI framework installed - OpenAI API key - Basic understanding of Python and AI concepts</p>"},{"location":"guide/creating-a-simple-workflow/#code-walkthrough","title":"Code Walkthrough","text":"<p>Let's examine the complete code and break it down line by line:</p>"},{"location":"guide/creating-a-simple-workflow/#1-environment-configuration","title":"1. Environment Configuration","text":"<p>Configure your code to read <code>OPENAI_API_KEY</code> from your environment, as well as <code>OPENAI_MODEL</code> and <code>OPENAI_SYSTEM_MESSAGE</code>. You can modify the default values if you prefer not to set environment variables, although it is recommended to set <code>OPENAI_API_KEY</code> as an environment variable for security.</p> <pre><code>import os\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"OPENAI_API_KEY environment variable is required\")\n\nmodel = os.getenv(\"OPENAI_MODEL\", \"gpt-4o\")\nsystem_message = os.getenv(\"OPENAI_SYSTEM_MESSAGE\", \"You are a helpful assistant.\")\n</code></pre>"},{"location":"guide/creating-a-simple-workflow/#2-main-function-setup","title":"2. Main Function Setup","text":"<p>The main function orchestrates the entire workflow. We start by defining a sample user question about the UK's capital, then create the necessary context and message objects.</p> <pre><code>import uuid\nfrom grafi.common.models.message import Message\nfrom grafi.common.models.invoke_context import InvokeContext\n\ndef main():\n    user_input = \"What is the capital of the United Kingdom\"\n\n    invoke_context = InvokeContext(\n        user_id=uuid.uuid4().hex,\n        conversation_id=uuid.uuid4().hex,\n        invoke_id=uuid.uuid4().hex,\n        assistant_request_id=uuid.uuid4().hex,\n    )\n\n    message = Message(\n        role=\"user\",\n        content=user_input\n    )\n</code></pre> <p>The <code>InvokeContext</code> maintains the workflow state and tracking information across different operations in the system. It provides essential context for conversation management and request tracing.</p> <ul> <li><code>user_id</code>: Optional user identifier, defaults to empty string.</li> <li><code>conversation_id</code>: Unique identifier for a conversation between user and assistant.</li> <li><code>invoke_id</code>: Unique identifier for each conversation invoke - an invoke can involve multiple agents.</li> <li><code>assistant_request_id</code>: Created when an agent receives a request from the user.</li> </ul> <p>The <code>Message</code> object represents the user's input: - <code>role</code>: Specifies this is a \"user\" message - <code>content</code>: Contains the actual question text</p>"},{"location":"guide/creating-a-simple-workflow/#3-node-creation","title":"3. Node Creation","text":"<p>Nodes are first-class citizens in Graphite - all functionality must be wrapped in a node. Since we want to query OpenAI for the capital of the United Kingdom, we create a Node using the builder pattern.</p> <p>The node subscribes to the <code>agent_input_topic</code> (the root topic in Graphite), uses the <code>OpenAITool</code> to query OpenAI's endpoint with the required configuration (<code>api_key</code>, <code>model</code>, and <code>system_message</code>), and publishes results to the <code>agent_output_topic</code> (the final topic in any Graphite workflow).</p> <pre><code>from grafi.nodes.node import Node\nfrom grafi.tools.llms.impl.openai_tool import OpenAITool\nfrom grafi.topics.input_topic import InputTopic\nfrom grafi.topics.output_topic import OutputTopic\n\n\nagent_input_topic = InputTopic(name=\"agent_input_topic\")\nagent_output_topic = OutputTopic(name=\"agent_output_topic\")\n\nllm_node = (\n    Node.builder()\n    .name(\"LLMNode\")\n    .subscribe(agent_input_topic)\n    .tool(\n        OpenAITool.builder()\n        .name(\"OpenAITool\")\n        .api_key(api_key)\n        .model(model)\n        .system_message(system_message)\n        .build()\n    )\n    .publish_to(agent_output_topic)\n    .build()\n)\n</code></pre>"},{"location":"guide/creating-a-simple-workflow/#4-workflow-creation","title":"4. Workflow Creation","text":"<p>Now that we have created a node with its input (subscribe) and output (publish_to) configuration, we must bind it to a workflow. We use Graphite's <code>EventDrivenWorkflow</code> with the builder pattern to attach the node.</p> <pre><code>from grafi.workflows.impl.event_driven_workflow import EventDrivenWorkflow\n\nworkflow = (\n    EventDrivenWorkflow.builder()\n    .name(\"OpenAIEventDrivenWorkflow\")\n    .node(llm_node)\n    .build()\n)\n</code></pre>"},{"location":"guide/creating-a-simple-workflow/#5-workflow-execution","title":"5. Workflow Execution","text":"<p>With the <code>EventDrivenWorkflow</code> object created, we can invoke it by passing our <code>invoke_context</code> and a <code>List[Message]</code>. The workflow will execute and return the results, which we can then print. Save this complete code as <code>main.py</code>.</p> <pre><code>async for result in workflow.invoke(\n    invoke_context,\n    [message]\n):\n    for output_message in result:\n        print(\"Output message:\", output_message.content)\n</code></pre>"},{"location":"guide/creating-a-simple-workflow/#6-entry-point","title":"6. Entry Point","text":"<p>Finally, add the standard Python entry point to run the main function when the script is executed directly.</p> <pre><code>if __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"guide/creating-a-simple-workflow/#running-the-code","title":"Running the Code","text":"<p>To run this example:</p> <ol> <li> <p>Set up environment variables:    <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\nexport OPENAI_MODEL=\"gpt-4o\"  # Optional\nexport OPENAI_SYSTEM_MESSAGE=\"You are a helpful assistant.\"  # Optional\n</code></pre></p> </li> <li> <p>Execute the script:    <pre><code>python main.py\n</code></pre></p> </li> <li> <p>Expected output:    <pre><code>Output message: The capital of the United Kingdom is London.\n</code></pre></p> </li> </ol>"},{"location":"guide/creating-a-simple-workflow/#key-concepts","title":"Key Concepts","text":""},{"location":"guide/creating-a-simple-workflow/#event-driven-architecture","title":"Event-Driven Architecture","text":"<p>The Graphite AI framework uses an event-driven approach where: - Nodes subscribe to topics to receive messages - Nodes publish responses to output topics - Workflows orchestrate the flow of events between nodes</p>"},{"location":"guide/creating-a-simple-workflow/#builder-pattern","title":"Builder Pattern","text":"<p>The framework extensively uses the builder pattern, allowing for: - Fluent, readable configuration - Step-by-step construction of complex objects - Flexible parameter setting</p>"},{"location":"guide/creating-a-simple-workflow/#context-management","title":"Context Management","text":"<p>The <code>InvokeContext</code> provides crucial metadata for: - Tracking user sessions - Managing conversation state - Debugging and logging</p>"},{"location":"guide/creating-a-simple-workflow/#next-steps","title":"Next Steps","text":"<p>This example demonstrates the basics of the Graphite AI framework. You can extend this by: - Adding multiple nodes for complex workflows - Implementing custom tools and integrations - Building more sophisticated conversation management - Adding error handling and logging</p> <p>The framework's event-driven nature makes it easy to create scalable, maintainable AI applications that can grow with your needs.</p>"},{"location":"guide/debugging-with-grafi-dev/","title":"Debugging and Development with Grafi-Dev: A Complete Guide","text":"<p>The Graphite AI framework is powerful, but like any complex system, debugging workflows and understanding execution flow can be challenging. Enter grafi-dev - a specialized development server designed to make debugging Graphite applications intuitive and efficient.</p> <p>In this guide, we'll explore how to use grafi-dev to debug your Graphite workflows, inspect events in real-time, and streamline your development process.</p>"},{"location":"guide/debugging-with-grafi-dev/#what-is-grafi-dev","title":"What is Grafi-Dev?","text":"<p>Grafi-dev is a development server that provides real-time debugging capabilities for Graphite applications. It offers:</p> <ul> <li>Real-time event monitoring: See events as they flow through your workflow</li> <li>Interactive debugging: Pause, inspect, and step through workflow execution  </li> <li>Event inspection: Deep dive into event payloads and transformations</li> <li>Workflow visualization: Understand how your events are being processed</li> <li>Hot reload: Make changes and see them reflected immediately</li> </ul>"},{"location":"guide/debugging-with-grafi-dev/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"guide/debugging-with-grafi-dev/#prerequisites","title":"Prerequisites","text":"<p>Before installing grafi-dev, ensure you have: - Node.js (version 16 or higher) - A Graphite project to debug - Basic familiarity with Graphite workflows</p>"},{"location":"guide/debugging-with-grafi-dev/#installing-grafi-dev","title":"Installing Grafi-Dev","text":"<p>Install grafi-dev globally using npm:</p> <pre><code>npm install -g @binome-dev/graphite-dev\n</code></pre> <p>Or if you prefer using it locally in your project:</p> <pre><code>npm install --save-dev @binome-dev/graphite-dev\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#verify-installation","title":"Verify Installation","text":"<p>Check that grafi-dev is installed correctly:</p> <pre><code>grafi-dev --version\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#getting-started-your-first-debug-session","title":"Getting Started: Your First Debug Session","text":"<p>Let's walk through setting up grafi-dev with a simple Graphite workflow.</p>"},{"location":"guide/debugging-with-grafi-dev/#step-1-prepare-your-graphite-project","title":"Step 1: Prepare Your Graphite Project","text":"<p>First, ensure your Graphite project has a proper structure. Here's a minimal example:</p> <pre><code># workflow.py\nfrom graphite import Workflow, event_handler\n\nclass MyWorkflow(Workflow):\n    @event_handler(\"user.message\")\n    async def handle_message(self, event):\n        print(f\"Received: {event.data['message']}\")\n        await self.emit(\"response.generated\", {\n            \"reply\": f\"Echo: {event.data['message']}\"\n        })\n\n    @event_handler(\"response.generated\")\n    async def handle_response(self, event):\n        print(f\"Sending: {event.data['reply']}\")\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#step-2-configure-grafi-dev","title":"Step 2: Configure Grafi-Dev","text":"<p>Create a <code>grafi-dev.config.js</code> file in your project root:</p> <pre><code>module.exports = {\n  // Port for the development server\n  port: 3001,\n\n  // Path to your Graphite application\n  appPath: './workflow.py',\n\n  // Enable hot reload\n  hotReload: true,\n\n  // Event filtering (optional)\n  eventFilters: [\n    'user.*',\n    'response.*'\n  ],\n\n  // Debug settings\n  debug: {\n    pauseOnError: true,\n    showEventPayloads: true,\n    logLevel: 'debug'\n  }\n};\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#step-3-start-the-debug-server","title":"Step 3: Start the Debug Server","text":"<p>Launch grafi-dev from your project directory:</p> <pre><code>grafi-dev start\n</code></pre> <p>You should see output similar to:</p> <pre><code>\ud83d\ude80 Grafi-Dev Server starting...\n\ud83d\udcca Dashboard available at: http://localhost:3001\n\ud83d\udd27 Watching for changes in: ./workflow.py\n\u2705 Connected to Graphite application\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#step-4-open-the-debug-dashboard","title":"Step 4: Open the Debug Dashboard","text":"<p>Navigate to <code>http://localhost:3001</code> in your browser. You'll see the grafi-dev dashboard with several panels:</p> <ul> <li>Event Stream: Real-time view of events flowing through your workflow</li> <li>Workflow Graph: Visual representation of your event handlers and their connections</li> <li>Inspector: Detailed view of selected events and their payloads</li> <li>Console: Logs and debug output from your application</li> </ul>"},{"location":"guide/debugging-with-grafi-dev/#key-features-and-how-to-use-them","title":"Key Features and How to Use Them","text":""},{"location":"guide/debugging-with-grafi-dev/#real-time-event-monitoring","title":"Real-Time Event Monitoring","text":"<p>The Event Stream panel shows all events as they occur in your workflow. Each event displays:</p> <ul> <li>Timestamp: When the event was emitted</li> <li>Event Type: The event name (e.g., \"user.message\")</li> <li>Source: Which handler emitted the event</li> <li>Payload Size: Amount of data in the event</li> <li>Processing Status: Whether the event was handled successfully</li> </ul> <p>Click on any event to inspect its full payload in the Inspector panel.</p>"},{"location":"guide/debugging-with-grafi-dev/#interactive-debugging","title":"Interactive Debugging","text":""},{"location":"guide/debugging-with-grafi-dev/#setting-breakpoints","title":"Setting Breakpoints","text":"<p>You can set breakpoints directly in the dashboard:</p> <ol> <li>Navigate to the Workflow Graph panel</li> <li>Click on any event handler node</li> <li>Select \"Add Breakpoint\" from the context menu</li> </ol> <p>When execution hits a breakpoint, the workflow pauses and you can: - Inspect the current event data - Step through the handler line by line - Modify event payloads before continuing - Skip to the next event handler</p>"},{"location":"guide/debugging-with-grafi-dev/#stepping-through-execution","title":"Stepping Through Execution","text":"<p>When paused at a breakpoint, use the debug controls:</p> <pre><code># Continue execution\nc or continue\n\n# Step to next line\ns or step\n\n# Step over function calls\nn or next\n\n# Step into function calls\ni or into\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#event-inspection-and-modification","title":"Event Inspection and Modification","text":"<p>The Inspector panel allows you to:</p>"},{"location":"guide/debugging-with-grafi-dev/#view-event-details","title":"View Event Details","text":"<pre><code>{\n  \"type\": \"user.message\",\n  \"timestamp\": \"2025-07-20T10:30:45.123Z\",\n  \"source\": \"web_interface\",\n  \"data\": {\n    \"message\": \"Hello, Graphite!\",\n    \"user_id\": \"user_123\",\n    \"session_id\": \"sess_456\"\n  },\n  \"metadata\": {\n    \"correlation_id\": \"corr_789\",\n    \"retry_count\": 0\n  }\n}\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#modify-event-data","title":"Modify Event Data","text":"<p>Before an event is processed, you can modify its payload:</p> <ol> <li>Click on an event in the Event Stream</li> <li>Edit the JSON in the Inspector panel</li> <li>Click \"Apply Changes\"</li> <li>The modified event will be processed with your changes</li> </ol> <p>This is incredibly useful for testing edge cases without modifying your code.</p>"},{"location":"guide/debugging-with-grafi-dev/#workflow-visualization","title":"Workflow Visualization","text":"<p>The Workflow Graph panel provides a visual representation of your event flow:</p> <ul> <li>Nodes represent event handlers</li> <li>Edges show event flow between handlers</li> <li>Colors indicate handler status (idle, processing, error)</li> <li>Thickness of edges shows event frequency</li> </ul> <p>You can: - Click nodes to see handler code - Hover over edges to see recent events - Filter the graph by event types - Export the graph as an image</p>"},{"location":"guide/debugging-with-grafi-dev/#advanced-debugging-techniques","title":"Advanced Debugging Techniques","text":""},{"location":"guide/debugging-with-grafi-dev/#conditional-breakpoints","title":"Conditional Breakpoints","text":"<p>Set breakpoints that only trigger under specific conditions:</p> <pre><code>// In grafi-dev.config.js\nmodule.exports = {\n  debug: {\n    conditionalBreakpoints: [\n      {\n        handler: \"handle_message\",\n        condition: \"event.data.user_id === 'admin'\"\n      },\n      {\n        handler: \"handle_response\",\n        condition: \"event.data.reply.includes('error')\"\n      }\n    ]\n  }\n};\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#event-replay","title":"Event Replay","text":"<p>Grafi-dev automatically records all events. You can replay them for testing:</p> <ol> <li>Go to the Event Stream panel</li> <li>Select events you want to replay</li> <li>Click \"Replay Selected Events\"</li> <li>Watch as the events are re-processed through your workflow</li> </ol> <p>This is perfect for: - Testing fixes after identifying bugs - Reproducing intermittent issues - Performance testing with real event patterns</p>"},{"location":"guide/debugging-with-grafi-dev/#custom-event-injection","title":"Custom Event Injection","text":"<p>Inject custom events for testing:</p> <pre><code>// Through the dashboard\n{\n  \"type\": \"test.custom_event\",\n  \"data\": {\n    \"test_parameter\": \"test_value\",\n    \"scenario\": \"edge_case_1\"\n  }\n}\n</code></pre> <p>Or programmatically:</p> <pre><code>curl -X POST http://localhost:3001/api/inject \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"type\": \"user.message\",\n    \"data\": {\"message\": \"Test message\"}\n  }'\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#performance-profiling","title":"Performance Profiling","text":""},{"location":"guide/debugging-with-grafi-dev/#event-processing-metrics","title":"Event Processing Metrics","text":"<p>Grafi-dev tracks performance metrics for each handler:</p> <ul> <li>Average Processing Time: How long handlers take to process events</li> <li>Throughput: Events processed per second</li> <li>Memory Usage: Memory consumption during processing</li> <li>Error Rate: Percentage of events that cause errors</li> </ul>"},{"location":"guide/debugging-with-grafi-dev/#bottleneck-identification","title":"Bottleneck Identification","text":"<p>The Performance panel highlights: - Slow handlers (processing time &gt; threshold) - Memory leaks (increasing memory usage) - High error rates - Queue backups (events waiting to be processed)</p>"},{"location":"guide/debugging-with-grafi-dev/#optimization-suggestions","title":"Optimization Suggestions","text":"<p>Based on the metrics, grafi-dev provides optimization suggestions:</p> <pre><code>\u26a0\ufe0f  Handler 'process_large_data' is taking 2.3s on average\n\ud83d\udca1 Suggestion: Consider breaking this into smaller, parallel tasks\n\n\ud83d\udd0d Memory usage increased 45% in the last 5 minutes\n\ud83d\udca1 Suggestion: Check for memory leaks in event handlers\n\n\ud83d\udcc8 Queue depth is growing for 'user.message' events  \n\ud83d\udca1 Suggestion: Consider adding more worker processes\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#integration-with-development-workflow","title":"Integration with Development Workflow","text":""},{"location":"guide/debugging-with-grafi-dev/#cicd-integration","title":"CI/CD Integration","text":"<p>You can run grafi-dev in headless mode for continuous integration:</p> <pre><code># Run tests with grafi-dev monitoring\ngrafi-dev test --headless --config ci-config.js\n\n# Generate performance report\ngrafi-dev profile --duration 60s --output performance-report.json\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#team-collaboration","title":"Team Collaboration","text":"<p>Share debugging sessions with your team:</p> <pre><code># Start shared session\ngrafi-dev start --share\n\n# Others can connect to the session\ngrafi-dev connect --session &lt;session_id&gt;\n</code></pre> <p>Team members can: - See the same event stream - Set breakpoints collaboratively - Share event inspections - Review performance metrics together</p>"},{"location":"guide/debugging-with-grafi-dev/#best-practices","title":"Best Practices","text":""},{"location":"guide/debugging-with-grafi-dev/#1-use-descriptive-event-names","title":"1. Use Descriptive Event Names","text":"<p>Instead of generic names like \"event1\", use descriptive names:</p> <pre><code># \u274c Poor naming\nawait self.emit(\"event1\", data)\n\n# \u2705 Good naming  \nawait self.emit(\"user.authentication_failed\", {\n    \"user_id\": user_id,\n    \"failure_reason\": \"invalid_password\"\n})\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#2-structure-event-payloads-consistently","title":"2. Structure Event Payloads Consistently","text":"<p>Maintain consistent payload structure for easier debugging:</p> <pre><code># Standard event payload structure\n{\n    \"type\": \"user.message_received\",\n    \"data\": {\n        # Main event data\n        \"message\": \"Hello\",\n        \"user_id\": \"123\"\n    },\n    \"metadata\": {\n        # Debugging/tracking information\n        \"correlation_id\": \"corr_456\",\n        \"source\": \"web_interface\",\n        \"timestamp\": \"2025-07-20T10:30:45.123Z\"\n    }\n}\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#3-add-contextual-information","title":"3. Add Contextual Information","text":"<p>Include debugging context in your events:</p> <pre><code>await self.emit(\"data.processing_started\", {\n    \"data_type\": \"user_upload\",\n    \"file_size\": file_size,\n    \"processing_id\": processing_id,\n    # Debugging context\n    \"debug_info\": {\n        \"handler\": \"process_upload\",\n        \"thread_id\": threading.current_thread().ident,\n        \"memory_usage\": psutil.Process().memory_info().rss\n    }\n})\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#4-use-event-filters-strategically","title":"4. Use Event Filters Strategically","text":"<p>Configure filters to focus on relevant events:</p> <pre><code>// Focus on user interactions and errors\neventFilters: [\n    'user.*',           // All user events\n    '*.error',          // All error events  \n    'system.critical.*' // Critical system events\n]\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#5-regular-performance-reviews","title":"5. Regular Performance Reviews","text":"<p>Schedule regular performance reviews using grafi-dev:</p> <pre><code># Weekly performance snapshot\ngrafi-dev profile --duration 1h --schedule weekly --alert-on-degradation\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"guide/debugging-with-grafi-dev/#connection-issues","title":"Connection Issues","text":"<p>If grafi-dev can't connect to your Graphite application:</p> <ol> <li>Check the app path: Ensure <code>appPath</code> in your config points to the correct file</li> <li>Verify dependencies: Make sure your Graphite app has all required dependencies</li> <li>Check ports: Ensure no other services are using the grafi-dev port</li> <li>Firewall settings: Verify firewall isn't blocking the connection</li> </ol>"},{"location":"guide/debugging-with-grafi-dev/#performance-issues","title":"Performance Issues","text":"<p>If grafi-dev itself is slowing down:</p> <ol> <li>Reduce event history: Limit the number of events kept in memory</li> <li>Use event filters: Focus on specific event types</li> <li>Disable heavy features: Turn off workflow visualization for high-throughput scenarios</li> </ol> <pre><code>// Performance-optimized config\nmodule.exports = {\n  performance: {\n    maxEventHistory: 1000,      // Keep only recent events\n    enableVisualization: false, // Disable heavy graph rendering\n    samplingRate: 0.1          // Sample 10% of events\n  }\n};\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#memory-leaks","title":"Memory Leaks","text":"<p>If you notice memory usage growing:</p> <ol> <li>Check event retention: Events might not be garbage collected</li> <li>Review breakpoints: Paused workflows can accumulate events</li> <li>Monitor handler memory: Look for handlers that don't release resources</li> </ol>"},{"location":"guide/debugging-with-grafi-dev/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"guide/debugging-with-grafi-dev/#custom-plugins","title":"Custom Plugins","text":"<p>Extend grafi-dev with custom plugins:</p> <pre><code>// custom-plugin.js\nmodule.exports = {\n  name: 'custom-metrics',\n\n  onEventProcessed(event, duration) {\n    // Send metrics to external system\n    metricsClient.timing('event.processed', duration, {\n      event_type: event.type\n    });\n  },\n\n  onError(error, event) {\n    // Custom error handling\n    errorTracker.captureException(error, {\n      event_context: event\n    });\n  }\n};\n\n// In grafi-dev.config.js\nmodule.exports = {\n  plugins: [\n    require('./custom-plugin')\n  ]\n};\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#multiple-environment-support","title":"Multiple Environment Support","text":"<p>Configure different settings for different environments:</p> <pre><code>module.exports = {\n  development: {\n    port: 3001,\n    debug: { logLevel: 'debug' },\n    hotReload: true\n  },\n\n  staging: {\n    port: 3002,\n    debug: { logLevel: 'info' },\n    hotReload: false,\n    performance: { samplingRate: 0.5 }\n  },\n\n  production: {\n    port: 3003,\n    debug: { logLevel: 'error' },\n    hotReload: false,\n    performance: { samplingRate: 0.1 }\n  }\n};\n</code></pre>"},{"location":"guide/debugging-with-grafi-dev/#conclusion","title":"Conclusion","text":"<p>Grafi-dev transforms the Graphite development experience by providing powerful debugging and monitoring capabilities. With real-time event streams, interactive debugging, and performance profiling, you can:</p> <ul> <li>Debug faster: Identify issues quickly with real-time event monitoring</li> <li>Understand better: Visualize your workflow to see how events flow</li> <li>Optimize effectively: Use performance metrics to identify bottlenecks</li> <li>Collaborate seamlessly: Share debugging sessions with your team</li> </ul> <p>The key to getting the most out of grafi-dev is to integrate it early in your development process. Don't wait until you have problems - use it from day one to build better, more reliable Graphite applications.</p> <p>Remember: great debugging tools don't just fix problems - they prevent them. By understanding how your events flow and where your bottlenecks are, you can design better workflows from the start.</p> <p>Start debugging smarter with grafi-dev today!</p> <p>For more information, check out the grafi-dev GitHub repository and join our community discussions.</p>"},{"location":"guide/getting-started-with-assistants/","title":"Creating AI Assistants with Graphite AI Framework","text":"<p>Building on the foundational concepts from creating simple workflows, this guide demonstrates how to wrap your workflows in Graphite's Assistant framework. Assistants provide a higher-level abstraction that encapsulates workflow logic, making it easier to create reusable, maintainable AI components.</p>"},{"location":"guide/getting-started-with-assistants/#overview","title":"Overview","text":"<p>This guide will show you how to: - Convert a simple workflow into a reusable assistant - Implement the Assistant pattern with builder design - Create flexible, configurable AI assistants - Handle different types of user interactions - Build production-ready AI components</p>"},{"location":"guide/getting-started-with-assistants/#prerequisites","title":"Prerequisites","text":"<p>Before getting started, make sure you have: - Completed the Simple Workflow Guide - Python environment with Graphite AI framework installed - OpenAI API key configured - Understanding of Python classes and inheritance</p>"},{"location":"guide/getting-started-with-assistants/#comparison-workflow-vs-assistant","title":"Comparison: Workflow vs Assistant","text":"Aspect Simple Workflow Assistant Reusability Limited High Configuration Hardcoded Flexible Conversation Management Manual Built-in Error Handling Basic Comprehensive Type Safety Limited Full Testing Complex Simple"},{"location":"guide/getting-started-with-assistants/#from-workflow-to-assistant","title":"From Workflow to Assistant","text":"<p>In the simple workflow guide, we created a basic event-driven workflow. While functional, this approach has limitations: - No reusability across different contexts - Hardcoded configuration values - Limited conversation management - No encapsulation of business logic</p> <p>Assistants solve these problems by providing a structured, object-oriented approach to workflow management.</p>"},{"location":"guide/getting-started-with-assistants/#code-walkthrough","title":"Code Walkthrough","text":"<p>Let's examine how to transform our simple workflow into a powerful assistant. For this we will create an finance assistant that will provide us with financial information to make decisions.</p>"},{"location":"guide/getting-started-with-assistants/#global-configuration","title":"Global Configuration","text":"<pre><code>CONVERSATION_ID = uuid.uuid4().hex\n</code></pre> <p>Set <code>CONVERSATION_ID</code> to track conversation flow.</p>"},{"location":"guide/getting-started-with-assistants/#assistant-class-definition","title":"Assistant Class Definition","text":"<pre><code># main.py\nfrom grafi.assistants.assistant import Assistant\nfrom typing import Optional\n\nfrom pydantic import Field\n\nclass FinanceAssistant(Assistant):\n    \"\"\"Assistant for handling financial queries and analysis using OpenAI.\"\"\"\n\n    name: str = Field(default=\"FinanceAssistant\")\n    type: str = Field(default=\"FinanceAssistant\")\n    api_key: Optional[str] = Field(default=os.getenv(\"OPENAI_API_KEY\"))\n    model: str = Field(default=os.getenv(\"OPENAI_MODEL\", \"gpt-4o\"))\n    system_message: str = Field(default=os.getenv(\"OPENAI_SYSTEM_MESSAGE\"))\n</code></pre> <p>Create a class that defines the assistant class inheriting from Graphite's base <code>Assistant</code> class. A good practice is to use Pydantic fields for configuration with environment variable defaults: - <code>name</code> and <code>type</code>: Identify the assistant instance - <code>api_key</code>: OpenAI API key with environment variable fallback - <code>model</code>: OpenAI model selection with default - <code>system_message</code>: Customizable system prompt</p>"},{"location":"guide/getting-started-with-assistants/#builder-class-implementation","title":"Builder Class Implementation","text":"<pre><code># main.py\nfrom typing import Self\nfrom grafi.assistants.assistant_base import AssistantBaseBuilder\n\n\nclass FinanceAssistantBuilder(AssistantBaseBuilder[FinanceAssistant]):\n    \"\"\"Concrete builder for FinanceAssistant.\"\"\"\n\n    def api_key(self, api_key: str) -&gt; Self:\n        self.kwargs[\"api_key\"] = api_key\n        return self\n\n    def model(self, model: str) -&gt; Self:\n        self.kwargs[\"model\"] = model\n        return self\n\n    def system_message(self, system_message: str) -&gt; Self:\n        self.kwargs[\"system_message\"] = system_message\n        return self\n</code></pre> <p>Implement the builder pattern for fluent configuration: - Extends the base assistant builder - Provides methods for setting API key, model, and system message - Returns <code>self</code> for method chaining</p> <p>This class is used to set the fields from the <code>FinanceAssistant</code>. The magic happens in the <code>builder</code> method up next.</p>"},{"location":"guide/getting-started-with-assistants/#builder-pattern-implementation","title":"Builder Pattern Implementation","text":"<pre><code>class FinanceAssistant(Assistant):\n\n    ...\n\n    @classmethod\n    def builder(cls) -&gt; \"FinanceAssistantBuilder\":\n        \"\"\"Return a builder for FinanceAssistant.\"\"\"\n        return FinanceAssistantBuilder(cls)\n</code></pre> <p>Implement the builder pattern for fluent configuration of the assistant. This piece makes sure that when you call the builder() class methohd, instead of returning an instance of <code>FinanceAssistant</code> it will return an instance of <code>FinanceAssistantBuilder</code> which will configure the main assistant class's values.</p>"},{"location":"guide/getting-started-with-assistants/#workflow-construction","title":"Workflow Construction","text":"<pre><code>from grafi.nodes.node import Node\nfrom grafi.topics.input_topic import InputTopic\nfrom grafi.topics.output_topic import OutputTopic\nfrom grafi.tools.llms.impl.openai_tool import OpenAITool\nfrom grafi.workflows.impl.event_driven_workflow import EventDrivenWorkflow\n\n\n\nclass FinanceAssistant(Assistant):\n\n    ...\n\n\n    agent_input_topic = InputTopic(name=\"agent_input_topic\")\n\n    agent_output_topic = OutputTopic(name=\"agent_output_topic\")\n\n    def _construct_workflow(self) -&gt; \"FinanceAssistant\":\n        \"\"\"Construct the workflow for the assistant.\"\"\"\n        llm_node = (\n            Node.builder()\n            .name(\"OpenAINode\")\n            .subscribe(agent_input_topic)\n            .tool(\n                OpenAITool.builder()\n                .name(\"OpenAITool\")\n                .api_key(self.api_key)\n                .model(self.model)\n                .system_message(self.system_message)\n                .build()\n            )\n            .publish_to(agent_output_topic)\n            .build()\n        )\n\n        self.workflow = (\n            EventDrivenWorkflow.builder()\n            .name(\"FinanceWorkflow\")\n            .node(finance_llm_node)\n            .build()\n        )\n\n        return self\n</code></pre> <p>Here we implement the <code>_construct_workflow</code> method to create the internal workflow using the same pattern as the simple workflow guide, but now encapsulated within the assistant class. This method: - Creates an OpenAI node with instance-specific configuration. - Builds the event-driven workflow. - Stores the workflow as an instance variable.</p> <p>All the same principles that the simple workflow used apply here.</p>"},{"location":"guide/getting-started-with-assistants/#input-preparation","title":"Input Preparation","text":"<p>Prepare input data and context for workflow execution: - Creates a new <code>InvokeContext</code> if none is provided - Uses the global conversation ID for session continuity - Formats the user question as a <code>Message</code> object - Returns both the input data and context</p> <pre><code>from grafi.common.events.topic_events.publish_to_topic_event import PublishToTopicEvent\nfrom grafi.common.models.invoke_context import InvokeContext\nfrom typing import Optional\nfrom grafi.common.models.message import Message\n\nclass FinanceAssistant(Assistant):\n\n    ...\n\n    def get_input(self, question: str, invoke_context: Optional[InvokeContext] = None) -&gt; PublishToTopicEvent:\n        \"\"\"Prepare input data and invoke context.\"\"\"\n        if invoke_context is None:\n            logger.debug(\"Creating new InvokeContext with default conversation id for FinanceAssistant\")\n            invoke_context = InvokeContext(\n                user_id=uuid.uuid4().hex,\n                conversation_id=CONVERSATION_ID,\n                invoke_id=uuid.uuid4().hex,\n                assistant_request_id=uuid.uuid4().hex,\n            )\n\n        input_data = [\n            Message(\n                role=\"user\",\n                content=question,\n            )\n        ]\n\n        return PublishToTopicEvent(\n            invoke_context=execution_context, data=input_messages\n        )\n</code></pre> <p>This function is not part of the framework, but rather a helper function used to process inputs. It is not necessary, you are free to handle input as you wish.</p>"},{"location":"guide/getting-started-with-assistants/#assistant-execution","title":"Assistant Execution","text":"<pre><code>class FinanceAssistant(Assistant):\n\n    ...\n    async def run(self, question: str, invoke_context: Optional[InvokeContext] = None) -&gt; str:\n        \"\"\"Run the assistant with a question and return the response.\"\"\"\n        # Call helper function get_input()\n        input_event= self.get_input(question, invoke_context)\n        # This is the line that invokes the workflow\n        response_str = \"\"\n        async for output in super().invoke(input_event):\n            # Handle different content types\n            if output and len(output) &gt; 0:\n                content = output.data[0].content\n                if isinstance(content, str):\n                    response_str += content\n                elif content is not None:\n                    response_str += str(content)\n\n        return response_str\n</code></pre> <p>Main execution method that: - Prepares input data and context - Invokes the parent class's workflow execution <code>invoke()</code> method - Handles different response content types - Returns a clean string response</p>"},{"location":"guide/getting-started-with-assistants/#putting-it-all-together","title":"Putting it all together","text":"<p>Now that we have created the class for the assistance, we have to instantiate it and provide the fields in order to call it. A direct implementation of an assistant is as follows.</p> <pre><code># main.py\nimport asyncio\n\ndef main():\n    system_message = os.getenv(\"OPENAI_SYSTEM_MESSAGE\")\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    model = os.getenv(\"OPENAI_MODEL\")\n\n    builder = FinanceAssistant.builder()\n    assistant = (\n        builder\n        .system_message(system_message)\n        .model(model)\n        .api_key(api_key)\n        .build()\n    )\n\n    \"\"\"Main function to run the assistant.\"\"\"\n    user_input = \"What are the key factors to consider when choosing between a 401(k) and a Roth IRA?\"\n    result = await assistant.run(user_input)\n    print(\"Output message:\", result)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>A better approach would be to create a function that handles the creation of the agent so we can create multiple ones if needed.</p> <pre><code># main.py\ndef create_finance_assistant(\n    system_message: Optional[str] = None,\n    model: Optional[str] = None,\n    api_key: Optional[str] = None,\n) -&gt; FinanceAssistant:\n    \"\"\"Create a FinanceAssistant instance.\"\"\"\n    builder = FinanceAssistant.builder()\n\n    if system_message:\n        builder.system_message(system_message)\n    if model:\n        builder.model(model)\n    if api_key:\n        builder.api_key(api_key)\n\n    return builder.build()\n\ndef main():\n\n    system_message = os.getenv(\"OPENAI_SYSTEM_MESSAGE\")\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    model = os.getenv(\"OPENAI_MODEL\")\n# These values are for readability, as these values are already being set from the environment variables from within the class\n\n    assistant = create_finance_assistant(\n        system_message,\n        model,\n        api_key\n    )\n\n    \"\"\"Main function to run the assistant.\"\"\"\n    user_input = \"What are the key factors to consider when choosing between a 401(k) and a Roth IRA?\"\n    result = await assistant.run(user_input)\n    print(\"Output message:\", result)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guide/getting-started-with-assistants/#running-the-assistant","title":"Running the Assistant","text":"<p>To run this assistant example:</p> <ol> <li> <p>Set up environment variables: <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\nexport OPENAI_MODEL=\"gpt-4o\"  # Optional\nexport OPENAI_SYSTEM_MESSAGE=\"You are a knowledgeable financial advisor assistant. Help users with financial planning, investment analysis, market insights, and general financial questions. Provide accurate, helpful advice while emphasizing the importance of professional financial consultation for major decisions.\"  # Optional\n</code></pre></p> </li> <li> <p>Execute the script:</p> </li> </ol> <pre>\n<code>python main.py</code></pre> <ol> <li>Expected output: <pre><code>Output message:\nFinancial advice: When choosing between a 401(k) and a Roth IRA, it's important to consider several key factors. Each has its unique advantages and may be better suited to different financial situations and goals. Here are some factors to consider:\n\n1. **Tax Treatment**:\n- **401(k)**: Contributions are typically made with pre-tax dollars, which can lower your current taxable income. However, withdrawals during retirement are taxed as ordinary income.\n- **Roth IRA**: Contributions are made with after-tax dollars, meaning you pay taxes upfront. However, qualified withdrawals are tax-free, including the earnings.\n\n2. **Income and Contribution Limits**:\n- **401(k)**: As of 2023, the contribution limit is $22,500 annually, with an additional $7,500 catch-up contribution for those aged 50 and over. There are no income limits for 401(k) eligibility.\n- **Roth IRA**: The contribution limit is $6,500 annually, with a $1,000 catch-up contribution for those aged 50 and over. Contributions are restricted for high earners. For example, for 2023, contributions phase out for single filers with MAGI (Modified Adjusted Gross Income) between $138,000 and $153,000.\n</code></pre></li> </ol>"},{"location":"guide/getting-started-with-assistants/#key-benefits-of-the-assistant-pattern","title":"Key Benefits of the Assistant Pattern","text":""},{"location":"guide/getting-started-with-assistants/#1-encapsulation-and-reusability","title":"1. Encapsulation and Reusability","text":"<ul> <li>Workflow logic is encapsulated within the assistant class</li> <li>Easy to reuse across different applications</li> <li>Configuration is centralized and manageable</li> </ul>"},{"location":"guide/getting-started-with-assistants/#2-flexible-configuration","title":"2. Flexible Configuration","text":"<ul> <li>Environment variable support with defaults</li> <li>Builder pattern for fluent configuration</li> <li>Type-safe configuration with Pydantic</li> </ul>"},{"location":"guide/getting-started-with-assistants/#3-conversation-management","title":"3. Conversation Management","text":"<ul> <li>Built-in conversation ID management</li> <li>Context preservation across interactions</li> <li>Simplified input/output handling</li> </ul>"},{"location":"guide/getting-started-with-assistants/#4-production-readiness","title":"4. Production Readiness","text":"<ul> <li>Proper error handling and validation</li> <li>Logging integration</li> <li>Type hints for better IDE support</li> </ul>"},{"location":"guide/getting-started-with-assistants/#advanced-usage-examples","title":"Advanced Usage Examples","text":""},{"location":"guide/getting-started-with-assistants/#creating-multiple-assistant-instances","title":"Creating Multiple Assistant Instances","text":"<pre><code># Create specialized assistants for different use cases\nstock_assistant = create_finance_assistant(\n    system_message=\"You are a stock specialist,  you will process all contexst data and give out the best stock picks for a given date\",\n    model=\"gpt-4o\"\n)\n\ncurrency_assistant = create_finance_assistant(\n    system_message=\"You are a currency specialist, you will analyze all contexts and data to get the best currency trades possible\",\n    model=\"gpt-3.5-turbo\"\n)\n</code></pre>"},{"location":"guide/getting-started-with-assistants/#using-the-builder-pattern","title":"Using the Builder Pattern","text":"<pre><code>custom_assistant = (\n    FinanceAssistant.builder()\n    .api_key(\"your-api-key\")\n    .model(\"gpt-4o\")\n    .system_message(\"You are a custom assistant.\")\n    .build()\n)\n</code></pre>"},{"location":"guide/getting-started-with-assistants/#conversation-context-management","title":"Conversation Context Management","text":"<pre><code># Create a conversation context\ncontext = InvokeContext(\n    user_id=\"user123\",\n    conversation_id=\"conv456\",\n    invoke_id=uuid.uuid4().hex,\n    assistant_request_id=uuid.uuid4().hex,\n)\n\n# Use the same context across multiple interactions\nresponse1 = stock_assistant.run(\"Give me the best stock today to buy for a 10% gain in 2025\", context)\nresponse2 = currency_assistant.run(\"The lyra crashed, what's the best currency to exchange to?\", context)\n</code></pre>"},{"location":"guide/getting-started-with-assistants/#best-practices","title":"Best Practices","text":""},{"location":"guide/getting-started-with-assistants/#1-configuration-management","title":"1. Configuration Management","text":"<ul> <li>Use environment variables for sensitive data</li> <li>Provide sensible defaults</li> <li>Validate configuration in the constructor</li> </ul>"},{"location":"guide/getting-started-with-assistants/#2-error-handling","title":"2. Error Handling","text":"<ul> <li>Implement proper error handling in the <code>run</code> method</li> <li>Use logging for debugging and monitoring</li> <li>Provide meaningful error messages</li> </ul>"},{"location":"guide/getting-started-with-assistants/#3-type-safety","title":"3. Type Safety","text":"<ul> <li>Use Pydantic for configuration validation</li> <li>Implement proper type hints</li> <li>Use generic types for builder patterns</li> </ul>"},{"location":"guide/getting-started-with-assistants/#4-testing","title":"4. Testing","text":"<ul> <li>Create comprehensive test suites</li> <li>Test different conversation scenarios</li> <li>Mock external dependencies</li> </ul>"},{"location":"guide/getting-started-with-assistants/#next-steps","title":"Next Steps","text":"<p>With assistants implemented, you can:</p> <ol> <li>Build Complex Workflows: Create multi-node workflows within assistants</li> <li>Implement Custom Tools: Add specialized tools for specific use cases</li> <li>Add Conversation Memory: Implement persistent conversation storage</li> <li>Create Assistant Hierarchies: Build networks of cooperating assistants</li> <li>Add Monitoring: Implement comprehensive logging and metrics</li> <li>Use Grafi-Dev: Graphite's internal workflow UI</li> </ol> <p>The assistant pattern provides a solid foundation for building production-ready AI applications that are maintainable, testable, and scalable.</p>"},{"location":"user-guide/architecture/","title":"Architecture Overview","text":"<p>Graphite is an open-source framework designed for building domain-specific AI assistants through composable agentic workflows. We provide a highly extensible platform that empowers AI engineers to create custom workflows tailored to their specific business domains.</p> <p>In this documentation, we'll explore Graphite's architecture and each component in detail, while adhering to our core design philosophy: each component should only know what it needs to know. This principle of minimal coupling enables the flexibility and modularity that makes Graphite powerful.</p>"},{"location":"user-guide/architecture/#architecture","title":"Architecture","text":"<p>Below is an overview of the Graphite architecture. In the following sections, we'll break down each component and explore them in detail.</p> <p></p>"},{"location":"user-guide/assistant/","title":"Assistant","text":"<p>The Assistant serves as the primary interface between users and the underlying workflow system. It processes user input, manages workflows, and coordinates interactions between users and workflow components. Assistants use language models to process input and generate responses through structured workflows.</p>"},{"location":"user-guide/assistant/#architecture-overview","title":"Architecture Overview","text":"<p>Assistants in Graphite follow a two-tier architecture:</p> <ol> <li>AssistantBase - Abstract base class defining the core interface and properties</li> <li>Assistant - Concrete implementation that handles workflow execution and message processing</li> </ol>"},{"location":"user-guide/assistant/#assistantbase-class","title":"AssistantBase Class","text":"<p>The <code>AssistantBase</code> class provides an abstract foundation for all assistants, defining essential properties and methods that must be implemented by concrete assistant classes.</p>"},{"location":"user-guide/assistant/#class-configuration","title":"Class Configuration","text":"<pre><code>from grafi.assistants.assistant_base import AssistantBase\nfrom grafi.common.events.topic_events.publish_to_topic_event import PublishToTopicEvent\nfrom grafi.common.events.topic_events.consume_from_topic_event import ConsumeFromTopicEvent\nfrom typing import List, AsyncGenerator\n\nclass MyAssistant(AssistantBase):\n    def _construct_workflow(self):\n        # Implementation required\n        pass\n\n    async def invoke(self, input_data: PublishToTopicEvent) -&gt; AsyncGenerator[ConsumeFromTopicEvent, None]:\n        # Implementation required\n        pass\n</code></pre>"},{"location":"user-guide/assistant/#assistantbase-fields","title":"AssistantBase Fields","text":"Field Type Default Description <code>assistant_id</code> <code>str</code> <code>default_id</code> Unique identifier for the assistant instance <code>name</code> <code>str</code> <code>\"Assistant\"</code> Human-readable name for the assistant <code>type</code> <code>str</code> <code>\"assistant\"</code> Category or type specification for the assistant <code>oi_span_type</code> <code>OpenInferenceSpanKindValues</code> <code>AGENT</code> OpenInference span type for distributed tracing <code>workflow</code> <code>Workflow</code> <code>Workflow()</code> Associated workflow instance managed by the assistant"},{"location":"user-guide/assistant/#required-methods","title":"Required Methods","text":"<p>Subclasses must implement these abstract methods:</p> Method Signature Description <code>_construct_workflow</code> <code>() -&gt; AssistantBase</code> Constructs and configures the assistant's workflow <code>invoke</code> <code>(PublishToTopicEvent) -&gt; AsyncGenerator[ConsumeFromTopicEvent, None]</code> Asynchronous event processing with streaming support"},{"location":"user-guide/assistant/#lifecycle","title":"Lifecycle","text":"<p>The <code>AssistantBase</code> automatically calls <code>_construct_workflow()</code> during initialization via <code>model_post_init()</code>, ensuring the workflow is properly configured before the assistant is used.</p>"},{"location":"user-guide/assistant/#assistant-class","title":"Assistant Class","text":"<p>The concrete <code>Assistant</code> class extends <code>AssistantBase</code> and provides a complete implementation for workflow-based message processing. It includes automatic event recording and tracing through decorators.</p>"},{"location":"user-guide/assistant/#implementation-features","title":"Implementation Features","text":"<ul> <li>Automatic Event Recording: Uses <code>@record_assistant_invoke</code> decorator</li> <li>Workflow Delegation: Delegates all processing to the configured workflow</li> <li>Manifest Generation: Supports generating configuration manifests</li> <li>Serialization: Provides dictionary serialization capabilities</li> </ul>"},{"location":"user-guide/assistant/#assistant-methods","title":"Assistant Methods","text":"Method Signature Description <code>invoke</code> <code>(PublishToTopicEvent) -&gt; AsyncGenerator[ConsumeFromTopicEvent, None]</code> Processes input events asynchronously with streaming support <code>to_dict</code> <code>() -&gt; dict[str, Any]</code> Serializes the assistant's workflow configuration <code>generate_manifest</code> <code>(output_dir: str = \".\") -&gt; str</code> Generates a JSON manifest file for the assistant"},{"location":"user-guide/assistant/#method-details","title":"Method Details","text":""},{"location":"user-guide/assistant/#invoke","title":"invoke()","text":"<p>Asynchronously processes input events with support for streaming responses.</p> <pre><code>@record_assistant_invoke\nasync def invoke(self, input_data: PublishToTopicEvent) -&gt; AsyncGenerator[ConsumeFromTopicEvent, None]:\n    async for output in self.workflow.invoke(input_data):\n        yield output\n</code></pre> <p>Parameters:</p> <ul> <li><code>input_data</code>: A <code>PublishToTopicEvent</code> containing the data to be processed and context information</li> </ul> <p>Returns: Async generator yielding <code>ConsumeFromTopicEvent</code> objects</p> <p>Use Cases: Streaming responses, real-time processing, concurrent operations</p>"},{"location":"user-guide/assistant/#generate_manifest","title":"generate_manifest()","text":"<p>Creates a JSON manifest file containing the assistant's configuration.</p> <pre><code>def generate_manifest(self, output_dir: str = \".\") -&gt; str:\n    manifest_dict = self.to_dict()\n    output_path = os.path.join(output_dir, f\"{self.name}_manifest.json\")\n    # Writes JSON file\n    return output_path\n</code></pre> <p>Parameters:</p> <ul> <li><code>output_dir</code>: Directory for the manifest file (default: current directory)</li> </ul> <p>Returns: Path to the generated manifest file</p>"},{"location":"user-guide/assistant/#assistantbasebuilder-class","title":"AssistantBaseBuilder Class","text":"<p>The <code>AssistantBaseBuilder</code> provides a fluent interface for constructing assistant instances with proper configuration.</p>"},{"location":"user-guide/assistant/#builder-methods","title":"Builder Methods","text":"Method Parameters Description <code>oi_span_type</code> <code>OpenInferenceSpanKindValues</code> Sets the OpenInference span type for tracing <code>name</code> <code>str</code> Sets the assistant's name <code>type</code> <code>str</code> Sets the assistant's type category <code>event_store</code> <code>EventStore</code> Registers an event store for event recording"},{"location":"user-guide/assistant/#usage-example","title":"Usage Example","text":"<pre><code>from grafi.assistants.assistant_base import AssistantBaseBuilder\nfrom grafi.common.event_stores.in_memory_event_store import InMemoryEventStore\nfrom openinference.semconv.trace import OpenInferenceSpanKindValues\n\nbuilder = AssistantBaseBuilder(MyAssistant)\nassistant = (builder\n    .name(\"Customer Support Assistant\")\n    .type(\"support\")\n    .oi_span_type(OpenInferenceSpanKindValues.AGENT)\n    .event_store(InMemoryEventStore())\n    .build())\n</code></pre>"},{"location":"user-guide/builder-pattern/","title":"Builder Pattern","text":"<p>The Builder Pattern is a core design pattern used throughout Graphite to provide a fluent, type-safe, and consistent way to construct complex objects such as Assistants, Tools, Nodes, Workflows, and Topics. This pattern enables clean, readable object creation with method chaining while ensuring proper validation and configuration.</p>"},{"location":"user-guide/builder-pattern/#overview","title":"Overview","text":"<p>Graphite implements the Builder Pattern using a composition-based approach where builders are separate classes that construct target objects using kwargs. This design provides:</p> <ul> <li>Fluent Interface: Method chaining for readable configuration</li> <li>Type Safety: Compile-time type checking with proper return types</li> <li>Separation of Concerns: Builders are independent of the target classes</li> <li>Validation: Centralized validation logic in the <code>build()</code> method</li> <li>Consistency: Uniform construction pattern across all components</li> </ul>"},{"location":"user-guide/builder-pattern/#core-architecture","title":"Core Architecture","text":""},{"location":"user-guide/builder-pattern/#basebuilder-generic","title":"BaseBuilder (Generic)","text":"<p>The foundation of all builders in Graphite is the <code>BaseBuilder</code> class:</p> <pre><code>from typing import Any, Generic, TypeVar\nfrom pydantic import BaseModel\n\nT = TypeVar(\"T\", bound=BaseModel)\n\nclass BaseBuilder(Generic[T]):\n    \"\"\"Generic builder that can build *any* Pydantic model.\"\"\"\n\n    kwargs: dict[str, Any] = {}\n    _cls: type[T]\n\n    def __init__(self, cls: type[T]) -&gt; None:\n        self._cls = cls\n        self.kwargs = {}\n\n    def build(self) -&gt; T:\n        \"\"\"Return the fully configured product.\"\"\"\n        return self._cls(**self.kwargs)\n</code></pre> <p>Key Design Principles:</p> <ul> <li>kwargs-based: Builders accumulate configuration in a <code>kwargs</code> dictionary</li> <li>Generic: Single base class handles all Pydantic model types</li> <li>Immutable Target: Objects are constructed once with all parameters</li> <li>Type Safety: Generic type parameter ensures type safety</li> </ul>"},{"location":"user-guide/builder-pattern/#implementation-guide","title":"Implementation Guide","text":""},{"location":"user-guide/builder-pattern/#creating-a-new-builder","title":"Creating a New Builder","text":"<p>When implementing the builder pattern for a new component, follow these steps:</p>"},{"location":"user-guide/builder-pattern/#1-define-your-component-class","title":"1. Define Your Component Class","text":"<p>First, create your Pydantic model without any builder methods:</p> <pre><code>from typing import Optional\nfrom pydantic import BaseModel, Field\n\nclass DatabaseConnection(BaseModel):\n    \"\"\"Database connection configuration.\"\"\"\n\n    host: str\n    port: int = Field(default=5432)\n    database: str\n    username: str\n    password: Optional[str] = Field(default=None)\n    ssl_enabled: bool = Field(default=True)\n    timeout: int = Field(default=30)\n</code></pre>"},{"location":"user-guide/builder-pattern/#2-create-the-builder-class","title":"2. Create the Builder Class","text":"<p>Create a separate builder class that extends the appropriate base builder, and do not override the <code>build()</code> methods if no advanced type checks. For post initialization checks or operations, use pydantic <code>model_post_init()</code> instead.</p> <pre><code>from typing import Self, TypeVar, Optional\n\nT_DB = TypeVar(\"T_DB\", bound=DatabaseConnection)\n\nclass DatabaseConnectionBuilder(BaseBuilder[T_DB]):\n    \"\"\"Builder for DatabaseConnection instances.\"\"\"\n\n    def host(self, host: str) -&gt; Self:\n        self.kwargs[\"host\"] = host\n        return self\n\n    def port(self, port: int) -&gt; Self:\n        self.kwargs[\"port\"] = port\n        return self\n\n    def database(self, database: str) -&gt; Self:\n        self.kwargs[\"database\"] = database\n        return self\n\n    def username(self, username: str) -&gt; Self:\n        self.kwargs[\"username\"] = username\n        return self\n\n    def password(self, password: str) -&gt; Self:\n        self.kwargs[\"password\"] = password\n        return self\n\n    def ssl_enabled(self, enabled: bool) -&gt; Self:\n        self.kwargs[\"ssl_enabled\"] = enabled\n        return self\n\n    def timeout(self, timeout: int) -&gt; Self:\n        self.kwargs[\"timeout\"] = timeout\n        return self\n</code></pre>"},{"location":"user-guide/builder-pattern/#3-add-builder-class-to-its-object-class","title":"3. Add Builder class to its object class","text":"<pre><code>from typing import Optional\nfrom pydantic import BaseModel, Field\n\nclass DatabaseConnection(BaseModel):\n    \"\"\"Database connection configuration.\"\"\"\n\n    host: str\n    port: int = Field(default=5432)\n    database: str\n    username: str\n    password: Optional[str] = Field(default=None)\n    ssl_enabled: bool = Field(default=True)\n    timeout: int = Field(default=30)\n\n    @classmethod\n    def builder(cls) -&gt; \"DatabaseConnectionBuilder\":\n        \"\"\"Return a builder for DatabaseConnectionBuilder.\"\"\"\n        return DatabaseConnectionBuilder(cls)\n</code></pre>"},{"location":"user-guide/builder-pattern/#4-usage-examples","title":"4. Usage Examples","text":"<p>Here's how to use the builder:</p> <pre><code># Basic usage\ndb_config = (DatabaseConnection.builder()\n    .host(\"localhost\")\n    .database(\"myapp\")\n    .username(\"user\")\n    .password(\"secret\")\n    .build())\n\n# With optional parameters\ndb_config = (DatabaseConnection.builder()\n    .host(\"prod-db.example.com\")\n    .port(3306)\n    .database(\"production\")\n    .username(\"app_user\")\n    .password(\"secure_password\")\n    .ssl_enabled(True)\n    .timeout(60)\n    .build())\n\n# Error handling\ntry:\n    db_config = (DatabaseConnection.builder()\n        .host(\"localhost\")\n        # Missing required database and username\n        .build())\nexcept ValueError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"user-guide/builder-pattern/#advanced-builder-patterns","title":"Advanced Builder Patterns","text":""},{"location":"user-guide/builder-pattern/#builder-with-complex-validation","title":"Builder with Complex Validation","text":"<p>For components with complex validation rules:</p> <pre><code>class EmailServerBuilder(BaseBuilder[EmailServer]):\n    \"\"\"Builder for EmailServer with complex validation.\"\"\"\n\n    def build(self) -&gt; EmailServer:\n        \"\"\"Build with comprehensive validation.\"\"\"\n        # Required field validation\n        required_fields = [\"host\", \"port\", \"sender_email\"]\n        for field in required_fields:\n            if field not in self.kwargs:\n                raise ValueError(f\"{field} is required\")\n\n        # Conditional validation\n        if self.kwargs.get(\"use_tls\", False) and self.kwargs.get(\"port\") == 25:\n            raise ValueError(\"TLS cannot be used with port 25\")\n\n        # Cross-field validation\n        if self.kwargs.get(\"auth_required\", False):\n            if not self.kwargs.get(\"username\") or not self.kwargs.get(\"password\"):\n                raise ValueError(\"username and password required when auth_required=True\")\n\n        # Format validation\n        email = self.kwargs.get(\"sender_email\", \"\")\n        if \"@\" not in email:\n            raise ValueError(\"sender_email must be a valid email address\")\n\n        return super().build()\n</code></pre>"},{"location":"user-guide/builder-pattern/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/builder-pattern/#1-separation-of-concerns","title":"1. Separation of Concerns","text":"<p>Do: Keep builders separate from the target classes</p> <pre><code># Good - Builder is separate\nclass MyComponent(BaseModel):\n    name: str\n    value: int\n\nclass MyComponentBuilder(BaseBuilder[MyComponent]):\n    def name(self, name: str) -&gt; Self:\n        self.kwargs[\"name\"] = name\n        return self\n</code></pre> <p>Don't: Mix builder methods into the target class</p> <pre><code># Bad - Builder methods in target class\nclass MyComponent(BaseModel):\n    name: str\n    value: int\n\n    def with_name(self, name: str) -&gt; Self:  # Don't do this\n        self.name = name\n        return self\n</code></pre>"},{"location":"user-guide/builder-pattern/#2-validation-in-build","title":"2. Validation in build()","text":"<p>Perform validation in the <code>build()</code> method if have to, pydantic will validate the required fields.</p> <pre><code>def build(self) -&gt; MyComponent:\n    \"\"\"Build with validation.\"\"\"\n    # Validate required fields\n    if \"name\" not in self.kwargs:\n        raise ValueError(\"name is required\")\n\n    # Validate business rules\n    if self.kwargs.get(\"value\", 0) &lt; 0:\n        raise ValueError(\"value must be non-negative\")\n\n    return super().build()\n</code></pre>"},{"location":"user-guide/builder-pattern/#3-type-safety","title":"3. Type Safety","text":"<p>Use proper type annotations and generics:</p> <pre><code>T_MC = TypeVar(\"T_MC\", bound=MyComponent)\n\nclass MyComponentBuilder(BaseBuilder[T_MC]):\n    def name(self, name: str) -&gt; Self:  # Returns Self for chaining\n        self.kwargs[\"name\"] = name\n        return self\n</code></pre>"},{"location":"user-guide/builder-pattern/#4-error-messages","title":"4. Error Messages","text":"<p>Provide clear, actionable error messages:</p> <pre><code>def build(self) -&gt; MyComponent:\n    if \"host\" not in self.kwargs:\n        raise ValueError(\"host is required. Use .host('hostname') to set it.\")\n\n    if self.kwargs.get(\"port\", 0) &lt;= 0:\n        raise ValueError(\"port must be positive. Use .port(8080) to set a valid port.\")\n</code></pre>"},{"location":"user-guide/builder-pattern/#integration-with-existing-components","title":"Integration with Existing Components","text":"<p>When working with Graphite's existing components, use their provided builders:</p> <pre><code># Assistant construction\nassistant = (MyAssistant.builder()\n    .name(\"Customer Support\")\n    .type(\"support\")\n    .oi_span_type(OpenInferenceSpanKindValues.AGENT)\n    .event_store(InMemoryEventStore())\n    .build())\n\n# Workflow construction\nworkflow = (EventDrivenWorkflow.builder()\n    .name(\"Processing Pipeline\")\n    .node(preprocessing_node)\n    .node(llm_node)\n    .node(postprocessing_node)\n    .build())\n\n# Topic construction\ntopic = (Topic.builder()\n    .name(\"user_input\")\n    .condition(lambda msgs: len(msgs) &gt; 0)\n    .build())\n</code></pre>"},{"location":"user-guide/builder-pattern/#summary","title":"Summary","text":"<p>The Builder Pattern in Graphite provides a consistent, type-safe way to construct complex objects through:</p> <ul> <li>Separation: Builders are independent classes, not mixed into target objects</li> <li>Parameter-based Construction: All configuration accumulates in a parameters dictionary</li> <li>Generic Base: Single <code>BaseBuilder</code> class handles all model types</li> <li>Validation: Centralized validation logic in the <code>build()</code> method</li> <li>Type Safety: Proper generics and type annotations throughout</li> </ul> <p>This pattern enables readable, maintainable object construction while ensuring proper validation and configuration management across the entire framework.</p>"},{"location":"user-guide/command/","title":"Command","text":"<p>The Command implements the Command Pattern in Graphite, providing a crucial abstraction layer that separates workflow orchestration (Nodes) from execution logic (Tools). Commands encapsulate the execution logic and data preparation, allowing nodes to delegate processing to tools without needing to understand the internal implementation details.</p>"},{"location":"user-guide/command/#architecture-overview","title":"Architecture Overview","text":"<p>The Command Pattern in Graphite creates a clear separation between:</p> <ul> <li>Orchestration Layer: Nodes manage workflow execution and topic-based messaging</li> <li>Execution Layer: Tools perform the actual processing (LLM calls, function execution, etc.)</li> <li>Command Layer: Commands bridge the gap, handling data transformation and tool invocation</li> </ul> <p>This architecture enables flexible, testable, and maintainable workflows where components can be easily swapped, extended, or customized.</p>"},{"location":"user-guide/command/#core-benefits","title":"Core Benefits","text":""},{"location":"user-guide/command/#1-separation-of-concerns","title":"1. Separation of Concerns","text":"<p>Commands isolate tool invocation logic from workflow orchestration, making the system more modular and easier to understand.</p>"},{"location":"user-guide/command/#2-data-transformation","title":"2. Data Transformation","text":"<p>Commands handle the complex task of transforming topic event data into the format expected by tools, including:</p> <ul> <li>Message aggregation from multiple topic events</li> <li>Conversation history reconstruction</li> <li>Tool call message ordering</li> <li>Context-specific data preparation</li> </ul>"},{"location":"user-guide/command/#3-automatic-tool-registration","title":"3. Automatic Tool Registration","text":"<p>The <code>@use_command</code> decorator provides automatic command registration for tool types:</p> <pre><code>@use_command(LLMCommand)\nclass MyLLMTool(LLM):\n    # Tool implementation\n    pass\n\n# Command is automatically created when tool is used\ncommand = Command.for_tool(my_llm_tool)  # Returns LLMCommand instance\n</code></pre>"},{"location":"user-guide/command/#4-flexibility-and-extensibility","title":"4. Flexibility and Extensibility","text":"<p>Tools can be easily swapped without changing workflow structure, and new command types can be added for specialized processing needs.</p>"},{"location":"user-guide/command/#5-improved-testability","title":"5. Improved Testability","text":"<p>Commands can be tested independently from workflows and nodes, enabling better unit testing and debugging.</p>"},{"location":"user-guide/command/#base-command-class","title":"Base Command Class","text":"<p>The <code>Command</code> base class provides the foundational interface:</p> <pre><code>class Command(BaseModel):\n    \"\"\"Base command class for tool execution.\"\"\"\n\n    tool: Tool\n\n    async def invoke(self, invoke_context: InvokeContext,\n                       input_data: List[ConsumeFromTopicEvent]) -&gt; MsgsAGen:\n        \"\"\"Asynchronous tool invocation.\"\"\"\n        async for message in self.tool.invoke(\n            invoke_context,\n            self.get_tool_input(invoke_context, input_data)\n        ):\n            yield message\n\n    def get_tool_input(self, invoke_context: InvokeContext,\n                       input_data: List[ConsumeFromTopicEvent]) -&gt; Messages:\n        \"\"\"Transform topic events into tool input format.\"\"\"\n        all_messages = []\n        for event in input_data:\n            all_messages.extend(event.data)\n        return all_messages\n</code></pre>"},{"location":"user-guide/command/#key-methods","title":"Key Methods","text":"Method Description <code>invoke</code> Asynchronous tool execution supporting streaming <code>get_tool_input</code> Transforms topic events into tool-compatible format <code>to_dict</code> Serializes command state for persistence or debugging"},{"location":"user-guide/command/#factory-method","title":"Factory Method","text":"<p>The <code>Command.for_tool()</code> factory method automatically selects the appropriate command class:</p> <pre><code># Automatic command selection based on tool type\nllm_command = Command.for_tool(my_llm_tool)      # Returns LLMCommand\nfunc_command = Command.for_tool(my_func_tool)    # Returns FunctionCallCommand\nbase_command = Command.for_tool(generic_tool)    # Returns Command\n</code></pre>"},{"location":"user-guide/command/#built-in-command-types","title":"Built-in Command Types","text":""},{"location":"user-guide/command/#llmcommand","title":"LLMCommand","text":"<p>The <code>LLMCommand</code> handles complex data preparation for Language Model tools, including conversation history and tool call ordering. This command automatically applies sophisticated data preparation logic specific to LLM interactions.</p> <p>Key Features:</p> <ul> <li>Conversation History Reconstruction: Retrieves and orders conversation history from previous assistant responses</li> <li>Tool Call Message Ordering: Ensures tool call responses immediately follow their corresponding LLM tool calls</li> <li>Event Graph Processing: Uses topological sorting to maintain proper message chronology</li> <li>Context-Aware Data Preparation: Filters out current request data to prevent circular references</li> </ul> <p>Data Processing Flow:</p> <ol> <li>Retrieves conversation history from the event store</li> <li>Filters out messages from the current assistant request</li> <li>Processes current topic events using event graph topology</li> <li>Reorders tool call messages to follow their corresponding LLM messages</li> <li>Combines and sorts all messages by timestamp</li> </ol> <p>Use Cases:</p> <ul> <li>Conversational AI assistants with memory</li> <li>Context-aware language model interactions</li> </ul>"},{"location":"user-guide/command/#functioncallcommand","title":"FunctionCallCommand","text":"<p>The <code>FunctionCallCommand</code> processes tool call messages for function execution, extracting unprocessed function calls from topic events.</p> <p>Key Features:</p> <ul> <li>Unprocessed Tool Call Detection: Identifies tool calls that haven't been processed yet</li> <li>Duplicate Prevention: Filters out tool call messages that already have responses</li> <li>Event Processing: Handles messages from nodes in the workflow</li> </ul> <p>Data Processing Logic:</p> <pre><code>def get_tool_input(self, _: InvokeContext,\n                   node_input: List[ConsumeFromTopicEvent]) -&gt; Messages:\n    # Extract all input messages from events\n    input_messages = [msg for event in node_input for msg in event.data]\n\n    # Find already processed tool calls\n    processed_tool_calls = [msg.tool_call_id for msg in input_messages if msg.tool_call_id]\n\n    # Return only unprocessed tool call messages\n    tool_calls_messages = []\n    for message in input_messages:\n        if (message.tool_calls and\n            message.tool_calls[0].id not in processed_tool_calls):\n            tool_calls_messages.append(message)\n\n    return tool_calls_messages\n</code></pre> <p>Use Cases:</p> <ul> <li>Function calling in LLM workflows</li> <li>Tool execution based on model-generated tool calls  </li> <li>Structured function invocation with parameter extraction</li> </ul>"},{"location":"user-guide/command/#example-command-implementations","title":"Example Command Implementations","text":"<p>These examples show commands from test integrations that demonstrate specialized data preparation patterns.</p>"},{"location":"user-guide/command/#embeddingresponsecommand","title":"EmbeddingResponseCommand","text":"<p>The <code>EmbeddingResponseCommand</code> is used in test integrations for embedding-based retrieval tasks. It extracts the latest message for embedding processing:</p> <pre><code>class EmbeddingResponseCommand(Command):\n    def get_tool_input(self, invoke_context: InvokeContext,\n                       node_input: List[ConsumeFromTopicEvent]) -&gt; Messages:\n        # Only consider the last message contains the content to query\n        latest_event_data = node_input[-1].data\n        latest_message = (\n            latest_event_data[0]\n            if isinstance(latest_event_data, list)\n            else latest_event_data\n        )\n        return [latest_message]\n</code></pre> <p>Key Features:</p> <ul> <li>Latest Message Extraction: Focuses on the most recent message for processing</li> <li>Simple Data Preparation: Minimal transformation for embedding queries</li> </ul>"},{"location":"user-guide/command/#ragresponsecommand","title":"RagResponseCommand","text":"<p>The <code>RagResponseCommand</code> is used in test integrations for retrieval-augmented generation tasks. Similar to <code>EmbeddingResponseCommand</code>, it extracts the latest message:</p> <pre><code>class RagResponseCommand(Command):\n    def get_tool_input(self, invoke_context: InvokeContext,\n                       node_input: List[ConsumeFromTopicEvent]) -&gt; Messages:\n        # Only consider the last message contains the content to query\n        latest_event_data = node_input[-1].data\n        latest_message = (\n            latest_event_data[0]\n            if isinstance(latest_event_data, list)\n            else latest_event_data\n        )\n        return [latest_message]\n</code></pre> <p>Key Features:</p> <ul> <li>Query Focus: Extracts the latest user query for RAG processing</li> <li>Streamlined Input: Provides clean input for retrieval-augmented generation</li> </ul> Method Description <code>invoke(invoke_context, input_data)</code> Calls the <code>function_tool</code>'s asynchronous <code>invoke</code>, yielding one or more <code>Message</code> objects in an async generator. <code>get_function_specs()</code> Retrieves the function specifications (schema, name, parameters) from the underlying <code>function_tool</code>. <code>to_dict()</code> Serializes the command\u2019s current state, including the <code>function_tool</code> configuration. <p>By passing a <code>FunctionCallTool</code> to the <code>function_tool</code> field, you can seamlessly integrate function-based logic into a Node\u2019s orchestration without embedding invoke details in the Node or the tool consumer. This separation keeps workflows flexible and easy to extend.</p>"},{"location":"user-guide/command/#embedding-response-command-and-rag-response-command","title":"Embedding Response Command and RAG Response Command","text":"<p><code>EmbeddingResponseCommand</code> encapsulates a <code>RetrievalTool</code> for transforming input messages into embeddings, retrieving relevant content, and returning it as a <code>Message</code>. This command is used by <code>EmbeddingRetrievalNode</code>.</p> <p><code>EmbeddingResponseCommand</code> fields:</p> Field Description <code>retrieval_tool</code> A <code>RetrievalTool</code> instance for embedding-based lookups, returning relevant data <p><code>EmbeddingResponseCommand</code> methods:</p> Method Description <code>invoke(invoke_context, input_data)</code> Asynchronously calls <code>retrieval_tool.invoke</code>, yielding one or more <code>Message</code> objects. <code>to_dict()</code> Serializes the command\u2019s state, including the <code>retrieval_tool</code> configuration. <p><code>RagResponseCommand</code> similarly delegates to a <code>RagTool</code> that performs retrieval-augmented generation. This command is used by <code>RagNode</code>.</p> <p><code>RagResponseCommand</code> fields:</p> Field Description <code>rag_tool</code> A <code>RagTool</code> instance for retrieval-augmented generation. <p><code>RagResponseCommand</code> methods:</p> Method Description <code>invoke(invoke_context, input_data)</code> Asynchronously invokes <code>rag_tool.invoke</code>, yielding partial or complete messages from the retrieval-augmented flow. <code>to_dict()</code> Serializes the command\u2019s state, reflecting the assigned <code>RagTool</code> configuration. <p>Both commands enable a node to delegate specialized retrieval operations to their respective tools, without needing to manage the internal logic of how embeddings or RAG processes are performed.</p>"},{"location":"user-guide/command/#command-registration","title":"Command Registration","text":""},{"location":"user-guide/command/#using-the-use_command-decorator","title":"Using the @use_command Decorator","text":"<p>Register custom commands for specific tool types:</p> <pre><code>from grafi.common.models.command import use_command\n\n@use_command(MyCustomCommand)\nclass MySpecialTool(Tool):\n    \"\"\"Tool that requires special data preparation.\"\"\"\n    pass\n\nclass MyCustomCommand(Command):\n    def get_tool_input(self, invoke_context: InvokeContext,\n                       input_data: List[ConsumeFromTopicEvent]) -&gt; Messages:\n        # Custom data transformation logic\n        return transformed_messages\n</code></pre>"},{"location":"user-guide/command/#registry-lookup","title":"Registry Lookup","text":"<p>The command registry uses inheritance-based lookup:</p> <pre><code># Registry checks for exact match first\nTOOL_COMMAND_REGISTRY[MySpecialTool] = MyCustomCommand\n\n# Then checks parent classes\nif isinstance(tool, RegisteredToolType):\n    return AssociatedCommand(tool=tool)\n\n# Falls back to base Command\nreturn Command(tool=tool)\n</code></pre>"},{"location":"user-guide/command/#creating-custom-commands","title":"Creating Custom Commands","text":""},{"location":"user-guide/command/#when-to-create-custom-commands","title":"When to Create Custom Commands","text":"<p>Create custom commands when you need:</p> <ol> <li>Specialized Data Preparation: Complex transformation of topic events into tool input</li> <li>Context-Specific Logic: Tool invocation that depends on workflow context</li> <li>Multi-Source Data: Aggregating data from multiple sources beyond topic events</li> <li>Custom Error Handling: Specialized error processing or recovery logic</li> <li>Performance Optimization: Optimized data processing for specific use cases</li> </ol>"},{"location":"user-guide/command/#implementation-guide","title":"Implementation Guide","text":""},{"location":"user-guide/command/#1-define-your-custom-command","title":"1. Define Your Custom Command","text":"<pre><code>from typing import List\nfrom grafi.common.models.command import Command\nfrom grafi.common.events.topic_events.consume_from_topic_event import ConsumeFromTopicEvent\nfrom grafi.common.models.invoke_context import InvokeContext\nfrom grafi.common.models.message import Messages\n\nclass DatabaseQueryCommand(Command):\n    \"\"\"Command for database query tools with caching and optimization.\"\"\"\n\n    def get_tool_input(self, invoke_context: InvokeContext,\n                       input_data: List[ConsumeFromTopicEvent]) -&gt; Messages:\n        # Extract query parameters from messages\n        query_messages = []\n        for event in input_data:\n            for message in event.data:\n                if message.content and \"query:\" in message.content:\n                    query_messages.append(message)\n\n        # Add context-specific optimizations\n        if invoke_context.metadata.get(\"use_cache\"):\n            query_messages = self._add_cache_hints(query_messages)\n\n        return query_messages\n\n    def _add_cache_hints(self, messages: Messages) -&gt; Messages:\n        \"\"\"Add caching hints to query messages.\"\"\"\n        # Custom caching logic\n        return messages\n</code></pre>"},{"location":"user-guide/command/#2-register-the-command","title":"2. Register the Command","text":"<pre><code>@use_command(DatabaseQueryCommand)\nclass DatabaseQueryTool(Tool):\n    \"\"\"Tool for executing database queries.\"\"\"\n\n    def invoke(self, invoke_context: InvokeContext, input_data: Messages) -&gt; Messages:\n        # Database query implementation\n        pass\n</code></pre>"},{"location":"user-guide/command/#3-advanced-custom-command-with-multiple-data-sources","title":"3. Advanced Custom Command with Multiple Data Sources","text":"<pre><code>class MultiSourceCommand(Command):\n    \"\"\"Command that aggregates data from multiple sources.\"\"\"\n\n    def get_tool_input(self, invoke_context: InvokeContext,\n                       input_data: List[ConsumeFromTopicEvent]) -&gt; Messages:\n        # 1. Get base messages from topic events\n        base_messages = super().get_tool_input(invoke_context, input_data)\n\n        # 2. Retrieve external context\n        external_data = self._get_external_context(invoke_context)\n\n        # 3. Combine and optimize\n        combined_messages = self._combine_data_sources(\n            base_messages,\n            external_data,\n            invoke_context\n        )\n\n        return combined_messages\n\n    def _get_external_context(self, invoke_context: InvokeContext) -&gt; Messages:\n        \"\"\"Retrieve additional context from external sources.\"\"\"\n        # Fetch from databases, APIs, files, etc.\n        return external_messages\n\n    def _combine_data_sources(self, base: Messages, external: Messages,\n                              context: InvokeContext) -&gt; Messages:\n        \"\"\"Intelligently combine multiple data sources.\"\"\"\n        # Custom combination logic\n        return combined_messages\n</code></pre>"},{"location":"user-guide/command/#advanced-usage-patterns","title":"Advanced Usage Patterns","text":""},{"location":"user-guide/command/#1-conditional-command-selection","title":"1. Conditional Command Selection","text":"<pre><code>class ConditionalCommand(Command):\n    \"\"\"Command that adapts behavior based on context.\"\"\"\n\n    def get_tool_input(self, invoke_context: InvokeContext,\n                       input_data: List[ConsumeFromTopicEvent]) -&gt; Messages:\n        if invoke_context.metadata.get(\"mode\") == \"streaming\":\n            return self._prepare_streaming_input(input_data)\n        elif invoke_context.metadata.get(\"mode\") == \"batch\":\n            return self._prepare_batch_input(input_data)\n        else:\n            return super().get_tool_input(invoke_context, input_data)\n</code></pre>"},{"location":"user-guide/command/#2-command-with-state-management","title":"2. Command with State Management","text":"<pre><code>class StatefulCommand(Command):\n    \"\"\"Command that maintains state across invocations.\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._state_cache = {}\n\n    def get_tool_input(self, invoke_context: InvokeContext,\n                       input_data: List[ConsumeFromTopicEvent]) -&gt; Messages:\n        request_id = invoke_context.assistant_request_id\n\n        # Use cached state if available\n        if request_id in self._state_cache:\n            return self._update_with_cache(input_data, self._state_cache[request_id])\n\n        # Create new state entry\n        processed_data = self._process_fresh_input(input_data)\n        self._state_cache[request_id] = processed_data\n\n        return processed_data\n</code></pre>"},{"location":"user-guide/command/#3-command-with-error-recovery","title":"3. Command with Error Recovery","text":"<pre><code>class ResilientCommand(Command):\n    \"\"\"Command with built-in error recovery.\"\"\"\n\n    async def invoke(self, invoke_context: InvokeContext,\n                       input_data: List[ConsumeFromTopicEvent]) -&gt; MsgsAGen:\n        max_retries = 3\n        retry_count = 0\n\n        while retry_count &lt; max_retries:\n            try:\n                tool_input = self.get_tool_input(invoke_context, input_data)\n                async for message in self.tool.invoke(invoke_context, tool_input):\n                    yield message\n                break\n            except Exception as e:\n                retry_count += 1\n                if retry_count &gt;= max_retries:\n                    # Yield error message as fallback\n                    yield [Message(role=\"assistant\", content=f\"Error: {str(e)}\")]\n                else:\n                    # Modify input for retry\n                    input_data = self._prepare_retry_input(input_data, e)\n</code></pre>"},{"location":"user-guide/command/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/command/#1-keep-commands-focused","title":"1. Keep Commands Focused","text":"<p>Each command should have a single, well-defined responsibility:</p> <pre><code># Good - Focused on LLM data preparation\nclass LLMCommand(Command):\n    def get_tool_input(self, ...):\n        # Only LLM-specific data preparation\n        pass\n\n# Avoid - Mixed responsibilities\nclass LLMAndDatabaseCommand(Command):  # Don't do this\n    def get_tool_input(self, ...):\n        # Both LLM and database logic\n        pass\n</code></pre>"},{"location":"user-guide/command/#2-use-meaningful-names","title":"2. Use Meaningful Names","text":"<p>Command names should clearly indicate their purpose:</p> <pre><code># Good\nclass ConversationHistoryCommand(Command): pass\nclass FunctionCallProcessingCommand(Command): pass\nclass RealTimeDataCommand(Command): pass\n\n# Avoid\nclass MyCommand(Command): pass\nclass SpecialCommand(Command): pass\n</code></pre>"},{"location":"user-guide/command/#3-handle-edge-cases","title":"3. Handle Edge Cases","text":"<p>Always consider edge cases in data preparation:</p> <pre><code>def get_tool_input(self, invoke_context: InvokeContext,\n                   input_data: List[ConsumeFromTopicEvent]) -&gt; Messages:\n    if not input_data:\n        return []  # Handle empty input\n\n    messages = []\n    for event in input_data:\n        if not event.data:\n            continue  # Skip empty events\n\n        # Validate message format\n        valid_messages = [msg for msg in event.data if self._is_valid_message(msg)]\n        messages.extend(valid_messages)\n\n    return messages if messages else [Message(role=\"system\", content=\"No valid input\")]\n</code></pre>"},{"location":"user-guide/command/#4-document-complex-logic","title":"4. Document Complex Logic","text":"<p>Use clear documentation for complex data transformations:</p> <pre><code>def get_tool_input(self, invoke_context: InvokeContext,\n                   input_data: List[ConsumeFromTopicEvent]) -&gt; Messages:\n    \"\"\"\n    Prepare LLM input with proper tool call ordering.\n\n    Process:\n    1. Retrieve conversation history excluding current request\n    2. Process current topic events in topological order\n    3. Ensure tool call messages immediately follow LLM tool calls\n    4. Sort all messages by timestamp\n\n    Args:\n        invoke_context: Current invocation context\n        input_data: Topic events from workflow\n\n    Returns:\n        Properly ordered messages for LLM consumption\n    \"\"\"\n    # Implementation...\n</code></pre>"},{"location":"user-guide/command/#integration-with-workflows","title":"Integration with Workflows","text":"<p>Commands integrate seamlessly with Graphite's event-driven workflows:</p> <pre><code># In a Node\n@record_node_invoke\nasync def invoke(self, invoke_context: InvokeContext,\n           node_input: List[ConsumeFromTopicEvent]) -&gt; AsyncGenerator[Messages, None]:\n    # Command automatically handles data transformation\n    async for response in self.command.invoke(invoke_context, node_input):\n        yield response\n\n# Command selection happens automatically\nnode = Node.builder().tool(my_llm_tool).build()\n# node.command is automatically set to LLMCommand(tool=my_llm_tool)\n</code></pre> <p>This architecture enables clean separation of concerns while maintaining the flexibility to customize data processing for specific use cases.</p>"},{"location":"user-guide/containers/","title":"Containers","text":"<p>The Graphite container system provides a thread-safe singleton-based dependency injection container for managing shared resources throughout the application. It handles the registration and lifecycle of core components like event stores and tracers.</p>"},{"location":"user-guide/containers/#overview","title":"Overview","text":"<p>The container system provides:</p> <ul> <li>Singleton Pattern: Thread-safe singleton implementation for global access</li> <li>Dependency Injection: Centralized registration and retrieval of dependencies</li> <li>Event Store Management: Registration and default setup of event storage</li> <li>Tracing Integration: Registration and configuration of OpenTelemetry tracing</li> <li>Lazy Initialization: On-demand creation of default implementations</li> <li>Production Safety: Warnings for development-only components</li> </ul>"},{"location":"user-guide/containers/#core-components","title":"Core Components","text":""},{"location":"user-guide/containers/#singletonmeta","title":"SingletonMeta","text":"<p>A thread-safe meta-class that implements the singleton pattern.</p>"},{"location":"user-guide/containers/#features","title":"Features","text":"<ul> <li>Thread Safety: Uses threading locks to prevent race conditions</li> <li>Instance Management: Maintains a dictionary of singleton instances per class</li> <li>Memory Efficiency: Ensures only one instance exists per class type</li> </ul> <pre><code>class SingletonMeta(type):\n    _instances: dict[type, object] = {}\n    _lock: threading.Lock = threading.Lock()\n\n    def __call__(cls: \"SingletonMeta\", *args: Any, **kwargs: Any) -&gt; Any:\n        # Ensure thread-safe singleton creation\n        with cls._lock:\n            if cls not in cls._instances:\n                cls._instances[cls] = super().__call__(*args, **kwargs)\n        return cls._instances[cls]\n</code></pre>"},{"location":"user-guide/containers/#container","title":"Container","text":"<p>The main dependency injection container using singleton pattern.</p>"},{"location":"user-guide/containers/#properties","title":"Properties","text":"Property Type Description <code>event_store</code> <code>EventStore</code> Returns registered event store or creates default <code>tracer</code> <code>Tracer</code> Returns registered tracer or creates default"},{"location":"user-guide/containers/#methods","title":"Methods","text":"Method Signature Description <code>register_event_store</code> <code>(event_store: EventStore) -&gt; None</code> Register custom event store implementation <code>register_tracer</code> <code>(tracer: Tracer) -&gt; None</code> Register custom tracer implementation"},{"location":"user-guide/containers/#global-instance","title":"Global Instance","text":"<pre><code>container: Container = Container()\n</code></pre> <p>A pre-instantiated global container instance available throughout the application.</p>"},{"location":"user-guide/containers/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/containers/#basic-container-usage","title":"Basic Container Usage","text":"<pre><code>from grafi.common.containers.container import container\n\n# Access the global container instance\nevent_store = container.event_store\ntracer = container.tracer\n\nprint(f\"Event store type: {type(event_store)}\")\nprint(f\"Tracer type: {type(tracer)}\")\n</code></pre>"},{"location":"user-guide/containers/#custom-event-store-registration","title":"Custom Event Store Registration","text":"<pre><code>from grafi.common.containers.container import container\nfrom grafi.common.event_stores.event_store_postgres import EventStorePostgres\n\n# Create custom event store\npostgres_store = EventStorePostgres(\n    connection_string=\"postgresql://user:pass@localhost:5432/events\"\n)\n\n# Register with container\ncontainer.register_event_store(postgres_store)\n\n# Now all access will use the custom store\nevent_store = container.event_store\nassert isinstance(event_store, EventStorePostgres)\n</code></pre>"},{"location":"user-guide/containers/#custom-tracer-registration","title":"Custom Tracer Registration","text":"<pre><code>from grafi.common.containers.container import container\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.jaeger.thrift import JaegerExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\n\n# Create custom tracer with Jaeger export\ntracer_provider = TracerProvider()\njaeger_exporter = JaegerExporter(\n    agent_host_name=\"localhost\",\n    agent_port=6831,\n)\nspan_processor = BatchSpanProcessor(jaeger_exporter)\ntracer_provider.add_span_processor(span_processor)\n\n# Set as global tracer provider\ntrace.set_tracer_provider(tracer_provider)\ncustom_tracer = trace.get_tracer(__name__)\n\n# Register with container\ncontainer.register_tracer(custom_tracer)\n\n# Now all access will use the custom tracer\ntracer = container.tracer\n</code></pre>"},{"location":"user-guide/containers/#event-store-integration","title":"Event Store Integration","text":""},{"location":"user-guide/containers/#default-behavior","title":"Default Behavior","text":"<pre><code># First access creates default in-memory store\nevent_store = container.event_store\n# Logs warning: \"Using EventStoreInMemory. This is ONLY suitable for local testing...\"\n</code></pre>"},{"location":"user-guide/containers/#production-setup","title":"Production Setup","text":"<pre><code>def setup_production_container():\n    \"\"\"Setup container for production environment.\"\"\"\n    from grafi.common.event_stores.event_store_postgres import EventStorePostgres\n    import os\n\n    # Get database connection from environment\n    db_url = os.getenv(\"DATABASE_URL\")\n    if not db_url:\n        raise ValueError(\"DATABASE_URL environment variable required\")\n\n    # Create and register production event store\n    prod_store = EventStorePostgres(connection_string=db_url)\n    container.register_event_store(prod_store)\n\n    print(\"Production event store registered\")\n\n# Call during application startup\nsetup_production_container()\n</code></pre>"},{"location":"user-guide/containers/#event-store-validation","title":"Event Store Validation","text":"<pre><code>def validate_event_store():\n    \"\"\"Validate that production event store is configured.\"\"\"\n    from grafi.common.event_stores.event_store_in_memory import EventStoreInMemory\n\n    event_store = container.event_store\n\n    if isinstance(event_store, EventStoreInMemory):\n        raise RuntimeError(\n            \"Production environment detected with in-memory event store. \"\n            \"Please configure a persistent event store.\"\n        )\n\n    print(f\"Using production event store: {type(event_store).__name__}\")\n</code></pre>"},{"location":"user-guide/containers/#tracing-integration","title":"Tracing Integration","text":""},{"location":"user-guide/containers/#default-tracing-setup","title":"Default Tracing Setup","text":"<pre><code># First access creates default tracer with auto-configuration\ntracer = container.tracer\n# Uses setup_tracing with default parameters:\n# - tracing_options=TracingOptions.AUTO\n# - collector_endpoint=\"localhost\"\n# - collector_port=4317\n# - project_name=\"grafi-trace\"\n</code></pre>"},{"location":"user-guide/containers/#custom-tracing-configuration","title":"Custom Tracing Configuration","text":"<pre><code>def setup_custom_tracing():\n    \"\"\"Setup custom tracing configuration.\"\"\"\n    from grafi.common.instrumentations.tracing import setup_tracing, TracingOptions\n    import os\n\n    # Get tracing configuration from environment\n    collector_endpoint = os.getenv(\"OTEL_EXPORTER_OTLP_ENDPOINT\", \"localhost\")\n    collector_port = int(os.getenv(\"OTEL_EXPORTER_OTLP_PORT\", \"4317\"))\n    service_name = os.getenv(\"SERVICE_NAME\", \"grafi-service\")\n\n    # Setup custom tracer\n    custom_tracer = setup_tracing(\n        tracing_options=TracingOptions.ENABLED,\n        collector_endpoint=collector_endpoint,\n        collector_port=collector_port,\n        project_name=service_name\n    )\n\n    # Register with container\n    container.register_tracer(custom_tracer)\n\n    print(f\"Custom tracing configured for {service_name}\")\n\nsetup_custom_tracing()\n</code></pre>"},{"location":"user-guide/containers/#distributed-tracing","title":"Distributed Tracing","text":"<pre><code>def create_span_example():\n    \"\"\"Example of using container tracer for distributed tracing.\"\"\"\n    tracer = container.tracer\n\n    with tracer.start_as_current_span(\"process_user_request\") as span:\n        span.set_attribute(\"user.id\", \"12345\")\n        span.set_attribute(\"request.type\", \"get_profile\")\n\n        # Simulate processing\n        process_request()\n\n        span.set_attribute(\"response.status\", \"success\")\n\ndef process_request():\n    \"\"\"Nested span example.\"\"\"\n    tracer = container.tracer\n\n    with tracer.start_as_current_span(\"database_query\") as span:\n        span.set_attribute(\"db.operation\", \"SELECT\")\n        span.set_attribute(\"db.table\", \"users\")\n\n        # Database operation simulation\n        result = query_database()\n\n        span.set_attribute(\"db.rows_affected\", len(result))\n</code></pre>"},{"location":"user-guide/containers/#application-lifecycle-integration","title":"Application Lifecycle Integration","text":""},{"location":"user-guide/containers/#startup-configuration","title":"Startup Configuration","text":"<pre><code>class Application:\n    def __init__(self):\n        self.container = container\n\n    def configure_dependencies(self):\n        \"\"\"Configure all application dependencies.\"\"\"\n        self._setup_event_store()\n        self._setup_tracing()\n        self._validate_configuration()\n\n    def _setup_event_store(self):\n        \"\"\"Setup event store based on environment.\"\"\"\n        import os\n\n        if os.getenv(\"ENVIRONMENT\") == \"production\":\n            from grafi.common.event_stores.event_store_postgres import EventStorePostgres\n\n            db_url = os.getenv(\"DATABASE_URL\")\n            if not db_url:\n                raise ValueError(\"DATABASE_URL required in production\")\n\n            event_store = EventStorePostgres(connection_string=db_url)\n            self.container.register_event_store(event_store)\n        else:\n            # Development - use default in-memory store\n            pass\n\n    def _setup_tracing(self):\n        \"\"\"Setup tracing based on environment.\"\"\"\n        import os\n        from grafi.common.instrumentations.tracing import setup_tracing, TracingOptions\n\n        if os.getenv(\"TRACING_ENABLED\", \"false\").lower() == \"true\":\n            tracer = setup_tracing(\n                tracing_options=TracingOptions.ENABLED,\n                collector_endpoint=os.getenv(\"OTEL_ENDPOINT\", \"localhost\"),\n                collector_port=int(os.getenv(\"OTEL_PORT\", \"4317\")),\n                project_name=os.getenv(\"SERVICE_NAME\", \"grafi-app\")\n            )\n            self.container.register_tracer(tracer)\n\n    def _validate_configuration(self):\n        \"\"\"Validate container configuration.\"\"\"\n        # Access properties to trigger initialization\n        event_store = self.container.event_store\n        tracer = self.container.tracer\n\n        print(f\"Event store: {type(event_store).__name__}\")\n        print(f\"Tracer: {type(tracer).__name__}\")\n\n    def start(self):\n        \"\"\"Start the application.\"\"\"\n        self.configure_dependencies()\n        print(\"Application started with configured dependencies\")\n\n# Usage\napp = Application()\napp.start()\n</code></pre>"},{"location":"user-guide/containers/#graceful-shutdown","title":"Graceful Shutdown","text":"<pre><code>class ApplicationManager:\n    def __init__(self):\n        self.container = container\n\n    async def shutdown(self):\n        \"\"\"Gracefully shutdown application resources.\"\"\"\n        print(\"Shutting down application...\")\n\n        # Close event store connections\n        event_store = self.container.event_store\n        if hasattr(event_store, 'close'):\n            await event_store.close()\n\n        # Flush tracer spans\n        tracer = self.container.tracer\n        if hasattr(tracer, 'force_flush'):\n            tracer.force_flush()\n\n        print(\"Application shutdown complete\")\n</code></pre>"},{"location":"user-guide/containers/#testing-with-containers","title":"Testing with Containers","text":""},{"location":"user-guide/containers/#test-container-setup","title":"Test Container Setup","text":"<pre><code>import pytest\nfrom grafi.common.containers.container import Container\nfrom grafi.common.event_stores.event_store_in_memory import EventStoreInMemory\n\n@pytest.fixture\ndef test_container():\n    \"\"\"Create a test container with in-memory components.\"\"\"\n    test_container = Container()\n\n    # Use in-memory event store for tests\n    test_store = EventStoreInMemory()\n    test_container.register_event_store(test_store)\n\n    # Use no-op tracer for tests\n    from opentelemetry.trace import NoOpTracer\n    test_tracer = NoOpTracer()\n    test_container.register_tracer(test_tracer)\n\n    yield test_container\n\n    # Cleanup if needed\n    if hasattr(test_store, 'clear'):\n        test_store.clear()\n\ndef test_event_store_integration(test_container):\n    \"\"\"Test event store integration.\"\"\"\n    event_store = test_container.event_store\n\n    # Verify it's the test store\n    assert isinstance(event_store, EventStoreInMemory)\n\n    # Test basic operations\n    from grafi.common.events.event import Event\n    test_event = Event(event_id=\"test-123\")\n\n    event_store.record_event(test_event)\n    retrieved = await event_store.get_event(\"test-123\")\n\n    assert retrieved.event_id == \"test-123\"\n</code></pre>"},{"location":"user-guide/containers/#mock-container","title":"Mock Container","text":"<pre><code>from unittest.mock import Mock, patch\n\ndef test_with_mocked_container():\n    \"\"\"Test using mocked container dependencies.\"\"\"\n    # Create mock event store\n    mock_event_store = Mock()\n    mock_event_store.record_event.return_value = None\n    mock_event_store.get_events.return_value = []\n\n    # Create mock tracer\n    mock_tracer = Mock()\n    mock_span = Mock()\n    mock_tracer.start_as_current_span.return_value.__enter__.return_value = mock_span\n\n    # Patch container properties\n    with patch('grafi.common.containers.container.container.event_store', mock_event_store), \\\n         patch('grafi.common.containers.container.container.tracer', mock_tracer):\n\n        # Test code using container\n        from grafi.common.containers.container import container\n\n        event_store = container.event_store\n        tracer = container.tracer\n\n        # Verify mocks are used\n        assert event_store is mock_event_store\n        assert tracer is mock_tracer\n</code></pre>"},{"location":"user-guide/containers/#thread-safety","title":"Thread Safety","text":""},{"location":"user-guide/containers/#concurrent-access","title":"Concurrent Access","text":"<pre><code>import threading\nimport time\nfrom grafi.common.containers.container import container\n\ndef worker_function(worker_id: int, results: dict):\n    \"\"\"Worker function to test thread safety.\"\"\"\n    # Access container from multiple threads\n    event_store = container.event_store\n    tracer = container.tracer\n\n    # Store results for verification\n    results[worker_id] = {\n        'event_store_id': id(event_store),\n        'tracer_id': id(tracer)\n    }\n\ndef test_thread_safety():\n    \"\"\"Test that container is thread-safe.\"\"\"\n    results = {}\n    threads = []\n\n    # Create multiple threads\n    for i in range(10):\n        thread = threading.Thread(\n            target=worker_function,\n            args=(i, results)\n        )\n        threads.append(thread)\n        thread.start()\n\n    # Wait for all threads to complete\n    for thread in threads:\n        thread.join()\n\n    # Verify all threads got the same instances\n    event_store_ids = {r['event_store_id'] for r in results.values()}\n    tracer_ids = {r['tracer_id'] for r in results.values()}\n\n    assert len(event_store_ids) == 1, \"Event store should be singleton\"\n    assert len(tracer_ids) == 1, \"Tracer should be singleton\"\n\n    print(\"Thread safety test passed\")\n\n# Run the test\ntest_thread_safety()\n</code></pre>"},{"location":"user-guide/containers/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/containers/#container-configuration","title":"Container Configuration","text":"<ol> <li>Early Registration: Register dependencies during application startup</li> <li>Environment-Based Setup: Use environment variables for configuration</li> <li>Validation: Validate container configuration before starting main logic</li> <li>Production Safety: Never use in-memory stores in production</li> </ol>"},{"location":"user-guide/containers/#dependency-management","title":"Dependency Management","text":"<ol> <li>Single Responsibility: Keep container focused on dependency injection</li> <li>Lazy Loading: Let container handle lazy initialization of defaults</li> <li>Type Safety: Use proper type hints for registered dependencies</li> <li>Error Handling: Handle missing dependencies gracefully</li> </ol>"},{"location":"user-guide/containers/#testing-strategies","title":"Testing Strategies","text":"<ol> <li>Test Containers: Use separate container instances for tests</li> <li>Mock Dependencies: Mock container dependencies for unit tests</li> <li>Integration Tests: Test with real dependencies in integration tests</li> <li>Cleanup: Always clean up test resources</li> </ol>"},{"location":"user-guide/containers/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Singleton Benefits: Leverage singleton pattern for shared resources</li> <li>Thread Safety: Container is thread-safe by design</li> <li>Memory Efficiency: Single instances reduce memory overhead</li> <li>Initialization Cost: Lazy initialization spreads startup cost</li> </ol>"},{"location":"user-guide/containers/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/containers/#common-issues","title":"Common Issues","text":"<pre><code>def handle_container_errors():\n    \"\"\"Examples of handling container-related errors.\"\"\"\n    try:\n        # This might fail if dependencies are not available\n        event_store = container.event_store\n\n    except Exception as e:\n        print(f\"Failed to get event store: {e}\")\n        # Fallback to in-memory store\n        from grafi.common.event_stores.event_store_in_memory import EventStoreInMemory\n        fallback_store = EventStoreInMemory()\n        container.register_event_store(fallback_store)\n\ndef validate_production_setup():\n    \"\"\"Validate that production dependencies are properly configured.\"\"\"\n    import os\n    from grafi.common.event_stores.event_store_in_memory import EventStoreInMemory\n\n    if os.getenv(\"ENVIRONMENT\") == \"production\":\n        event_store = container.event_store\n\n        if isinstance(event_store, EventStoreInMemory):\n            raise RuntimeError(\n                \"Production environment using in-memory event store. \"\n                \"Configure persistent storage.\"\n            )\n\n        # Additional validation\n        if not hasattr(event_store, 'connection_pool'):\n            raise RuntimeError(\"Event store missing connection pool\")\n</code></pre>"},{"location":"user-guide/containers/#migration-guide","title":"Migration Guide","text":""},{"location":"user-guide/containers/#from-direct-dependencies-to-container","title":"From Direct Dependencies to Container","text":"<pre><code># Before: Direct dependency instantiation\n# event_store = EventStorePostgres(connection_string)\n# tracer = setup_tracing(...)\n\n# After: Using container\nfrom grafi.common.containers.container import container\n\n# Setup once during application startup\ncontainer.register_event_store(event_store)\ncontainer.register_tracer(tracer)\n\n# Use throughout application\nevent_store = container.event_store\ntracer = container.tracer\n</code></pre>"},{"location":"user-guide/containers/#existing-code-integration","title":"Existing Code Integration","text":"<pre><code>class ExistingService:\n    def __init__(self):\n        # Old way - direct instantiation\n        # self.event_store = EventStoreInMemory()\n\n        # New way - use container\n        from grafi.common.containers.container import container\n        self.event_store = container.event_store\n        self.tracer = container.tracer\n\n    def process_data(self, data):\n        # Use tracer from container\n        with self.tracer.start_as_current_span(\"process_data\") as span:\n            span.set_attribute(\"data.size\", len(data))\n\n            # Process data\n            result = self._transform_data(data)\n\n            # Record event using container's event store\n            from grafi.common.events.event import Event\n            event = Event(event_id=f\"processed-{result.id}\")\n            self.event_store.record_event(event)\n\n            return result\n</code></pre> <p>The container system provides a robust foundation for dependency injection in Graphite applications, ensuring thread-safe access to shared resources while maintaining flexibility for different deployment environments and testing scenarios.</p>"},{"location":"user-guide/conventional-rules/","title":"Conventional Rules","text":"<p>While the platform is designed for maximum flexibility, certain conventions guide how components interact. These rules ensure consistency, maintainability, and ease of integration across a range of use cases\u2014especially when handling user requests, generating outputs, and enabling powerful LLM features.</p>"},{"location":"user-guide/conventional-rules/#reserved-topics","title":"Reserved Topics","text":""},{"location":"user-guide/conventional-rules/#agent-input-topic","title":"Agent Input Topic","text":"<ul> <li>Triggering the Workflow: All new assistant requests with a fresh <code>assistant_request_id</code> begin by publishing the user input to the agent_input_topic.</li> <li>Downstream Consumption: Nodes that need to process initial requests consume from this topic, triggering the rest of the workflow.</li> </ul>"},{"location":"user-guide/conventional-rules/#agent-output-topics","title":"Agent Output Topics","text":"<ul> <li>Final Responses: All output events route to agent_output_topic, which the Assistant consumes to return data to the user or caller.</li> <li>Single Consumer: Only the Assistant should subscribe to this topic, avoiding conflicting read operations.</li> </ul>"},{"location":"user-guide/conventional-rules/#inworkflow-topics","title":"InWorkflow Topics","text":"<ul> <li>Human in the Loop: Used when user intervention is required within workflows; the system uses paired InWorkflowOutputTopic and InWorkflowInputTopic for coordinated human interactions.</li> <li>InWorkflowOutputTopic: Posts <code>OutputTopicEvent</code> objects that require human interaction or review.</li> <li>InWorkflowInputTopic: Receives user responses via <code>publish_input_data()</code> based on upstream events from the paired output topic.</li> <li>Workflow Coordination: These topics work together to enable seamless human-in-the-loop workflows with proper event coordination.</li> </ul> <p>Rationale: Structuring input and output channels ensures clarity, preventing multiple consumers from inadvertently processing final outputs and providing a clear path for user-driven requests.</p>"},{"location":"user-guide/conventional-rules/#outputtopicevent","title":"OutputTopicEvent","text":"<ul> <li>Dedicated for Assistant: If a newly received event is an <code>OutputTopicEvent</code>, the workflow's <code>on_event()</code> skips subscription checks, since only the Assistant should consume it.</li> <li>Exclusive Destination: <code>OutputTopicEvent</code> can only be published to agent_output_topic or InWorkflowOutputTopic, ensuring a clear boundary for user-facing outputs.</li> </ul> <p>Rationale: Limiting <code>OutputTopicEvent</code> usage avoids confusion over who should read final results, reinforcing the principle of single responsibility for returning data to the user.</p>"},{"location":"user-guide/conventional-rules/#stream-usage","title":"Stream Usage","text":"<ul> <li>Output Only: Streaming is only relevant for final outputs, letting the LLM emit partial content in real time.</li> <li>Asynchronous Requirement: Nodes, workflows, and assistants do not support synchronous streaming. Though the LLM tool may have a synchronous stream function, the system\u2019s architecture uses async flows.</li> <li>Usage Pattern: For practical examples, see <code>stream_assistant</code>; it shows how to handle partial token streams differently from normal async generators.</li> </ul> <p>Rationale: Maintaining an async-only stream approach for nodes, workflows, and assistants simplifies concurrency, reduces potential race conditions, and provides a consistent development experience.</p>"},{"location":"user-guide/conventional-rules/#llmfunctioncall","title":"LLMFunctionCall","text":"<ul> <li>Agent-Like Interactions: By calling functions, the LLM can access additional tools\u2014making the system more agent-like.</li> <li>Separation of Concerns: The LLM node focuses on generating or interpreting responses, while a separate <code>LLMFunctionCall</code> node invokes tool logic.</li> <li>Upstream Connections: Each <code>LLMFunctionCall</code> must directly connect to one or more LLM node via topic(s) when want to enable upstream LLM node tool calling feature.</li> <li>Downstream Connections: Each LLM node can directly connect to one or more <code>LLMFunctionCall</code> nodes. If the workflow builder detects an <code>LLMFunctionCall</code> node downstream, it attaches the relevant function specs to the LLM node\u2019s final output message, letting the LLM know which tools are available.</li> </ul> <p>Rationale: Decoupling LLM operations from tool invocation keeps the graph modular, fosters reusability, and ensures that an LLM can dynamically discover and call specialized tools within a single workflow.</p>"},{"location":"user-guide/event-driven-workflow/","title":"Event-Driven Workflow","text":"<p>The event-driven workflow serves as the dynamic execution layer of Graphite, orchestrating node execution through a publish/subscribe (pub/sub) model. This architecture structures workflows as interconnected nodes that communicate via event queues (topics), enabling asynchronous, scalable, and flexible processing.</p>"},{"location":"user-guide/event-driven-workflow/#fields","title":"Fields","text":"<p>The fields of the EventDrivenWorkflow are:</p> Field Name Type Description <code>name</code> <code>str</code> Unique identifier for the workflow instance (default: <code>\"EventDrivenWorkflow\"</code>). <code>type</code> <code>str</code> The type identifier for the workflow, typically matching the class name (<code>\"EventDrivenWorkflow\"</code>). <code>oi_span_type</code> <code>OpenInferenceSpanKindValues</code> Semantic attribute from OpenInference used for tracing and observability (default: <code>AGENT</code>). <code>workflow_id</code> <code>str</code> Unique identifier for the workflow instance, auto-generated UUID. <code>nodes</code> <code>Dict[str, Node]</code> Collection of all nodes defined within this workflow, keyed by node name. <code>_topics</code> <code>Dict[str, TopicBase]</code> Private dictionary storing all event topics managed by the workflow. <code>_topic_nodes</code> <code>Dict[str, List[str]]</code> Private mapping of topic names to lists of node names subscribed to each topic. <code>_invoke_queue</code> <code>deque[Node]</code> Private queue of nodes that are ready to execute, triggered by event availability. <code>_tracker</code> <code>AsyncNodeTracker</code> Tracks active nodes and workflow idle state for proper async termination."},{"location":"user-guide/event-driven-workflow/#methods","title":"Methods","text":"<p>The following table summarizes key methods within the EventDrivenWorkflow class, highlighting their role in managing workflow execution, event handling, and node orchestration:</p> Method Name Description <code>model_post_init</code> Model initialization hook that sets up topics and handles function calling nodes after model creation. <code>builder</code> Class method that returns a WorkflowBuilder for fluent workflow construction. <code>_add_topics</code> Sets up topic subscriptions and node-to-topic mappings from node configurations. <code>_add_topic</code> Registers a topic within the workflow and sets default publish event handlers. <code>_handle_function_calling_nodes</code> Links function calling nodes with LLM nodes to enable function specification sharing. <code>_commit_events</code> Commits processed events to their respective topics, updating consumer offsets. <code>_get_output_events</code> Retrieves and returns consumed events from agent output and in-workflow output topics. <code>invoke</code> Asynchronous workflow execution with streaming support and concurrent node processing. <code>_invoke_node</code> Invokes a single node asynchronously with proper stream handling and error management. <code>init_workflow</code> Asynchronously initializes workflow state, either restoring from stored events or creating new workflow with input data. <code>on_event</code> Event handler that responds to topic publish events, evaluates node readiness, and queues nodes for execution. <code>to_dict</code> Serializes the workflow to a dictionary representation including nodes, topics, and topic-node mappings."},{"location":"user-guide/event-driven-workflow/#workflow-execution-flow","title":"Workflow Execution Flow","text":"<p>The following diagram illustrates the execution flow of a Node within an event-driven workflow:</p> <pre><code>graph TD\n    Start[Start Workflow Execution] --&gt; Init[initial_workflow: Restore or Initialize State]\n    Init --&gt; EventListener[Event Listener: on_event]\n    EventListener --&gt; CheckCriteria[Evaluate Node can_invoke Criteria]\n    CheckCriteria --&gt;|Conditions Met| QueueNode[Add Node to _invoke_queue]\n    QueueNode --&gt; InvokeNode[Execute Next Node in Queue]\n    InvokeNode --&gt; PublishResults[Publish Results via _publish_events]\n    PublishResults --&gt; EventListener\n    EventListener --&gt;|No Pending Nodes| Output[Assistant Output]\n</code></pre>"},{"location":"user-guide/event-driven-workflow/#workflow-initialization","title":"Workflow Initialization","text":"<p>When an EventDrivenWorkflow instance is initialized, the <code>model_post_init</code> method performs several critical setup tasks:</p> <ol> <li>Topic Setup (<code>_add_topics</code>): Constructs the workflow graph by analyzing each node's subscription expressions and publish destinations</li> <li>Function Calling Integration (<code>_handle_function_calling_nodes</code>): Links function-calling nodes with LLM nodes to enable tool recognition</li> </ol> <p>The topic setup process:</p> <ul> <li>Extracts topics from node subscription expressions using <code>extract_topics</code></li> <li>Maps topic names to subscribing node names in <code>_topic_nodes</code></li> <li>Registers all topics in the <code>_topics</code> dictionary with event handlers</li> <li>Validates that required <code>InputTopic</code> and <code>OutputTopic</code> exist</li> </ul> <p>For function calling capabilities:</p> <ul> <li>Identifies nodes using <code>FunctionCallTool</code></li> <li>Links them with LLM nodes that publish to the same topics</li> <li>Enables LLMs to recognize and invoke available function tools</li> </ul> <pre><code>graph TD\n    A[Initialize Workflow] --&gt; B[model_post_init]\n    B --&gt; C[_add_topics: Build Topic Graph]\n    B --&gt; D[_handle_function_calling_nodes]\n    C --&gt; E([Extract Topics from Nodes])\n    C --&gt; F([Map Topics to Nodes])\n    C --&gt; G([Register Event Handlers])\n    D --&gt; H([Link Function Tools to LLMs])\n    E --&gt; I([Validate Required Topics])\n    F --&gt; I\n    G --&gt; I\n    H --&gt; I\n    I --&gt; J[Workflow Ready]\n</code></pre>"},{"location":"user-guide/event-driven-workflow/#node-execution-process","title":"Node Execution Process","text":"<p>The EventDrivenWorkflow uses asynchronous execution with the <code>invoke</code> method, handling node orchestration through async/await patterns.</p>"},{"location":"user-guide/event-driven-workflow/#asynchronous-execution-invoke","title":"Asynchronous Execution (<code>invoke</code>)","text":"<p>The asynchronous execution model provides sophisticated event-driven processing with proper coordination:</p> <ol> <li>Workflow Initialization: Sets up initial state with <code>init_workflow</code></li> <li>Concurrent Node Processing: Spawns individual tasks for each node using <code>_invoke_node</code> </li> <li>Output Listening: Creates listeners for each output topic to capture results</li> <li>Event Streaming: Uses <code>MergeIdleQueue</code> to stream events as they become available</li> <li>Proper Termination: Coordinates workflow completion using <code>AsyncNodeTracker</code></li> </ol>"},{"location":"user-guide/event-driven-workflow/#key-components","title":"Key Components","text":"<ul> <li>AsyncNodeTracker: Manages active node state and idle detection</li> <li>Output Listeners: Monitor output topics for new events</li> <li>MergeIdleQueue: Coordinates between event availability and workflow idle state</li> <li>Offset Management: Commits events immediately to prevent duplicates</li> </ul>"},{"location":"user-guide/event-driven-workflow/#event-driven-node-triggering","title":"Event-Driven Node Triggering","text":"<p>The pub/sub model governs node execution through the <code>on_event</code> handler:</p> <ol> <li>Event Reception: Receives <code>PublishToTopicEvent</code> from topics</li> <li>Subscriber Identification: Finds all nodes subscribed to the published topic</li> <li>Readiness Check: Evaluates each node's <code>can_invoke()</code> criteria</li> <li>Queue Management: Adds ready nodes to <code>_invoke_queue</code> for execution</li> </ol> <pre><code>graph TD\n    A[Event Published] --&gt; B[on_event Handler]\n    B --&gt; C[Find Subscribed Nodes]\n    C --&gt; D{Node can_invoke?}\n    D --&gt;|Yes| E[Add to _invoke_queue]\n    D --&gt;|No| F[Wait for More Events]\n    E --&gt; G[Node Execution]\n    G --&gt; H[Publish Results]\n    H --&gt; A\n    F --&gt; A\n</code></pre>"},{"location":"user-guide/event-driven-workflow/#input-preparation-execution-logic","title":"Input Preparation &amp; Execution Logic","text":"<p>Before executing a node, the workflow's <code>get_node_input</code> method collects all relevant events from the node's subscribed topics:</p> <ol> <li>Topic Iteration: Processes each topic in <code>node._subscribed_topics</code></li> <li>Event Consumption: Uses <code>topic.consume(node.name)</code> to retrieve available events</li> <li>Event Conversion: Converts topic events to <code>ConsumeFromTopicEvent</code> instances</li> <li>Input Assembly: Returns list of consumed events for node execution</li> </ol>"},{"location":"user-guide/event-driven-workflow/#subscription-logic-support","title":"Subscription Logic Support","text":"<p>The system provides flexible subscription expressions through topic expressions that support complex logic:</p> <ul> <li>AND Logic: Node executes when ALL subscription conditions are met</li> <li>OR Logic: Node executes when ANY subscription condition is met</li> </ul>"},{"location":"user-guide/event-driven-workflow/#async-flow-architecture","title":"Async Flow Architecture","text":"<p>The EventDrivenWorkflow implements a sophisticated async architecture that addresses the challenges of coordinating multiple concurrent nodes while ensuring proper workflow termination and data consistency.</p>"},{"location":"user-guide/event-driven-workflow/#workflow-components","title":"Workflow Components","text":"<p>The async workflow relies on two key components for coordination and output management:</p>"},{"location":"user-guide/event-driven-workflow/#asyncnodetracker","title":"AsyncNodeTracker","text":"<p>The <code>AsyncNodeTracker</code> manages workflow state and coordination by tracking active nodes, monitoring processing cycles, and detecting idle states. It provides the foundation for proper workflow termination detection.</p>"},{"location":"user-guide/event-driven-workflow/#asyncoutputqueue","title":"AsyncOutputQueue","text":"<p>The <code>AsyncOutputQueue</code> manages the collection and streaming of output events from multiple topics. It coordinates concurrent listeners and provides a unified async iterator interface for consuming events.</p> <p>For detailed information about these components, see the Workflow Components documentation.</p>"},{"location":"user-guide/event-driven-workflow/#component-integration","title":"Component Integration","text":"<p>The async workflow integrates these components to provide robust event streaming:</p> <pre><code># Create output queue with topics and tracker\noutput_queue = AsyncOutputQueue(output_topics, self.name, self._tracker)\nawait output_queue.start_listeners()\n\n# Stream events as they arrive\nasync for event in output_queue:\n    yield event.data\n</code></pre>"},{"location":"user-guide/event-driven-workflow/#offset-management-and-duplicate-prevention","title":"Offset Management and Duplicate Prevention","text":"<p>The async workflow implements proper offset management to prevent duplicate data:</p> <pre><code>async for event in MergeIdleQueue(queue, self._tracker):\n    consumed_output_event = ConsumeFromTopicEvent(...)\n\n    # Commit BEFORE yielding to prevent duplicate data\n    await self._commit_events(\n        consumer_name=self.name, events=[consumed_output_event]\n    )\n\n    # Now yield the data after committing\n    yield event.data\n</code></pre>"},{"location":"user-guide/event-driven-workflow/#key-principles","title":"Key Principles","text":"<ul> <li>Immediate Consumption: Consumed offset advanced on fetch</li> <li>Commit Before Yield: Prevents duplicate data in output stream</li> <li>Atomic Operations: Event processing and commitment are coordinated</li> </ul>"},{"location":"user-guide/event-driven-workflow/#node-lifecycle-in-async-mode","title":"Node Lifecycle in Async Mode","text":"<p>Each node runs in its own async task with proper coordination:</p> <pre><code>async def _invoke_node(self, invoke_context: InvokeContext, node: Node):\n    buffer: Dict[str, List[TopicEvent]] = {}\n\n    try:\n        while not self._stop_requested:\n            # Wait for node to have sufficient data\n            await wait_node_invoke(node)\n\n            # Signal node is becoming active\n            await self._tracker.enter(node.name)\n\n            try:\n                # Process events and publish results\n                async for msgs in node.invoke(invoke_context, consumed_events):\n                    published_events = await publish_events(...)\n                    # Notify downstream nodes immediately\n                    for event in published_events:\n                        if event.name in self._topic_nodes:\n                            topic = self._topics[event.name]\n                            async with topic.event_cache._cond:\n                                topic.event_cache._cond.notify_all()\n\n                # Commit processed events\n                await self._commit_events(...)\n\n            finally:\n                # Signal node is no longer active\n                await self._tracker.leave(node.name)\n\n    except asyncio.CancelledError:\n        logger.info(f\"Node {node.name} was cancelled\")\n        raise\n</code></pre>"},{"location":"user-guide/event-driven-workflow/#node-coordination-features","title":"Node Coordination Features","text":"<ul> <li>Buffer Management: Each node maintains its own event buffer</li> <li>Activity Signaling: Nodes signal when they become active/inactive</li> <li>Immediate Notification: Downstream nodes notified immediately after publishing</li> <li>Proper Cleanup: Resources cleaned up on cancellation</li> </ul>"},{"location":"user-guide/event-driven-workflow/#workflow-termination-logic","title":"Workflow Termination Logic","text":"<p>The workflow terminates when all conditions are met:</p> <ol> <li>No Active Nodes: <code>AsyncNodeTracker</code> reports idle state</li> <li>No Pending Data: Output topics have no unconsumed data</li> <li>No Progress: Activity count indicates no new processing cycles</li> </ol> <pre><code># Check for workflow completion\nif tracker.is_idle() and not topic.can_consume(consumer_name):\n    current_activity = tracker.get_activity_count()\n\n    # If no new activity since last check, we're done\n    if current_activity == last_activity_count:\n        break\n\n    last_activity_count = current_activity\n</code></pre> <p>This multi-layered approach ensures that workflows terminate cleanly without missing data or creating race conditions between upstream completion and downstream activation. - Complex Expressions: Combination of AND/OR operators for advanced logic</p> <p>Important Consideration for OR Logic: When using OR-based subscriptions, nodes are queued as soon as one condition is satisfied. Messages from other subscribed topics in the OR expression may not be available when <code>get_node_input</code> executes, potentially causing data inconsistencies. Careful design is recommended when implementing OR-based logic.</p>"},{"location":"user-guide/event-driven-workflow/#node-execution-flow","title":"Node Execution Flow","text":"<pre><code>graph TD\n    A[Node Ready to Execute] --&gt; B[get_node_input]\n    B --&gt; C[Check Subscribed Topics]\n    C --&gt; D[Consume Available Events]\n    D --&gt; E[Convert to ConsumeFromTopicEvent]\n    E --&gt; F[Execute Node]\n    F --&gt; G[Publish Results via _publish_events]\n    G --&gt; H[Trigger on_event for Subscribers]\n</code></pre>"},{"location":"user-guide/event-driven-workflow/#restoring-unfinished-workflows","title":"Restoring Unfinished Workflows","text":"<p>A key advantage of this event-driven architecture is its ability to restore and resume unfinished workflows. When <code>initial_workflow()</code> is called, the system:</p> <ol> <li>Event Store Check: Retrieves stored events for the given <code>assistant_request_id</code></li> <li>State Restoration: If events exist, restores workflow topics and state from stored events</li> <li>Fresh Start: If no events exist, publishes input to <code>InputTopic</code> to begin new workflow</li> <li>Topic Restoration: Replays stored <code>PublishToTopicEvent</code> and <code>OutputTopicEvent</code> to restore topic states</li> <li>Node Queue Setup: Evaluates node readiness and populates <code>_invoke_queue</code> appropriately</li> </ol>"},{"location":"user-guide/event-driven-workflow/#benefits-of-workflow-restoration","title":"Benefits of Workflow Restoration","text":"<ul> <li>Human-in-the-Loop Support: Workflows can pause while awaiting user input and resume seamlessly</li> <li>Failure Recovery: Resume from failure points without restarting entire workflows</li> <li>Cost Efficiency: Prevents unnecessary re-execution of previous steps, reducing LLM call costs</li> <li>State Consistency: Maintains exact workflow state across interruptions</li> </ul> <pre><code>graph TD\n    A[initial_workflow Called] --&gt; B{Events Exist for assistant_request_id?}\n    B --&gt;|No| C[Publish Input to InputTopic]\n    B --&gt;|Yes| D[Reset All Topics]\n    D --&gt; E[Restore Events from Event Store]\n    E --&gt; F[Replay PublishToTopicEvent Events]\n    F --&gt; G[Evaluate Node Readiness]\n    G --&gt; H[Populate _invoke_queue]\n    C --&gt; I[Begin Fresh Workflow]\n    H --&gt; I\n</code></pre> <p>This restoration mechanism enables robust, fault-tolerant workflows that can handle interruptions gracefully while maintaining consistency and efficiency.</p>"},{"location":"user-guide/event-interfaces/","title":"Event Interfaces","text":""},{"location":"user-guide/event-interfaces/#overview","title":"Overview","text":"<p>Graphite uses a consistent event-driven interface pattern where components communicate exclusively through two primary event types: - PublishToTopicEvent: Used to publish data to topics - ConsumeFromTopicEvent: Used to consume data from topics</p> <p>This design creates a clean separation of concerns and enables loose coupling between components.</p>"},{"location":"user-guide/event-interfaces/#interface-patterns","title":"Interface Patterns","text":""},{"location":"user-guide/event-interfaces/#component-communication-flow","title":"Component Communication Flow","text":"<p>The event interface creates a bidirectional flow pattern:</p> <pre><code>graph LR\n    A[Assistant/Workflow] --&gt;|PublishToTopicEvent| B[Topics]\n    B --&gt;|ConsumeFromTopicEvent| C[Nodes]\n    C --&gt;|PublishToTopicEvent| D[Topics]\n    D --&gt;|ConsumeFromTopicEvent| A\n</code></pre>"},{"location":"user-guide/event-interfaces/#component-interfaces","title":"Component Interfaces","text":"Component Input Type Output Type Description Assistant <code>PublishToTopicEvent</code> <code>List[ConsumeFromTopicEvent]</code> Receives published events, returns consumed events Workflow <code>PublishToTopicEvent</code> <code>List[ConsumeFromTopicEvent]</code> Orchestrates nodes through event flow Node <code>List[ConsumeFromTopicEvent]</code> <code>PublishToTopicEvent</code> Consumes events, processes, publishes results Tool <code>Messages</code> <code>Messages</code> Transforms message data (used within nodes)"},{"location":"user-guide/event-interfaces/#event-structure","title":"Event Structure","text":""},{"location":"user-guide/event-interfaces/#publishtotopicevent","title":"PublishToTopicEvent","text":"<p>Published when a component sends data to a topic:</p> <pre><code>from grafi.common.events.topic_events.publish_to_topic_event import PublishToTopicEvent\nfrom grafi.common.models.invoke_context import InvokeContext\nfrom grafi.common.models.message import Message\n\nevent = PublishToTopicEvent(\n    publisher_name=\"ProcessorNode\",\n    publisher_type=\"node\",\n    invoke_context=InvokeContext(session_id=\"session-123\"),\n    consumed_event_ids=[\"evt_1\", \"evt_2\"],  # Events that led to this publication\n    data=[Message(role=\"assistant\", content=\"Processed result\")]\n)\n</code></pre>"},{"location":"user-guide/event-interfaces/#consumefromtopicevent","title":"ConsumeFromTopicEvent","text":"<p>Created when data is consumed from a topic:</p> <pre><code>from grafi.common.events.topic_events.consume_from_topic_event import ConsumeFromTopicEvent\n\nevent = ConsumeFromTopicEvent(\n    name=\"output_topic\",\n    type=TopicType.DEFAULT_TOPIC_TYPE,\n    offset=42,\n    publisher_name=\"ProcessorNode\",  # Original publisher\n    publisher_type=\"node\",\n    invoke_context=InvokeContext(session_id=\"session-123\"),\n    consumed_event_ids=[\"evt_1\", \"evt_2\"],\n    data=[Message(role=\"assistant\", content=\"Processed result\")]\n)\n</code></pre>"},{"location":"user-guide/event-interfaces/#implementation-examples","title":"Implementation Examples","text":""},{"location":"user-guide/event-interfaces/#assistant-implementation","title":"Assistant Implementation","text":"<pre><code>from grafi.assistants.assistant import Assistant\nfrom grafi.common.events.topic_events.publish_to_topic_event import PublishToTopicEvent\nfrom grafi.common.events.topic_events.consume_from_topic_event import ConsumeFromTopicEvent\nfrom typing import List, AsyncGenerator\n\nclass MyAssistant(Assistant):\n    async def invoke(\n        self,\n        input_data: PublishToTopicEvent\n    ) -&gt; AsyncGenerator[ConsumeFromTopicEvent, None]:\n        \"\"\"Asynchronous streaming of events.\"\"\"\n        async for output in self.workflow.invoke(input_data):\n            yield output\n</code></pre>"},{"location":"user-guide/event-interfaces/#node-implementation","title":"Node Implementation","text":"<pre><code>from grafi.nodes.node import Node\nfrom grafi.common.models.invoke_context import InvokeContext\nfrom grafi.common.events.topic_events.consume_from_topic_event import ConsumeFromTopicEvent\nfrom grafi.common.events.topic_events.publish_to_topic_event import PublishToTopicEvent\nfrom typing import List, AsyncGenerator\n\nclass ProcessorNode(Node):\n    async def invoke(\n        self,\n        invoke_context: InvokeContext,\n        node_input: List[ConsumeFromTopicEvent]\n    ) -&gt; AsyncGenerator[PublishToTopicEvent, None]:\n        \"\"\"Process consumed events and publish result.\"\"\"\n        # Execute command on input data\n        async for response in self.command.invoke(invoke_context, node_input):\n            # Wrap response in PublishToTopicEvent\n            yield PublishToTopicEvent(\n                publisher_name=self.name,\n                publisher_type=self.type,\n                invoke_context=invoke_context,\n                consumed_event_ids=[event.event_id for event in node_input],\n                data=response\n            )\n</code></pre>"},{"location":"user-guide/event-interfaces/#workflow-integration","title":"Workflow Integration","text":"<pre><code>from grafi.workflows.workflow import Workflow\nfrom grafi.common.events.topic_events.publish_to_topic_event import PublishToTopicEvent\nfrom grafi.common.events.topic_events.consume_from_topic_event import ConsumeFromTopicEvent\n\nclass MyWorkflow(Workflow):\n    async def invoke(self, input_data: PublishToTopicEvent) -&gt; AsyncGenerator[ConsumeFromTopicEvent, None]:\n        \"\"\"Execute workflow asynchronously.\"\"\"\n        # Initialize workflow with input event\n        await self.init_workflow(input_data)\n\n        # Process nodes until completion\n        while not self._invoke_queue.empty():\n            node = self._invoke_queue.get()\n            async for output in node.invoke(...):\n                # Publish output to topics\n                yield output\n</code></pre>"},{"location":"user-guide/event-interfaces/#benefits-of-event-interfaces","title":"Benefits of Event Interfaces","text":""},{"location":"user-guide/event-interfaces/#1-loose-coupling","title":"1. Loose Coupling","text":"<p>Components don't need direct references to each other - they communicate through events and topics.</p>"},{"location":"user-guide/event-interfaces/#2-traceability","title":"2. Traceability","text":"<p>Every event carries: - <code>invoke_context</code>: Request correlation information - <code>consumed_event_ids</code>: Chain of events that led to this event - Publisher information: Source component details</p>"},{"location":"user-guide/event-interfaces/#3-flexibility","title":"3. Flexibility","text":"<ul> <li>Easy to add new components without modifying existing ones</li> <li>Components can be tested in isolation</li> <li>Workflows can be composed dynamically</li> </ul>"},{"location":"user-guide/event-interfaces/#4-observability","title":"4. Observability","text":"<ul> <li>Complete audit trail through event chain</li> <li>Easy to trace data flow through the system</li> <li>Built-in support for distributed tracing</li> </ul>"},{"location":"user-guide/event-interfaces/#5-recovery","title":"5. Recovery","text":"<ul> <li>Events can be replayed from any point</li> <li>Workflows can resume from interruption</li> <li>State can be reconstructed from event history</li> </ul>"},{"location":"user-guide/event-interfaces/#migration-from-direct-invocation","title":"Migration from Direct Invocation","text":"<p>If migrating from older patterns that used direct method calls:</p> <p>Old Pattern: <pre><code>def invoke(self, invoke_context: InvokeContext, input_data: Messages) -&gt; Messages:\n    return self.workflow.invoke(invoke_context, input_data)\n</code></pre></p> <p>New Pattern: <pre><code>async def invoke(self, input_data: PublishToTopicEvent) -&gt; AsyncGenerator[ConsumeFromTopicEvent, None]:\n    async for output in self.workflow.invoke(input_data):\n        yield output\n</code></pre></p> <p>Key changes: 1. Input is now a single <code>PublishToTopicEvent</code> instead of separate context and data 2. Output is an async generator yielding <code>ConsumeFromTopicEvent</code> objects 3. Context and data are embedded within the event objects 4. Method is now async and uses <code>async for</code> for streaming responses 4. Event IDs enable tracing the full processing chain</p>"},{"location":"user-guide/event-interfaces/#best-practices","title":"Best Practices","text":"<ol> <li>Always preserve event chains: Include <code>consumed_event_ids</code> when creating new events</li> <li>Use descriptive names: Set meaningful <code>publisher_name</code> values for debugging</li> <li>Type your components: Use proper type hints for event interfaces</li> <li>Handle streaming properly: Use async generators for streaming responses</li> <li>Validate event data: Ensure data types match expected formats</li> </ol>"},{"location":"user-guide/event-interfaces/#see-also","title":"See Also","text":"<ul> <li>Events System - Detailed event documentation</li> <li>Topics - Topic-based messaging</li> <li>Event-Driven Workflow - Workflow orchestration</li> <li>Node - Node component documentation</li> </ul>"},{"location":"user-guide/invoke-decorators/","title":"Invoke Decorators","text":"<p>Graphite provides a comprehensive set of decorators for recording execution events, adding distributed tracing, and exposing functions to LLMs. These decorators automatically handle event logging, error tracking, and observability without requiring manual instrumentation.</p>"},{"location":"user-guide/invoke-decorators/#overview","title":"Overview","text":"<p>The decorators are categorized into two main types:</p> <ol> <li>Recording Decorators - Automatically record events, add tracing spans, and handle errors</li> <li>Function Decorators - Expose functions to LLMs with automatic schema generation</li> </ol>"},{"location":"user-guide/invoke-decorators/#llm-function-decorator","title":"LLM Function Decorator","text":""},{"location":"user-guide/invoke-decorators/#llm_function","title":"@llm_function","text":"<p>The <code>@llm_function</code> decorator exposes methods to Language Learning Models by automatically generating function specifications.</p> <p>Location: <code>grafi.common.decorators.llm_function</code></p> <p>Purpose:</p> <ul> <li>Extracts function metadata (name, docstring, parameters, type hints)</li> <li>Constructs a <code>FunctionSpec</code> object with JSON Schema-compatible parameter descriptions</li> <li>Stores the specification as a <code>_function_spec</code> attribute on the decorated function</li> </ul> <p>Usage:</p> <pre><code>from grafi.common.decorators.llm_function import llm_function\n\n@llm_function\ndef calculate_sum(x: int, y: int, precision: float = 0.1) -&gt; float:\n    \"\"\"\n    Calculate the sum of two numbers with optional precision.\n\n    Args:\n        x (int): The first number to add.\n        y (int): The second number to add.\n        precision (float, optional): Precision level. Defaults to 0.1.\n\n    Returns:\n        float: The sum of x and y.\n    \"\"\"\n    return float(x + y)\n</code></pre> <p>Features:</p> <ul> <li>Automatically maps Python types to JSON Schema types</li> <li>Extracts parameter descriptions from docstrings</li> <li>Marks parameters without defaults as required</li> <li>Supports type hints for comprehensive schema generation</li> </ul> <p>Type Mapping:</p> Python Type JSON Schema Type <code>str</code> <code>\"string\"</code> <code>int</code> <code>\"integer\"</code> <code>float</code> <code>\"number\"</code> <code>bool</code> <code>\"boolean\"</code> <code>list</code> <code>\"array\"</code> <code>dict</code> <code>\"object\"</code> Other <code>\"string\"</code> (default)"},{"location":"user-guide/invoke-decorators/#recording-decorators","title":"Recording Decorators","text":"<p>Recording decorators provide automatic event logging, distributed tracing, and error handling for different component types. They come in synchronous and asynchronous variants.</p>"},{"location":"user-guide/invoke-decorators/#assistant-decorators","title":"Assistant Decorators","text":""},{"location":"user-guide/invoke-decorators/#record_assistant_invoke","title":"@record_assistant_invoke","text":"<p>Location: <code>grafi.common.decorators.record_assistant_invoke</code></p> <p>Records synchronous assistant invocations with event logging and tracing.</p> <p>Features:</p> <ul> <li>Records <code>AssistantInvokeEvent</code> before execution</li> <li>Creates distributed tracing spans with OpenInference attributes</li> <li>Records <code>AssistantRespondEvent</code> on success or <code>AssistantFailedEvent</code> on error</li> <li>Captures input/output data as JSON</li> </ul> <p>Usage:</p> <pre><code>from grafi.common.decorators.record_assistant_invoke import record_assistant_invoke\nfrom grafi.common.events.topic_events.publish_to_topic_event import PublishToTopicEvent\nfrom grafi.common.events.topic_events.consume_from_topic_event import ConsumeFromTopicEvent\nfrom typing import AsyncGenerator\n\n@record_assistant_invoke\nasync def invoke(self, input_data: PublishToTopicEvent) -&gt; AsyncGenerator[ConsumeFromTopicEvent, None]:\n    # Assistant implementation\n    async for output in self.workflow.invoke(input_data):\n        yield output\n</code></pre>"},{"location":"user-guide/invoke-decorators/#node-decorators","title":"Node Decorators","text":""},{"location":"user-guide/invoke-decorators/#record_node_invoke","title":"@record_node_invoke","text":"<p>Location: <code>grafi.common.decorators.record_node_invoke</code></p> <p>Records synchronous node invocations in event-driven workflows.</p> <p>Features:</p> <ul> <li>Records <code>NodeInvokeEvent</code> with subscription and publication topic information</li> <li>Tracks subscribed topics and publish destinations</li> <li>Captures <code>ConsumeFromTopicEvent</code> input data</li> <li>Records success/failure events with comprehensive context</li> </ul> <p>Usage:</p> <pre><code>from grafi.common.decorators.record_node_invoke import record_node_invoke\nfrom grafi.common.models.invoke_context import InvokeContext\nfrom grafi.common.events.topic_events.consume_from_topic_event import ConsumeFromTopicEvent\nfrom grafi.common.events.topic_events.publish_to_topic_event import PublishToTopicEvent\nfrom typing import List, AsyncGenerator\n\n@record_node_invoke\nasync def invoke(self, invoke_context: InvokeContext,\n           node_input: List[ConsumeFromTopicEvent]) -&gt; AsyncGenerator[PublishToTopicEvent, None]:\n    # Node processing logic - execute command and yield PublishToTopicEvents\n    async for response in self.command.invoke(invoke_context, node_input):\n        yield PublishToTopicEvent(\n            publisher_name=self.name,\n            publisher_type=self.type,\n            invoke_context=invoke_context,\n            consumed_event_ids=[event.event_id for event in node_input],\n            data=response\n        )\n</code></pre>"},{"location":"user-guide/invoke-decorators/#tool-decorators","title":"Tool Decorators","text":""},{"location":"user-guide/invoke-decorators/#record_tool_invoke","title":"@record_tool_invoke","text":"<p>Location: <code>grafi.common.decorators.record_decorators</code></p> <p>Records synchronous tool invocations with event logging and tracing.</p> <p>Features:</p> <ul> <li>Records <code>ToolInvokeEvent</code> before execution</li> <li>Creates tool-specific tracing spans</li> <li>Records <code>ToolRespondEvent</code> on success or <code>ToolFailedEvent</code> on error</li> <li>Captures tool execution context and results</li> </ul> <p>Usage:</p> <pre><code>from grafi.common.decorators.record_decorators import record_tool_invoke\n\n@record_tool_invoke\ndef invoke(self, invoke_context: InvokeContext, input_data: Messages) -&gt; Messages:\n    # Tool execution logic\n    return tool_results\n</code></pre>"},{"location":"user-guide/invoke-decorators/#workflow-decorators","title":"Workflow Decorators","text":""},{"location":"user-guide/invoke-decorators/#record_workflow_invoke","title":"@record_workflow_invoke","text":"<p>Location: <code>grafi.common.decorators.record_workflow_invoke</code></p> <p>Records synchronous workflow invocations with comprehensive event logging.</p> <p>Features:</p> <ul> <li>Records <code>WorkflowInvokeEvent</code> before execution</li> <li>Creates workflow-level tracing spans</li> <li>Records <code>WorkflowRespondEvent</code> on success or <code>WorkflowFailedEvent</code> on error</li> <li>Tracks workflow execution context and results</li> </ul> <p>Usage:</p> <pre><code>from grafi.common.decorators.record_workflow_invoke import record_workflow_invoke\nfrom grafi.common.events.topic_events.publish_to_topic_event import PublishToTopicEvent\nfrom grafi.common.events.topic_events.consume_from_topic_event import ConsumeFromTopicEvent\nfrom typing import List\n\n@record_workflow_invoke\ndef invoke(self, input_data: PublishToTopicEvent) -&gt; List[ConsumeFromTopicEvent]:\n    # Workflow orchestration logic\n    # Initialize workflow, execute nodes, return consumed events\n    return output_events\n</code></pre>"},{"location":"user-guide/invoke-decorators/#common-features","title":"Common Features","text":""},{"location":"user-guide/invoke-decorators/#event-recording","title":"Event Recording","text":"<p>All recording decorators automatically:</p> <ul> <li>Record invocation events before execution begins</li> <li>Record success events with output data upon completion</li> <li>Record failure events with error details when exceptions occur</li> <li>Capture timing and execution context information</li> </ul>"},{"location":"user-guide/invoke-decorators/#distributed-tracing","title":"Distributed Tracing","text":"<p>The decorators integrate with OpenInference tracing standards:</p> <ul> <li>Create spans with appropriate names (<code>{component_name}.invoke</code> or <code>{component_name}.run</code>)</li> <li>Set OpenInference span attributes for component identification</li> <li>Capture input/output data as span attributes</li> <li>Record error information in span attributes on failure</li> </ul>"},{"location":"user-guide/invoke-decorators/#streaming-support","title":"Streaming Support","text":"<p>Async decorators handle streaming responses:</p> <ul> <li>Preserve async generator behavior for streaming outputs</li> <li>Aggregate streaming content for final result tracking</li> <li>Distinguish between streaming and non-streaming messages</li> <li>Maintain observability without affecting streaming performance</li> </ul>"},{"location":"user-guide/invoke-decorators/#error-handling","title":"Error Handling","text":"<p>All decorators provide comprehensive error handling:</p> <ul> <li>Capture and record exception details in events</li> <li>Add error information to tracing spans</li> <li>Re-raise exceptions to preserve original error behavior</li> <li>Maintain error context for debugging and monitoring</li> </ul>"},{"location":"user-guide/invoke-decorators/#span-attributes","title":"Span Attributes","text":"<p>The decorators set standardized span attributes:</p> Attribute Type Examples Component ID <code>assistant_id</code>, <code>node_id</code>, <code>tool_id</code>, <code>workflow_id</code> Component Metadata <code>name</code>, <code>type</code>, <code>model</code> (for assistants) Execution Context All fields from <code>InvokeContext</code> Data <code>input</code>, <code>output</code> (serialized as JSON) Topics (Nodes) <code>subscribed_topics</code>, <code>publish_to_topics</code> OpenInference <code>openinference.span.kind</code> Errors <code>error</code> (error message when exceptions occur)"},{"location":"user-guide/invoke-decorators/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Choose the Right Decorator: Use synchronous decorators for blocking operations and asynchronous decorators for streaming or async operations.</p> </li> <li> <p>Consistent Application: Apply decorators consistently across similar component types for uniform observability.</p> </li> <li> <p>Function Specification: Use <code>@llm_function</code> on methods that should be available to LLMs, ensuring comprehensive docstrings and type hints.</p> </li> <li> <p>Performance Considerations: Recording decorators add minimal overhead but consider the impact of JSON serialization for large data structures.</p> </li> <li> <p>Error Context: Leverage the automatic error recording for debugging and monitoring failed invocations.</p> </li> <li> <p>Streaming Behavior: For async decorators, ensure your async generators yield complete message batches for proper aggregation.</p> </li> </ol>"},{"location":"user-guide/models/","title":"Models","text":"<p>In Graphite, various models provide the fundamental data structures that underpin the event-driven workflow. Message represents the content exchanged between users, assistants, and language models, enabling consistent communication and processing. Event captures the various actions and state changes in the system, from workflow initiation to final outputs. Topics define the named channels where events are published and consumed, establishing a structured mechanism for coordinating data flow across the platform. Additional models include Command for encapsulating tool execution logic and InvokeContext for maintaining workflow state across operations.</p>"},{"location":"user-guide/models/#message","title":"Message","text":"<p><code>Message</code> extends OpenAI's chat completion structure, serving as a standardized data structure for both incoming and outgoing content in the event-driven workflow. Each <code>Message</code> instance retains essential metadata such as timestamps, unique identifiers, and optional tool references, facilitating robust and traceable communication between users, assistants, and LLM tools.</p>"},{"location":"user-guide/models/#fields","title":"Fields","text":"Field Description <code>name</code> An optional name indicating the source or identifier for the message (e.g., function name). <code>message_id</code> A unique identifier for the message, defaulting to a generated UUID. <code>timestamp</code> The time in nanoseconds when the message was created, allowing strict chronological ordering. <code>content</code> The message content - can be string, dictionary, list of dictionaries, or None. <code>refusal</code> The refusal message generated by the model, if applicable. <code>annotations</code> Annotations for the message, such as when using web search tools. <code>audio</code> Audio response data from the model when audio output modality is requested. <code>role</code> Specifies the speaker's role (<code>system</code>, <code>user</code>, <code>assistant</code>, or <code>tool</code>). <code>tool_call_id</code> Associates the message with a particular tool invocation if relevant. <code>tools</code> An optional list of OpenAI's <code>ChatCompletionToolParam</code> for referencing available tool calls. <code>function_call</code> Deprecated field replaced by <code>tool_calls</code>. <code>tool_calls</code> The tool calls generated by the model, such as function calls. <code>is_streaming</code> Boolean flag indicating if this message is part of a streaming response."},{"location":"user-guide/models/#type-aliases","title":"Type Aliases","text":"<ul> <li><code>Messages = List[Message]</code> - A list of Message objects</li> <li><code>MsgsAGen = AsyncGenerator[Messages, None]</code> - An async generator yielding Messages</li> </ul>"},{"location":"user-guide/models/#usage-example","title":"Usage Example","text":"<pre><code>from grafi.common.models.message import Message\n\n# Creating a user message\nuser_message = Message(\n    role=\"user\",\n    content=\"What is the capital of France?\"\n)\n\n# Creating an assistant message\nassistant_message = Message(\n    role=\"assistant\",\n    content=\"The capital of France is Paris.\"\n)\n</code></pre>"},{"location":"user-guide/models/#invokecontext","title":"InvokeContext","text":"<p><code>InvokeContext</code> maintains the workflow state and tracking information across different operations in the system. It provides essential context for conversation management and request tracing.</p>"},{"location":"user-guide/models/#invokecontext-fields","title":"InvokeContext Fields","text":"Field Description <code>conversation_id</code> Unique identifier for a conversation between user and assistant. <code>invoke_id</code> Unique identifier for each conversation invoke - an invoke can involve multiple agents. <code>assistant_request_id</code> Created when an agent receives a request from the user. <code>user_id</code> Optional user identifier, defaults to empty string. <code>kwargs</code> Optional additional keyword arguments and context for the workflow"},{"location":"user-guide/models/#invokecontext-usage-example","title":"InvokeContext Usage Example","text":"<pre><code>from grafi.common.models.invoke_context import InvokeContext\n\ncontext = InvokeContext(\n    conversation_id=\"conv_123\",\n    invoke_id=\"invoke_456\",\n    assistant_request_id=\"req_789\",\n    user_id=\"user_001\"\n)\n</code></pre>"},{"location":"user-guide/models/#command","title":"Command","text":"<p><code>Command</code> encapsulates tool execution logic using the Command Pattern. It provides a standardized interface for invoking tools with proper input handling and context management.</p>"},{"location":"user-guide/models/#command-fields","title":"Command Fields","text":"Field Description <code>tool</code> The Tool instance this command will execute."},{"location":"user-guide/models/#command-methods","title":"Command Methods","text":"Method Description <code>for_tool(tool)</code> Class method factory to create appropriate command for a tool type. <code>invoke(context, data)</code> Asynchronously invokes the tool, yielding response messages. <code>get_tool_input(context, data)</code> Processes input events and extracts messages for tool consumption. <code>to_dict()</code> Serializes the command to a dictionary representation."},{"location":"user-guide/models/#command-usage-example","title":"Command Usage Example","text":"<pre><code>from grafi.common.models.command import Command\nfrom grafi.tools.tool import Tool\n\n# Create command from tool\ncommand = Command.for_tool(my_tool)\n\n# Invoke the command\nasync for response in command.invoke(invoke_context, input_datas):\n    # Process response\n</code></pre>"},{"location":"user-guide/models/#functionspec","title":"FunctionSpec","text":"<p><code>FunctionSpec</code> defines the structure for function specifications that can be converted to OpenAI tool parameters.</p>"},{"location":"user-guide/models/#functionspec-fields","title":"FunctionSpec Fields","text":"Field Description <code>name</code> The name of the function. <code>description</code> Description of what the function does. <code>parameters</code> Schema defining the function's parameters."},{"location":"user-guide/models/#functionspec-methods","title":"FunctionSpec Methods","text":"Method Description <code>to_openai_tool()</code> Converts the function spec to OpenAI's <code>ChatCompletionToolParam</code>."},{"location":"user-guide/models/#supporting-classes","title":"Supporting Classes","text":"<ul> <li>ParameterSchema: Defines individual parameter types and descriptions</li> <li>ParametersSchema: Defines the overall parameters structure with properties and required fields</li> </ul>"},{"location":"user-guide/models/#basebuilder","title":"BaseBuilder","text":"<p><code>BaseBuilder</code> provides a generic builder pattern implementation for constructing Python models with a fluent interface.</p>"},{"location":"user-guide/models/#basebuilder-methods","title":"BaseBuilder Methods","text":"Method Description <code>build()</code> Returns the fully configured model instance. <p>This builder is extended by specific builders like <code>NodeBaseBuilder</code> to provide domain-specific construction methods.</p>"},{"location":"user-guide/models/#mcp-connections","title":"MCP Connections","text":"<p>The <code>mcp_connections.py</code> module defines comprehensive models for Model Context Protocol (MCP) connections, supporting multiple transport mechanisms for connecting to MCP servers. These TypedDict-based models ensure type safety and provide clear interfaces for different connection types.</p>"},{"location":"user-guide/models/#connection-types","title":"Connection Types","text":"<p>The system supports four distinct connection transport types through a union type <code>Connection</code>:</p>"},{"location":"user-guide/models/#1-standard-io-connection","title":"1. Standard I/O Connection","text":"<p>Transport: <code>stdio</code> - Connects via standard input/output streams</p> <p>Primary Use: Local process communication</p> Field Type Description <code>command</code> <code>str</code> The executable to run to start the server <code>args</code> <code>list[str]</code> Command line arguments to pass to the executable <code>env</code> <code>dict[str, str] \\| None</code> Environment variables for the spawned process <code>cwd</code> <code>str \\| Path \\| None</code> Working directory for the spawned process <code>encoding</code> <code>str</code> Text encoding for message communication <code>encoding_error_handler</code> <code>EncodingErrorHandler</code> Error handling strategy for encoding issues <code>session_kwargs</code> <code>dict[str, Any] \\| None</code> Additional keyword arguments for the ClientSession"},{"location":"user-guide/models/#2-sseconnection","title":"2. SSEConnection","text":"<p>Transport: <code>sse</code> - Server-Sent Events for real-time communication</p> <p>Primary Use: HTTP-based streaming connections</p> Field Type Description <code>url</code> <code>str</code> URL of the SSE endpoint to connect to <code>headers</code> <code>dict[str, Any] \\| None</code> HTTP headers to send to the SSE endpoint <code>timeout</code> <code>float</code> HTTP timeout duration <code>sse_read_timeout</code> <code>float</code> SSE-specific read timeout <code>session_kwargs</code> <code>dict[str, Any] \\| None</code> Additional keyword arguments for the ClientSession <code>httpx_client_factory</code> <code>Optional[Callable]</code> Custom factory for creating HTTP async client instances"},{"location":"user-guide/models/#3-streamable-http-connection","title":"3. Streamable HTTP Connection","text":"<p>Transport: <code>streamable_http</code> - HTTP with streaming capabilities</p> <p>Primary Use: Long-lived HTTP connections with streaming support</p> Field Type Description <code>url</code> <code>str</code> URL of the endpoint to connect to <code>headers</code> <code>dict[str, Any] \\| None</code> HTTP headers to send to the endpoint <code>timeout</code> <code>timedelta</code> HTTP timeout duration <code>sse_read_timeout</code> <code>timedelta</code> Wait time for new events before disconnecting <code>terminate_on_close</code> <code>bool</code> Whether to terminate the session on close <code>session_kwargs</code> <code>dict[str, Any] \\| None</code> Additional keyword arguments for the ClientSession <code>httpx_client_factory</code> <code>Optional[Callable]</code> Custom factory for creating HTTP async client instances"},{"location":"user-guide/models/#4-websocket-connection","title":"4. WebSocket Connection","text":"<p>Transport: <code>websocket</code> - WebSocket protocol for bidirectional communication</p> <p>Primary Use: Real-time bidirectional communication</p> Field Type Description <code>url</code> <code>str</code> URL of the WebSocket endpoint to connect to <code>session_kwargs</code> <code>dict[str, Any] \\| None</code> Additional keyword arguments for the ClientSession"},{"location":"user-guide/models/#supporting-types","title":"Supporting Types","text":""},{"location":"user-guide/models/#encodingerrorhandler","title":"EncodingErrorHandler","text":"<pre><code>EncodingErrorHandler = Literal[\"strict\", \"ignore\", \"replace\"]\n</code></pre> <p>Defines how encoding errors should be handled when communicating with standard I/O-based connections:</p> <ul> <li><code>strict</code>: Raise an exception on encoding errors (default Python behavior)</li> <li><code>ignore</code>: Ignore encoding errors and skip problematic characters  </li> <li><code>replace</code>: Replace problematic characters with placeholder characters</li> </ul>"},{"location":"user-guide/models/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/models/#standard-io-connection-example","title":"Standard I/O Connection Example","text":"<pre><code>stdio_config: StdioConnection = {\n    \"transport\": \"stdio\",\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"my_mcp_server\"],\n    \"env\": {\"DEBUG\": \"1\"},\n    \"cwd\": \"/path/to/server\",\n    \"encoding\": \"utf-8\",\n    \"encoding_error_handler\": \"replace\",\n    \"session_kwargs\": None\n}\n</code></pre>"},{"location":"user-guide/models/#sse-connection-example","title":"SSE Connection Example","text":"<pre><code>sse_config: SSEConnection = {\n    \"transport\": \"sse\",\n    \"url\": \"https://api.example.com/mcp/events\",\n    \"headers\": {\"Authorization\": \"Bearer token123\"},\n    \"timeout\": 30.0,\n    \"sse_read_timeout\": 60.0,\n    \"session_kwargs\": {\"trust_env\": True},\n    \"httpx_client_factory\": None\n}\n</code></pre>"},{"location":"user-guide/models/#websocket-connection-example","title":"WebSocket Connection Example","text":"<pre><code>ws_config: WebsocketConnection = {\n    \"transport\": \"websocket\",\n    \"url\": \"wss://api.example.com/mcp/ws\",\n    \"session_kwargs\": {\"ping_interval\": 20}\n}\n</code></pre> <p>These connection models provide a flexible and type-safe foundation for integrating with various MCP server implementations, whether they're local processes, HTTP-based services, or real-time communication endpoints.</p> <p>These models collectively provide a robust foundation for the event-driven architecture, ensuring consistent data handling, proper type safety, and extensible design patterns throughout the Graphite platform.</p>"},{"location":"user-guide/models/#additional-models","title":"Additional Models","text":""},{"location":"user-guide/models/#default-id-generation","title":"Default ID Generation","text":"<ul> <li><code>default_id</code>: A field generator that creates UUIDs automatically using <code>uuid.uuid4().hex</code></li> <li><code>EventId</code>: Type alias for string-based event identifiers</li> </ul>"},{"location":"user-guide/models/#eventid-type","title":"EventId Type","text":"<p>EventId is a string type.</p>"},{"location":"user-guide/node/","title":"Node","text":"<p>A Node is a discrete component in a graph-based agent system that operates under an event-driven model. Its primary role is to represent its position within a workflow graph, manage event subscriptions, and designate topics for publishing. In addition, it delegates invoke to a Command object, adhering to the Command Pattern. Each Node comprises the following elements:</p> <ul> <li>Unique Identity</li> <li>Distinguished by a unique node_id, name, and type.</li> <li>The name must be unique within a given workflow.</li> <li>Subscribed Topics</li> <li>Stores the event topics to which the node subscribes, typically originating from upstream publishers.</li> <li>Subscriptions can reference explicit topic names or apply custom subscription strategies.</li> <li>Publish-To Topics</li> <li>Stores the event topics designated for downstream nodes to subscribe to, facilitating event routing.</li> <li>Command for Invoke</li> <li>Encapsulates invoke logic through a Command object.</li> <li>Allows integration of new or specialized commands without modifying the node's existing structure.</li> </ul>"},{"location":"user-guide/node/#nodebase","title":"NodeBase","text":"<p>The <code>NodeBase</code> class serves as the abstract foundation for all nodes in the graph-based agent system. It defines the core structure and interface that all node implementations must follow, including:</p> <ul> <li>Core Properties: Essential attributes like <code>node_id</code>, <code>name</code>, <code>type</code>, and <code>tool</code></li> <li>Event Management: Subscription expressions and publish-to topics for event-driven communication</li> <li>Command Integration: Internal command management through the Command Pattern</li> <li>Builder Pattern: Provides a fluent interface for node construction via <code>NodeBaseBuilder</code></li> </ul> <p>The base class defines an abstract method <code>invoke</code> that must be implemented by concrete subclasses to define their specific behavior.</p>"},{"location":"user-guide/node/#node-implementation","title":"Node Implementation","text":"<p>The <code>Node</code> class is a concrete implementation of <code>NodeBase</code> that provides a working node with standard invoke behavior. Unlike the abstract base class, <code>Node</code> actually implements the <code>invoke</code> method by delegating to an internal <code>Command</code> object.</p> <p>Key features of the Node class include:</p> <ul> <li>Automatic Command Setup: Creates commands automatically from tools during initialization</li> <li>Event-Driven Invocation: Uses <code>can_invoke()</code> to determine readiness based on subscribed topics</li> <li>Event-Based Interface: Consumes <code>ConsumeFromTopicEvent</code> objects and publishes <code>PublishToTopicEvent</code> objects</li> <li>Decorator Support: Includes built-in instrumentation via <code>@record_node_invoke</code></li> </ul> <p>The following table describes each field within the Node class, highlighting its purpose and usage in the workflow:</p> Field Description <code>node_id</code> A unique identifier for the node instance. <code>name</code> A unique name identifying the node within the workflow. <code>type</code> Defines the category or type of node, indicating its function. <code>tool</code> Optional tool that the node uses; automatically creates a command when set. <code>command</code> Property providing access to the internal command object for invoke logic. <code>oi_span_type</code> Semantic attribute from OpenInference for tracing purposes. <code>subscribed_expressions</code> List of DSL-based subscription expressions used by the node. <code>publish_to</code> List of designated topics the node publishes events to. <code>_subscribed_topics</code> Internal mapping of subscribed topic names to Topic instances. <code>_command</code> Private field storing the command object. <p>The following table summarizes the methods available in the Node class, highlighting their purpose and intended usage:</p> Method Description <code>builder</code> Class method that returns a <code>NodeBaseBuilder</code> for fluent node construction. <code>model_post_init</code> Model post-initialization hook that sets up subscribed topics and auto-creates commands from tools during initialization. <code>invoke</code> Asynchronously invokes the node's command, yielding <code>PublishToTopicEvent</code> objects as they become available. <code>can_invoke</code> Evaluates subscription conditions to determine whether the node is ready to invoke based on available topics. <code>can_invoke_with_topics</code> Checks if the node can invoke given a specific list of topic names. <code>to_dict</code> Serializes node attributes to a dictionary, suitable for persistence or transmission."},{"location":"user-guide/node/#nodebasebuilder","title":"NodeBaseBuilder","text":"<p>The <code>NodeBaseBuilder</code> class provides a fluent interface for constructing nodes with a builder pattern. It allows for readable and maintainable node configuration through method chaining.</p> <p>Available builder methods:</p> Method Description <code>name</code> Sets the unique name for the node within the workflow. <code>type</code> Sets the node type, indicating its function or category. <code>tool</code> Sets the tool for the node; automatically creates a command from the tool. <code>oi_span_type</code> Sets the OpenInference span type for tracing purposes. <code>subscribe</code> Adds subscription expressions to topics or SubExpr objects. <code>publish_to</code> Adds topics that this node will publish events to. <p>Example usage:</p> <pre><code>from grafi.nodes.node import Node\nfrom grafi.topics.input_topic import InputTopic\nfrom grafi.topics.output_topic import OutputTopic\n\nnode = Node.builder()\n    .name(\"ProcessorNode\")\n    .type(\"DataProcessor\")\n    .tool(my_tool)\n    .subscribe(input_topic)\n    .publish_to(output_topic)\n    .build()\n\n# Node invoke signature\nfrom grafi.common.models.invoke_context import InvokeContext\nfrom grafi.common.events.topic_events.consume_from_topic_event import ConsumeFromTopicEvent\nfrom grafi.common.events.topic_events.publish_to_topic_event import PublishToTopicEvent\n\n# Asynchronous invocation\ncontext = InvokeContext(session_id=\"session-123\")\ninput_datas = [ConsumeFromTopicEvent(...)]  # List of consumed events\n\nasync for output_event in node.invoke(context, input_datas):\n    # Process each PublishToTopicEvent as it's generated\n    pass\n</code></pre>"},{"location":"user-guide/workflow-components/","title":"Workflow Components","text":"<p>This document details the internal components that power the event-driven workflow's asynchronous execution model. These components work together to provide efficient concurrent processing, proper termination detection, and streaming output capabilities.</p>"},{"location":"user-guide/workflow-components/#asyncnodetracker","title":"AsyncNodeTracker","text":"<p>The <code>AsyncNodeTracker</code> is a critical component that manages the state of active nodes in an asynchronous workflow. It provides coordination between concurrent node executions and enables proper workflow termination detection.</p>"},{"location":"user-guide/workflow-components/#purpose","title":"Purpose","text":"<p>The AsyncNodeTracker serves several key purposes:</p> <ol> <li>Activity Tracking: Monitors which nodes are currently processing events</li> <li>Idle Detection: Determines when the entire workflow has no active nodes</li> <li>Progress Monitoring: Tracks processing cycles to detect workflow progress</li> <li>Synchronization: Provides coordination primitives for concurrent operations</li> </ol>"},{"location":"user-guide/workflow-components/#architecture","title":"Architecture","text":"<pre><code>class AsyncNodeTracker:\n    def __init__(self) -&gt; None:\n        self._active: set[str] = set()  # Currently active nodes\n        self._processing_count: Dict[str, int] = defaultdict(int)  # Processing cycles per node\n        self._cond = asyncio.Condition()  # Coordination primitive\n        self._idle_event = asyncio.Event()  # Signals when workflow is idle\n</code></pre>"},{"location":"user-guide/workflow-components/#key-methods","title":"Key Methods","text":"Method Description <code>enter(node_name: str)</code> Marks a node as active when it begins processing <code>leave(node_name: str)</code> Marks a node as inactive when it completes processing <code>is_idle()</code> Returns True if no nodes are currently active <code>wait_idle_event()</code> Async wait until the workflow becomes idle <code>get_activity_count()</code> Returns total processing cycles across all nodes <code>reset()</code> Resets the tracker to initial state"},{"location":"user-guide/workflow-components/#usage-example","title":"Usage Example","text":"<pre><code>tracker = AsyncNodeTracker()\n\n# Node entering active state\nawait tracker.enter(\"node1\")\nassert not tracker.is_idle()\n\n# Node completing processing\nawait tracker.leave(\"node1\")\nassert tracker.is_idle()\n\n# Wait for workflow to become idle\nawait tracker.wait_idle_event()\n</code></pre>"},{"location":"user-guide/workflow-components/#workflow-integration","title":"Workflow Integration","text":"<p>The AsyncNodeTracker integrates with the workflow execution in several ways:</p> <ol> <li>Node Lifecycle: Each node signals when it enters/leaves active state</li> <li>Termination Detection: Output listeners use idle state to determine completion</li> <li>Progress Monitoring: Activity count helps detect stalled workflows</li> </ol> <pre><code>graph TD\n    A[Node Receives Events] --&gt; B[tracker.enter(node_name)]\n    B --&gt; C[Node Processing]\n    C --&gt; D[tracker.leave(node_name)]\n    D --&gt; E{All Nodes Idle?}\n    E --&gt;|Yes| F[Signal Workflow Complete]\n    E --&gt;|No| G[Continue Processing]\n</code></pre>"},{"location":"user-guide/workflow-components/#asyncoutputqueue","title":"AsyncOutputQueue","text":"<p>The <code>AsyncOutputQueue</code> manages the collection and streaming of output events from multiple topics in an asynchronous workflow. It coordinates between output topic listeners and provides a unified async iterator interface for consuming events.</p>"},{"location":"user-guide/workflow-components/#purpose_1","title":"Purpose","text":"<p>The AsyncOutputQueue addresses several challenges:</p> <ol> <li>Multi-Topic Monitoring: Efficiently monitors multiple output topics concurrently</li> <li>Stream Coordination: Provides a single stream from multiple sources</li> <li>Termination Detection: Integrates with AsyncNodeTracker for proper completion</li> <li>Type Safety: Provides proper type hints for async iteration</li> </ol>"},{"location":"user-guide/workflow-components/#architecture_1","title":"Architecture","text":"<pre><code>class AsyncOutputQueue:\n    def __init__(\n        self,\n        output_topics: List[TopicBase],\n        consumer_name: str,\n        tracker: AsyncNodeTracker,\n    ):\n        self.output_topics = output_topics\n        self.consumer_name = consumer_name\n        self.tracker = tracker\n        self.queue: asyncio.Queue[TopicEvent] = asyncio.Queue()\n        self.listener_tasks: List[asyncio.Task] = []\n</code></pre>"},{"location":"user-guide/workflow-components/#key-features","title":"Key Features","text":"Feature Description Async Iteration Implements <code>__aiter__</code> and <code>__anext__</code> for async for loop support Concurrent Listeners Spawns separate listener tasks for each output topic Idle Detection Monitors workflow idle state to determine when to stop iteration Graceful Shutdown Provides methods to cleanly stop all listener tasks"},{"location":"user-guide/workflow-components/#usage-example_1","title":"Usage Example","text":"<pre><code># Create output queue with topics and tracker\noutput_queue = AsyncOutputQueue(output_topics, \"consumer_name\", tracker)\n\n# Start listening to all topics\nawait output_queue.start_listeners()\n\ntry:\n    # Consume events as they arrive\n    async for event in output_queue:\n        # Process event\n        yield event.data\nfinally:\n    # Clean shutdown\n    await output_queue.stop_listeners()\n</code></pre>"},{"location":"user-guide/workflow-components/#output-listener-logic","title":"Output Listener Logic","text":"<p>Each output topic gets its own listener that implements sophisticated termination logic:</p> <pre><code>async def _output_listener(self, topic: TopicBase):\n    last_activity_count = 0\n\n    while True:\n        # Wait for either new data or idle state\n        topic_task = asyncio.create_task(topic.consume(self.consumer_name))\n        idle_event_waiter = asyncio.create_task(self.tracker.wait_idle_event())\n\n        done, pending = await asyncio.wait(\n            {topic_task, idle_event_waiter},\n            return_when=asyncio.FIRST_COMPLETED,\n        )\n\n        # Process new events\n        if topic_task in done:\n            output_events = topic_task.result()\n            for event in output_events:\n                await self.queue.put(event)\n\n        # Check for completion\n        if idle_event_waiter in done and self.tracker.is_idle():\n            current_activity = self.tracker.get_activity_count()\n\n            # No new activity and no data = done\n            if current_activity == last_activity_count and not topic.can_consume(self.consumer_name):\n                break\n\n            last_activity_count = current_activity\n</code></pre>"},{"location":"user-guide/workflow-components/#iteration-and-termination","title":"Iteration and Termination","text":"<p>The AsyncOutputQueue implements async iteration with proper idle detection:</p> <pre><code>async def __anext__(self) -&gt; TopicEvent:\n    while True:\n        queue_task = asyncio.create_task(self.queue.get())\n        idle_task = asyncio.create_task(self.tracker._idle_event.wait())\n\n        done, pending = await asyncio.wait(\n            {queue_task, idle_task},\n            return_when=asyncio.FIRST_COMPLETED,\n        )\n\n        # Data available - return it\n        if queue_task in done:\n            idle_task.cancel()\n            return queue_task.result()\n\n        # Workflow idle - check if we should stop\n        queue_task.cancel()\n        await asyncio.sleep(0)  # Allow downstream activation\n\n        if self.tracker.is_idle() and self.queue.empty():\n            raise StopAsyncIteration\n</code></pre>"},{"location":"user-guide/workflow-components/#integration-with-eventdrivenworkflow","title":"Integration with EventDrivenWorkflow","text":"<p>These components work together in the EventDrivenWorkflow's async execution:</p> <pre><code>async def invoke(self, input_data: PublishToTopicEvent) -&gt; AsyncGenerator[ConsumeFromTopicEvent, None]:\n    # Initialize workflow\n    await self.init_workflow(invoke_context, input)\n\n    # Start node processing tasks\n    node_processing_tasks = [\n        asyncio.create_task(self._invoke_node(invoke_context, node))\n        for node in self.nodes.values()\n    ]\n\n    # Create output queue for streaming results\n    output_topics = [t for t in self._topics.values()\n                     if t.type in (AGENT_OUTPUT_TOPIC_TYPE, IN_WORKFLOW_OUTPUT_TOPIC_TYPE)]\n\n    output_queue = AsyncOutputQueue(output_topics, self.name, self._tracker)\n    await output_queue.start_listeners()\n\n    try:\n        # Stream results as they arrive\n        async for event in output_queue:\n            yield event.data\n    finally:\n        await output_queue.stop_listeners()\n</code></pre>"},{"location":"user-guide/workflow-components/#coordination-flow","title":"Coordination Flow","text":"<pre><code>graph TD\n    A[Workflow Start] --&gt; B[Initialize AsyncNodeTracker]\n    B --&gt; C[Start Node Tasks]\n    C --&gt; D[Create AsyncOutputQueue]\n    D --&gt; E[Start Output Listeners]\n    E --&gt; F{Events Available?}\n    F --&gt;|Yes| G[Yield Event Data]\n    G --&gt; F\n    F --&gt;|No &amp; Idle| H[Check Termination]\n    H --&gt;|Activity Changed| F\n    H --&gt;|No Activity| I[Stop Iteration]\n    I --&gt; J[Cleanup Tasks]\n</code></pre>"},{"location":"user-guide/workflow-components/#benefits","title":"Benefits","text":"<p>The combination of AsyncNodeTracker and AsyncOutputQueue provides:</p> <ol> <li>Efficient Concurrency: Multiple nodes and topics processed in parallel</li> <li>Proper Termination: Detects when workflow is truly complete</li> <li>Stream Processing: Events streamed as available, not batched</li> <li>Resource Management: Clean startup and shutdown of async tasks</li> <li>Type Safety: Full type hints for better IDE support and error detection</li> </ol>"},{"location":"user-guide/workflow-components/#best-practices","title":"Best Practices","text":"<p>When working with these components:</p> <ol> <li>Always Clean Up: Use try/finally blocks to ensure listener tasks are stopped</li> <li>Monitor Activity: Use activity counts to detect stalled workflows</li> <li>Handle Cancellation: Nodes should handle CancelledError gracefully</li> <li>Coordinate Properly: Ensure nodes signal enter/leave at appropriate times</li> <li>Test Thoroughly: These components handle complex async coordination</li> </ol> <p>These workflow components form the foundation of Graphite's robust asynchronous execution model, enabling scalable and reliable event-driven processing.</p>"},{"location":"user-guide/events/event_graph/","title":"Event Graph","text":"<p>The Graphite event graph system provides advanced event relationship modeling and topological analysis capabilities for topic-based events. It enables visualization of event dependencies, causal relationships, and proper ordering of events for replay and analysis scenarios.</p>"},{"location":"user-guide/events/event_graph/#overview","title":"Overview","text":"<p>The event graph system is designed to:</p> <ul> <li>Model Relationships: Capture dependencies between topic events</li> <li>Topological Ordering: Provide chronologically correct event sequences</li> <li>Causal Analysis: Understand event causality and dependencies</li> <li>Event Replay: Support proper event ordering for state reconstruction</li> </ul>"},{"location":"user-guide/events/event_graph/#core-components","title":"Core Components","text":""},{"location":"user-guide/events/event_graph/#eventgraphnode","title":"EventGraphNode","text":"<p>Represents a single event within the graph structure.</p>"},{"location":"user-guide/events/event_graph/#fields","title":"Fields","text":"Field Type Description <code>event_id</code> <code>EventId</code> Unique identifier for the event <code>event</code> <code>TopicEvent</code> The actual topic event data <code>upstream_events</code> <code>List[EventId]</code> Events that this event depends on <code>downstream_events</code> <code>List[EventId]</code> Events that depend on this event"},{"location":"user-guide/events/event_graph/#methods","title":"Methods","text":"Method Signature Description <code>to_dict</code> <code>() -&gt; dict[str, Any]</code> Serialize node to dictionary <code>from_dict</code> <code>classmethod (data: dict) -&gt; EventGraphNode</code> Deserialize node from dictionary"},{"location":"user-guide/events/event_graph/#eventgraph","title":"EventGraph","text":"<p>The main graph structure that manages event relationships and provides analysis capabilities.</p>"},{"location":"user-guide/events/event_graph/#eventgraph-fields","title":"EventGraph Fields","text":"Field Type Description <code>nodes</code> <code>Dict[EventId, EventGraphNode]</code> All nodes in the graph <code>root_nodes</code> <code>List[EventGraphNode]</code> Entry point nodes (no dependencies)"},{"location":"user-guide/events/event_graph/#core-methods","title":"Core Methods","text":"Method Signature Description <code>build_graph</code> <code>(consume_events, topic_events) -&gt; None</code> Build graph from events <code>get_root_event_nodes</code> <code>() -&gt; List[EventGraphNode]</code> Get all root nodes <code>get_topology_sorted_events</code> <code>() -&gt; List[EventGraphNode]</code> Get topologically sorted events"},{"location":"user-guide/events/event_graph/#graph-construction-algorithm","title":"Graph Construction Algorithm","text":"<p>The event graph is built by analyzing the relationships between consume and publish events.</p>"},{"location":"user-guide/events/event_graph/#algorithm-overview","title":"Algorithm Overview","text":"<pre><code>graph TD\n    A[Start with Consume Events] --&gt; B[Clear Existing Graph]\n    B --&gt; C[Create Topic-Offset Mapping]\n    C --&gt; D[Process Each Consume Event]\n    D --&gt; E[Find Corresponding Publish Event]\n    E --&gt; F[Process Consumed Event IDs]\n    F --&gt; G[Build Upstream Relations]\n    G --&gt; H[Add Downstream Relations]\n    H --&gt; I[Complete Graph]\n</code></pre>"},{"location":"user-guide/events/event_graph/#implementation-details","title":"Implementation Details","text":""},{"location":"user-guide/events/event_graph/#step-1-topic-offset-mapping","title":"Step 1: Topic-Offset Mapping","text":"<pre><code># Create mapping for efficient lookup\ntopic_offset_to_publish = {\n    f\"{event.name}::{event.offset}\": event\n    for event in topic_events.values()\n    if isinstance(event, PublishToTopicEvent)\n}\n</code></pre>"},{"location":"user-guide/events/event_graph/#step-2-recursive-relationship-building","title":"Step 2: Recursive Relationship Building","text":"<pre><code>def build_node_relations(consume_event: ConsumeFromTopicEvent) -&gt; None:\n    if consume_event.event_id in visited:\n        return\n\n    visited.add(consume_event.event_id)\n    current_node = self._add_event(consume_event)\n\n    # Find corresponding publish event\n    publish_key = f\"{consume_event.name}::{consume_event.offset}\"\n    publish_event = topic_offset_to_publish.get(publish_key)\n\n    if publish_event:\n        # Process consumed events of the publish event\n        for consumed_id in publish_event.consumed_event_ids:\n            consumed_event = topic_events.get(consumed_id)\n            if isinstance(consumed_event, ConsumeFromTopicEvent):\n                child_node = self._add_event(consumed_event)\n                current_node.upstream_events.append(child_node.event_id)\n                build_node_relations(consumed_event)\n</code></pre>"},{"location":"user-guide/events/event_graph/#example-graph-structure","title":"Example Graph Structure","text":"<pre><code>graph TB\n    subgraph \"Event Graph Example\"\n        A[Consume Event A&lt;br/&gt;Topic: input&lt;br/&gt;Offset: 1] --&gt; B[Publish Event B&lt;br/&gt;Topic: processing&lt;br/&gt;Offset: 1]\n        B --&gt; C[Consume Event C&lt;br/&gt;Topic: processing&lt;br/&gt;Offset: 1]\n        C --&gt; D[Publish Event D&lt;br/&gt;Topic: output&lt;br/&gt;Offset: 1]\n\n        E[Consume Event E&lt;br/&gt;Topic: input&lt;br/&gt;Offset: 2] --&gt; F[Publish Event F&lt;br/&gt;Topic: processing&lt;br/&gt;Offset: 2]\n        F --&gt; G[Consume Event G&lt;br/&gt;Topic: processing&lt;br/&gt;Offset: 2]\n        G --&gt; D\n    end\n\n    style A fill:#e1f5fe\n    style E fill:#e1f5fe\n    style D fill:#e8f5e8\n</code></pre>"},{"location":"user-guide/events/event_graph/#topological-sorting-algorithm","title":"Topological Sorting Algorithm","text":"<p>The system provides topologically sorted events using a modified Kahn's algorithm with timestamp-based ordering.</p>"},{"location":"user-guide/events/event_graph/#algorithm-visualization","title":"Algorithm Visualization","text":"<pre><code>graph TD\n    A[Compute In-Degrees] --&gt; B[Initialize Min-Heap with Zero In-Degree Nodes]\n    B --&gt; C[Pop Node with Earliest Timestamp]\n    C --&gt; D[Add to Result]\n    D --&gt; E[Decrement In-Degrees of Upstream Nodes]\n    E --&gt; F{Any Zero In-Degree Nodes?}\n    F --&gt;|Yes| G[Add to Heap]\n    G --&gt; C\n    F --&gt;|No| H{Heap Empty?}\n    H --&gt;|No| C\n    H --&gt;|Yes| I[Reverse Result]\n    I --&gt; J[Return Topologically Sorted Events]\n</code></pre>"},{"location":"user-guide/events/event_graph/#topological-sorting-implementation","title":"Topological Sorting Implementation","text":""},{"location":"user-guide/events/event_graph/#step-1-in-degree-calculation","title":"Step 1: In-Degree Calculation","text":"<pre><code># Compute in-degrees for all nodes\nin_degree: Dict[EventId, int] = {}\nfor node in self.nodes.values():\n    in_degree[node.event_id] = 0\n\nfor node in self.nodes.values():\n    for up_id in node.upstream_events:\n        in_degree[up_id] += 1\n</code></pre>"},{"location":"user-guide/events/event_graph/#step-2-timestamp-based-priority-queue","title":"Step 2: Timestamp-Based Priority Queue","text":"<pre><code># Initialize min-heap with timestamp priority\nmin_heap: List[tuple] = []\nfor node in self.nodes.values():\n    if in_degree[node.event_id] == 0:\n        heapq.heappush(\n            min_heap,\n            (-node.event.timestamp.timestamp(), node.event_id)\n        )\n</code></pre>"},{"location":"user-guide/events/event_graph/#step-3-topological-processing","title":"Step 3: Topological Processing","text":"<pre><code>while min_heap:\n    ts, ev_id = heapq.heappop(min_heap)\n    current_node = self.nodes[ev_id]\n    result.append(current_node)\n\n    # Process upstream dependencies\n    for up_id in current_node.upstream_events:\n        in_degree[up_id] -= 1\n        if in_degree[up_id] == 0:\n            upstream_node = self.nodes[up_id]\n            heapq.heappush(\n                min_heap,\n                (-upstream_node.event.timestamp.timestamp(), upstream_node.event_id)\n            )\n</code></pre>"},{"location":"user-guide/events/event_graph/#topological-order-example","title":"Topological Order Example","text":"<pre><code>gantt\n    title Event Processing Timeline\n    dateFormat X\n    axisFormat %s\n\n    section Processing Order\n    Event A (t=100) :done, a, 0, 1\n    Event E (t=150) :done, e, 1, 2\n    Event B (t=200) :done, b, 2, 3\n    Event F (t=250) :done, f, 3, 4\n    Event C (t=300) :done, c, 4, 5\n    Event G (t=350) :done, g, 5, 6\n    Event D (t=400) :done, d, 6, 7\n</code></pre>"},{"location":"user-guide/events/event_graph/#usage-patterns","title":"Usage Patterns","text":""},{"location":"user-guide/events/event_graph/#basic-graph-construction","title":"Basic Graph Construction","text":"<pre><code>from grafi.common.events.event_graph import EventGraph\nfrom grafi.common.events.topic_events.consume_from_topic_event import ConsumeFromTopicEvent\nfrom grafi.common.events.topic_events.publish_to_topic_event import PublishToTopicEvent\n\n# Create event graph\ngraph = EventGraph()\n\n# Collect events\nconsume_events = [...]  # List of ConsumeFromTopicEvent\ntopic_events = {...}    # Dict mapping event IDs to events\n\n# Build the graph\ngraph.build_graph(consume_events, topic_events)\n\n# Get topologically sorted events\nsorted_events = graph.get_topology_sorted_events()\n</code></pre>"},{"location":"user-guide/events/event_graph/#event-replay-scenario","title":"Event Replay Scenario","text":"<pre><code>def replay_events_in_order(graph: EventGraph):\n    \"\"\"Replay events in proper causal order.\"\"\"\n    sorted_events = graph.get_topology_sorted_events()\n\n    for node in sorted_events:\n        event = node.event\n        print(f\"Replaying event {event.event_id} at {event.timestamp}\")\n        # Process event...\n</code></pre>"},{"location":"user-guide/events/event_graph/#dependency-analysis","title":"Dependency Analysis","text":"<pre><code>def analyze_event_dependencies(graph: EventGraph, event_id: str):\n    \"\"\"Analyze dependencies for a specific event.\"\"\"\n    if event_id not in graph.nodes:\n        return None\n\n    node = graph.nodes[event_id]\n\n    return {\n        \"event_id\": event_id,\n        \"upstream_count\": len(node.upstream_events),\n        \"downstream_count\": len(node.downstream_events),\n        \"upstream_events\": node.upstream_events,\n        \"downstream_events\": node.downstream_events\n    }\n</code></pre>"},{"location":"user-guide/events/event_graph/#root-event-analysis","title":"Root Event Analysis","text":"<pre><code>def find_root_causes(graph: EventGraph):\n    \"\"\"Find all events that started processing chains.\"\"\"\n    root_nodes = graph.get_root_event_nodes()\n\n    return [\n        {\n            \"event_id\": node.event_id,\n            \"timestamp\": node.event.timestamp,\n            \"name\": node.event.name,\n            \"downstream_count\": len(node.downstream_events)\n        }\n        for node in root_nodes\n    ]\n</code></pre>"},{"location":"user-guide/events/event_graph/#advanced-use-cases","title":"Advanced Use Cases","text":""},{"location":"user-guide/events/event_graph/#event-causality-tracing","title":"Event Causality Tracing","text":"<pre><code>graph LR\n    subgraph \"Causality Chain\"\n        A[User Request] --&gt; B[Input Processing]\n        B --&gt; C[Data Transformation]\n        C --&gt; D[ML Processing]\n        D --&gt; E[Output Generation]\n        E --&gt; F[Response Delivery]\n    end\n\n    subgraph \"Event Graph Representation\"\n        A1[ConsumeEvent A] --&gt; B1[PublishEvent B]\n        B1 --&gt; C1[ConsumeEvent C]\n        C1 --&gt; D1[PublishEvent D]\n        D1 --&gt; E1[ConsumeEvent E]\n        E1 --&gt; F1[PublishEvent F]\n    end\n</code></pre>"},{"location":"user-guide/events/event_graph/#parallel-processing-analysis","title":"Parallel Processing Analysis","text":"<pre><code>def identify_parallel_opportunities(graph: EventGraph):\n    \"\"\"Identify events that can be processed in parallel.\"\"\"\n    sorted_events = graph.get_topology_sorted_events()\n    parallel_groups = []\n\n    current_level = []\n    processed_dependencies = set()\n\n    for node in sorted_events:\n        # Check if all dependencies are satisfied\n        dependencies_satisfied = all(\n            dep_id in processed_dependencies\n            for dep_id in node.upstream_events\n        )\n\n        if dependencies_satisfied:\n            current_level.append(node.event_id)\n        else:\n            if current_level:\n                parallel_groups.append(current_level)\n                current_level = [node.event_id]\n\n        processed_dependencies.add(node.event_id)\n\n    if current_level:\n        parallel_groups.append(current_level)\n\n    return parallel_groups\n</code></pre>"},{"location":"user-guide/events/event_graph/#event-graph-validation","title":"Event Graph Validation","text":"<pre><code>def validate_event_graph(graph: EventGraph):\n    \"\"\"Validate the integrity of the event graph.\"\"\"\n    issues = []\n\n    # Check for cycles\n    sorted_events = graph.get_topology_sorted_events()\n    if len(sorted_events) != len(graph.nodes):\n        issues.append(\"Graph contains cycles\")\n\n    # Check for orphaned nodes\n    referenced_nodes = set()\n    for node in graph.nodes.values():\n        referenced_nodes.update(node.upstream_events)\n        referenced_nodes.update(node.downstream_events)\n\n    orphaned = set(graph.nodes.keys()) - referenced_nodes - {node.event_id for node in graph.root_nodes}\n    if orphaned:\n        issues.append(f\"Orphaned nodes found: {orphaned}\")\n\n    # Check for consistency\n    for node in graph.nodes.values():\n        for upstream_id in node.upstream_events:\n            if upstream_id not in graph.nodes:\n                issues.append(f\"Node {node.event_id} references non-existent upstream {upstream_id}\")\n\n    return issues\n</code></pre>"},{"location":"user-guide/events/event_graph/#serialization-and-persistence","title":"Serialization and Persistence","text":""},{"location":"user-guide/events/event_graph/#graph-serialization","title":"Graph Serialization","text":"<pre><code># Serialize graph to dictionary\ngraph_dict = graph.to_dict()\n\n# Save to JSON\nimport json\nwith open(\"event_graph.json\", \"w\") as f:\n    json.dump(graph_dict, f, indent=2)\n\n# Load from JSON\nwith open(\"event_graph.json\", \"r\") as f:\n    graph_data = json.load(f)\n\n# Reconstruct graph\ngraph = EventGraph.from_dict(graph_data)\n</code></pre>"},{"location":"user-guide/events/event_graph/#integration-with-event-store","title":"Integration with Event Store","text":"<pre><code>def save_graph_to_event_store(graph: EventGraph, event_store: EventStore):\n    \"\"\"Save event graph analysis as a special event.\"\"\"\n    from grafi.common.events.topic_events.topic_event import TopicEvent\n\n    graph_event = TopicEvent(\n        name=\"event_graph_analysis\",\n        offset=0,\n        data=graph.to_dict()\n    )\n\n    event_store.record_event(graph_event)\n</code></pre>"},{"location":"user-guide/events/event_graph/#performance-considerations","title":"Performance Considerations","text":""},{"location":"user-guide/events/event_graph/#time-complexity","title":"Time Complexity","text":"Operation Time Complexity Description Graph Construction O(V + E) V = events, E = relationships Topological Sort O(V log V + E) Heap operations + edge processing Root Node Finding O(V) Linear scan of all nodes Dependency Lookup O(1) Direct dictionary access"},{"location":"user-guide/events/event_graph/#memory-optimization","title":"Memory Optimization","text":"<pre><code>def optimize_graph_memory(graph: EventGraph):\n    \"\"\"Optimize graph memory usage for large datasets.\"\"\"\n    # Remove unnecessary downstream references if only upstream needed\n    for node in graph.nodes.values():\n        if not_needed_downstream():\n            node.downstream_events.clear()\n\n    # Implement node pooling for frequent operations\n    # Consider lazy loading for large graphs\n</code></pre>"},{"location":"user-guide/events/event_graph/#scaling-strategies","title":"Scaling Strategies","text":"<ol> <li>Chunked Processing: Process large event sets in chunks</li> <li>Lazy Loading: Load only required portions of the graph</li> <li>Caching: Cache topological sort results</li> <li>Parallel Construction: Build sub-graphs in parallel</li> </ol>"},{"location":"user-guide/events/event_graph/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/events/event_graph/#graph-construction","title":"Graph Construction","text":"<ol> <li>Event Validation: Validate events before adding to graph</li> <li>Cycle Detection: Check for cycles during construction</li> <li>Memory Management: Monitor memory usage for large graphs</li> <li>Error Handling: Handle malformed event relationships gracefully</li> </ol>"},{"location":"user-guide/events/event_graph/#analysis-operations","title":"Analysis Operations","text":"<ol> <li>Batch Processing: Process multiple analyses together</li> <li>Result Caching: Cache expensive analysis results</li> <li>Incremental Updates: Support incremental graph updates</li> <li>Parallel Analysis: Use parallel processing for independent analyses</li> </ol>"},{"location":"user-guide/events/event_graph/#integration-patterns","title":"Integration Patterns","text":"<ol> <li>Event Store Integration: Persist graphs for historical analysis</li> <li>Monitoring Integration: Use graphs for real-time monitoring</li> <li>Debugging Support: Leverage graphs for debugging event flows</li> <li>Performance Analysis: Use graphs to identify bottlenecks</li> </ol>"},{"location":"user-guide/events/event_graph/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/events/event_graph/#common-issues","title":"Common Issues","text":"<ol> <li>Cycle Detection: Use topological sort to detect cycles</li> <li>Missing Dependencies: Validate all referenced events exist</li> <li>Memory Issues: Monitor graph size and optimize accordingly</li> <li>Performance Problems: Profile and optimize hot paths</li> </ol>"},{"location":"user-guide/events/event_graph/#debugging-tools","title":"Debugging Tools","text":"<pre><code>def debug_graph_structure(graph: EventGraph):\n    \"\"\"Print detailed graph structure for debugging.\"\"\"\n    print(f\"Graph contains {len(graph.nodes)} nodes\")\n    print(f\"Root nodes: {len(graph.root_nodes)}\")\n\n    for node in graph.nodes.values():\n        print(f\"Node {node.event_id}:\")\n        print(f\"  Upstream: {node.upstream_events}\")\n        print(f\"  Downstream: {node.downstream_events}\")\n        print(f\"  Timestamp: {node.event.timestamp}\")\n</code></pre> <p>The event graph system provides powerful capabilities for understanding and analyzing event relationships in complex topic-based messaging scenarios, enabling proper event ordering, causality analysis, and system debugging.</p>"},{"location":"user-guide/events/event_store/","title":"Event Store","text":"<p>The Graphite event store system provides persistent storage and retrieval capabilities for events within the event-driven architecture. It supports multiple storage backends including in-memory and PostgreSQL implementations, enabling flexible deployment options from development to production environments.</p>"},{"location":"user-guide/events/event_store/#overview","title":"Overview","text":"<p>The event store system is built around a common interface that supports:</p> <ul> <li>Event Recording: Store single events or batches of events</li> <li>Event Retrieval: Query events by ID, assistant request, or conversation</li> <li>Multiple Backends: In-memory for development, PostgreSQL for production</li> <li>Event Reconstruction: Deserialize stored events back to typed objects</li> </ul>"},{"location":"user-guide/events/event_store/#base-eventstore-interface","title":"Base EventStore Interface","text":"<p>All event store implementations inherit from the base <code>EventStore</code> class, which defines the standard interface for event persistence.</p>"},{"location":"user-guide/events/event_store/#core-methods","title":"Core Methods","text":"Method Signature Description <code>record_event</code> <code>(event: Event) -&gt; None</code> Record a single event to the store <code>record_events</code> <code>(events: List[Event]) -&gt; None</code> Record multiple events in batch <code>clear_events</code> <code>() -&gt; None</code> Clear all events from the store <code>get_events</code> <code>() -&gt; List[Event]</code> Retrieve all events from the store <code>get_event</code> <code>(event_id: str) -&gt; Optional[Event]</code> Get a specific event by ID <code>get_agent_events</code> <code>(assistant_request_id: str) -&gt; List[Event]</code> Get events for an assistant request <code>get_conversation_events</code> <code>(conversation_id: str) -&gt; List[Event]</code> Get events for a conversation"},{"location":"user-guide/events/event_store/#event-reconstruction","title":"Event Reconstruction","text":"<p>The base class provides helper methods for converting stored data back to event objects:</p> Method Signature Description <code>_create_event_from_dict</code> <code>(event_dict: Dict[str, Any]) -&gt; Optional[Event]</code> Create event object from dictionary <code>_get_event_class</code> <code>(event_type: str) -&gt; Optional[Type[Event]]</code> Get event class for event type"},{"location":"user-guide/events/event_store/#in-memory-event-store","title":"In-Memory Event Store","text":"<p>The <code>EventStoreInMemory</code> provides a simple, memory-based storage solution ideal for development, testing, and lightweight applications.</p>"},{"location":"user-guide/events/event_store/#features","title":"Features","text":"<ul> <li>Zero Dependencies: No external database required</li> <li>Fast Access: Direct memory access for all operations</li> <li>Simplicity: Easy setup and configuration</li> <li>Temporary Storage: Data is lost when application restarts</li> </ul>"},{"location":"user-guide/events/event_store/#usage","title":"Usage","text":"<pre><code>from grafi.common.event_stores.event_store_in_memory import EventStoreInMemory\nfrom grafi.common.events.tool_events.tool_invoke_event import ToolInvokeEvent\n\n# Initialize the store\nevent_store = EventStoreInMemory()\n\n# Record an event\nevent = ToolInvokeEvent(\n    invoke_context=context,\n    tool_name=\"OpenAI Tool\",\n    tool_type=\"LLMTool\",\n    input_data=[Message(role=\"user\", content=\"Hello\")]\n)\nevent_store.record_event(event)\n\n# Retrieve events\nall_events = event_store.get_events()\nspecific_event = await event_store.get_event(event.event_id)\nrequest_events = event_store.get_agent_events(\"req_123\")\n</code></pre>"},{"location":"user-guide/events/event_store/#implementation-details","title":"Implementation Details","text":"<pre><code>class EventStoreInMemory(EventStore):\n    def __init__(self) -&gt; None:\n        self.events = []\n\n    async def record_event(self, event: Event) -&gt; None:\n        self.events.append(event)\n\n    async def get_events(self) -&gt; List[Event]:\n        return self.events.copy()\n\n    async def get_event(self, event_id: str) -&gt; Optional[Event]:\n        for event in self.events:\n            if event.event_id == event_id:\n                return event\n        return None\n</code></pre>"},{"location":"user-guide/events/event_store/#postgresql-event-store","title":"PostgreSQL Event Store","text":"<p>The <code>EventStorePostgres</code> provides a production-ready, persistent storage solution using PostgreSQL as the backend database.</p>"},{"location":"user-guide/events/event_store/#postgresql-features","title":"PostgreSQL Features","text":"<ul> <li>Persistent Storage: Data survives application restarts</li> <li>ACID Compliance: Full transaction support</li> <li>Scalability: Handles large volumes of events</li> <li>Advanced Querying: SQL-based filtering and aggregation</li> <li>JSON Support: Native JSONB storage for event data</li> </ul>"},{"location":"user-guide/events/event_store/#database-schema","title":"Database Schema","text":"<p>The PostgreSQL implementation uses a single <code>events</code> table:</p> Column Type Description <code>id</code> <code>Integer</code> Auto-increment primary key <code>event_id</code> <code>String</code> Unique event identifier (indexed) <code>conversation_id</code> <code>String</code> Conversation identifier (indexed) <code>assistant_request_id</code> <code>String</code> Assistant request identifier (indexed) <code>event_type</code> <code>String</code> Type of event <code>event_context</code> <code>JSONB</code> Event context data <code>data</code> <code>JSON</code> Event-specific data <code>timestamp</code> <code>DateTime</code> Event creation timestamp"},{"location":"user-guide/events/event_store/#setup-and-configuration","title":"Setup and Configuration","text":"<pre><code>from grafi.common.event_stores.event_store_postgres import EventStorePostgres\n\n# Initialize with database URL\ndb_url = \"postgresql://user:password@localhost:5432/graphite_events\"\nevent_store = EventStorePostgres(db_url)\n\n# The database schema is automatically created\n</code></pre>"},{"location":"user-guide/events/event_store/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/events/event_store/#recording-events","title":"Recording Events","text":"<pre><code># Record single event\nawait event_store.record_event(event)\n\n# Record multiple events (batch operation)\nevents = [event1, event2, event3]\nawait event_store.record_events(events)\n</code></pre>"},{"location":"user-guide/events/event_store/#querying-events","title":"Querying Events","text":"<pre><code># Get specific event\nevent = await event_store.get_event(\"event_123\")\n\n# Get all events for an assistant request\nrequest_events = await event_store.get_agent_events(\"req_456\")\n\n# Get all events for a conversation\nconversation_events = await event_store.get_conversation_events(\"conv_789\")\n</code></pre>"},{"location":"user-guide/events/event_store/#error-handling","title":"Error Handling","text":"<p>The PostgreSQL store includes comprehensive error handling:</p> <pre><code>def record_event(self, event: Event) -&gt; None:\n    session = self.Session()\n    try:\n        # Convert and store event\n        model = EventModel(...)\n        session.add(model)\n        session.commit()\n    except Exception as e:\n        session.rollback()\n        logger.error(f\"Failed to record event: {e}\")\n        raise e\n    finally:\n        session.close()\n</code></pre>"},{"location":"user-guide/events/event_store/#event-serialization-and-deserialization","title":"Event Serialization and Deserialization","text":"<p>The event store system handles automatic conversion between event objects and storage formats.</p>"},{"location":"user-guide/events/event_store/#serialization-process","title":"Serialization Process","text":"<ol> <li>Event to Dictionary: Events are converted to dictionaries using <code>event.to_dict()</code></li> <li>Context Extraction: Key fields are extracted for indexing</li> <li>JSON Storage: Event data is stored as JSON/JSONB</li> </ol> <pre><code>event_dict = event.to_dict()\n# Produces:\n{\n    \"event_id\": \"evt_123\",\n    \"assistant_request_id\": \"req_456\",\n    \"event_type\": \"ToolInvoke\",\n    \"event_context\": {...},\n    \"data\": {...},\n    \"timestamp\": \"2025-07-01T10:30:00Z\"\n}\n</code></pre>"},{"location":"user-guide/events/event_store/#deserialization-process","title":"Deserialization Process","text":"<ol> <li>Type Resolution: Event type determines the target class</li> <li>Class Loading: Appropriate event class is loaded</li> <li>Object Creation: Event object is reconstructed using <code>from_dict()</code></li> </ol> <pre><code>def _get_event_class(self, event_type: str) -&gt; Optional[Type[Event]]:\n    event_classes = {\n        EventType.TOOL_INVOKE.value: ToolInvokeEvent,\n        EventType.TOOL_RESPOND.value: ToolRespondEvent,\n        EventType.ASSISTANT_INVOKE.value: AssistantInvokeEvent,\n        # ... all supported event types\n    }\n    return event_classes.get(event_type)\n</code></pre>"},{"location":"user-guide/events/event_store/#supported-event-types","title":"Supported Event Types","text":"<p>The event store supports all event types defined in the Graphite events system:</p>"},{"location":"user-guide/events/event_store/#component-events","title":"Component Events","text":"<ul> <li>Node Events: <code>NODE_INVOKE</code>, <code>NODE_RESPOND</code>, <code>NODE_FAILED</code></li> <li>Tool Events: <code>TOOL_INVOKE</code>, <code>TOOL_RESPOND</code>, <code>TOOL_FAILED</code></li> <li>Workflow Events: <code>WORKFLOW_INVOKE</code>, <code>WORKFLOW_RESPOND</code>, <code>WORKFLOW_FAILED</code></li> <li>Assistant Events: <code>ASSISTANT_INVOKE</code>, <code>ASSISTANT_RESPOND</code>, <code>ASSISTANT_FAILED</code></li> </ul>"},{"location":"user-guide/events/event_store/#topic-events","title":"Topic Events","text":"<ul> <li>Basic Events: <code>TOPIC_EVENT</code></li> <li>Communication Events: <code>PUBLISH_TO_TOPIC</code>, <code>CONSUME_FROM_TOPIC</code></li> <li>Output Events: <code>OUTPUT_TOPIC</code></li> </ul>"},{"location":"user-guide/events/event_store/#integration-patterns","title":"Integration Patterns","text":""},{"location":"user-guide/events/event_store/#event-driven-architecture","title":"Event-Driven Architecture","text":"<pre><code>class MyWorkflow:\n    def __init__(self, event_store: EventStore):\n        self.event_store = event_store\n\n    async def process_request(self, request):\n        # Record start event\n        start_event = WorkflowInvokeEvent(...)\n        self.event_store.record_event(start_event)\n\n        try:\n            # Process request\n            result = self.do_work(request)\n\n            # Record success event\n            success_event = WorkflowRespondEvent(...)\n            await self.event_store.record_event(success_event)\n\n            return result\n        except Exception as e:\n            # Record failure event\n            failure_event = WorkflowFailedEvent(...)\n            await self.event_store.record_event(failure_event)\n            raise\n</code></pre>"},{"location":"user-guide/events/event_store/#event-sourcing","title":"Event Sourcing","text":"<pre><code>async def rebuild_conversation_state(conversation_id: str, event_store: EventStore):\n    \"\"\"Rebuild conversation state from events.\"\"\"\n    events = await event_store.get_conversation_events(conversation_id)\n\n    state = ConversationState()\n    for event in sorted(events, key=lambda e: e.timestamp):\n        state.apply_event(event)\n\n    return state\n</code></pre>"},{"location":"user-guide/events/event_store/#observability-and-monitoring","title":"Observability and Monitoring","text":"<pre><code>async def monitor_assistant_performance(assistant_request_id: str, event_store: EventStore):\n    \"\"\"Monitor assistant performance using events.\"\"\"\n    events = await event_store.get_agent_events(assistant_request_id)\n\n    invoke_events = [e for e in events if isinstance(e, AssistantInvokeEvent)]\n    respond_events = [e for e in events if isinstance(e, AssistantRespondEvent)]\n    failed_events = [e for e in events if isinstance(e, AssistantFailedEvent)]\n\n    return {\n        \"total_requests\": len(invoke_events),\n        \"successful_responses\": len(respond_events),\n        \"failures\": len(failed_events),\n        \"success_rate\": len(respond_events) / len(invoke_events) if invoke_events else 0\n    }\n</code></pre>"},{"location":"user-guide/events/event_store/#configuration-and-deployment","title":"Configuration and Deployment","text":""},{"location":"user-guide/events/event_store/#development-configuration","title":"Development Configuration","text":"<pre><code># Use in-memory store for development\nfrom grafi.common.event_stores.event_store_in_memory import EventStoreInMemory\n\nevent_store = EventStoreInMemory()\n</code></pre>"},{"location":"user-guide/events/event_store/#production-configuration","title":"Production Configuration","text":"<pre><code># Use PostgreSQL for production\nfrom grafi.common.event_stores.event_store_postgres import EventStorePostgres\nimport os\n\ndb_url = os.getenv(\"DATABASE_URL\", \"postgresql://user:pass@localhost/events\")\nevent_store = EventStorePostgres(db_url)\n</code></pre>"},{"location":"user-guide/events/event_store/#environment-based-selection","title":"Environment-Based Selection","text":"<pre><code>def create_event_store() -&gt; EventStore:\n    \"\"\"Create event store based on environment.\"\"\"\n    env = os.getenv(\"ENVIRONMENT\", \"development\")\n\n    if env == \"production\":\n        db_url = os.getenv(\"DATABASE_URL\")\n        if not db_url:\n            raise ValueError(\"DATABASE_URL required for production\")\n        return EventStorePostgres(db_url)\n    else:\n        return EventStoreInMemory()\n</code></pre>"},{"location":"user-guide/events/event_store/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/events/event_store/#event-store-selection","title":"Event Store Selection","text":"<ol> <li>Development: Use <code>EventStoreInMemory</code> for rapid development and testing</li> <li>Testing: Use <code>EventStoreInMemory</code> for unit tests and integration tests</li> <li>Production: Use <code>EventStorePostgres</code> for persistent, scalable storage</li> <li>CI/CD: Use <code>EventStoreInMemory</code> for continuous integration pipelines</li> </ol>"},{"location":"user-guide/events/event_store/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Batch Operations: Use <code>record_events()</code> for multiple events</li> <li>Indexing: Leverage database indexes for common query patterns</li> <li>Connection Pooling: Configure SQLAlchemy connection pooling</li> <li>Query Optimization: Use specific queries rather than retrieving all events</li> </ol>"},{"location":"user-guide/events/event_store/#error-handling-best-practices","title":"Error Handling Best Practices","text":"<ol> <li>Transaction Management: Use database transactions for consistency</li> <li>Retry Logic: Implement retry mechanisms for transient failures</li> <li>Logging: Log all event store operations for debugging</li> <li>Graceful Degradation: Handle event store failures gracefully</li> </ol>"},{"location":"user-guide/events/event_store/#data-management","title":"Data Management","text":"<ol> <li>Archival: Implement event archival for old events</li> <li>Partitioning: Use database partitioning for large event volumes</li> <li>Backup: Regular backup of event data</li> <li>Monitoring: Monitor event store performance and capacity</li> </ol>"},{"location":"user-guide/events/event_store/#security-considerations","title":"Security Considerations","text":""},{"location":"user-guide/events/event_store/#data-protection","title":"Data Protection","text":"<ul> <li>Encryption: Encrypt sensitive event data</li> <li>Access Control: Implement proper database access controls</li> <li>Audit Logging: Log access to event data</li> <li>Data Retention: Implement appropriate data retention policies</li> </ul>"},{"location":"user-guide/events/event_store/#privacy-compliance","title":"Privacy Compliance","text":"<ul> <li>Personal Data: Handle personal data in events according to privacy regulations</li> <li>Data Anonymization: Consider anonymizing events for analytics</li> <li>Right to be Forgotten: Implement event deletion capabilities</li> </ul>"},{"location":"user-guide/events/event_store/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/events/event_store/#common-issues","title":"Common Issues","text":"<ol> <li>Connection Failures: Check database connectivity and credentials</li> <li>Schema Errors: Ensure database schema is properly created</li> <li>Serialization Errors: Verify event objects implement required methods</li> <li>Performance Issues: Monitor query performance and optimize indexes</li> </ol>"},{"location":"user-guide/events/event_store/#debugging","title":"Debugging","text":"<pre><code>import logging\n\n# Enable SQLAlchemy logging\nlogging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)\n\n# Monitor event store operations\nlogger.info(f\"Recording event: {event.event_type}\")\nawait event_store.record_event(event)\nlogger.info(f\"Event recorded successfully: {event.event_id}\")\n</code></pre> <p>The event store system provides a robust foundation for event persistence in Graphite's event-driven architecture, supporting both development flexibility and production scalability requirements.</p>"},{"location":"user-guide/events/events/","title":"Events System","text":"<p>The Graphite events system provides a comprehensive event-driven architecture for tracking, monitoring, and coordinating activities across assistants, nodes, tools, workflows, and topics. This system enables observability, debugging, and loose coupling between components through a standardized event model.</p>"},{"location":"user-guide/events/events/#overview","title":"Overview","text":"<p>The events system is built around a hierarchical structure where:</p> <ul> <li>Base Event: Provides common fields and functionality for all events</li> <li>Category Events: Abstract base classes for different component types (Assistant, Node, Tool, Workflow, Topic)  </li> <li>Specific Events: Concrete event implementations for specific actions (Invoke, Respond, Failed)</li> </ul>"},{"location":"user-guide/events/events/#base-event-class","title":"Base Event Class","text":"<p>All events inherit from the base <code>Event</code> class, which provides core functionality and common fields.</p>"},{"location":"user-guide/events/events/#fields","title":"Fields","text":"Field Type Description <code>event_id</code> <code>EventId</code> Unique identifier for the event (automatically generated using <code>default_id</code>). <code>event_version</code> <code>str</code> Version identifier for the event schema (defaults to <code>\"1.0\"</code>). <code>invoke_context</code> <code>InvokeContext</code> Context information about the operation that triggered this event. <code>event_type</code> <code>EventType</code> Enumerated type indicating the specific kind of event. <code>timestamp</code> <code>datetime</code> UTC timestamp when the event was created (automatically generated)."},{"location":"user-guide/events/events/#methods","title":"Methods","text":"Method Signature Description <code>event_dict</code> <code>() -&gt; Dict[str, Any]</code> Returns base event data as a dictionary with flattened invoke_context. <code>event_base</code> <code>classmethod (event_dict: dict) -&gt; Tuple[...]</code> Extracts common event data from a dictionary. <code>to_dict</code> <code>() -&gt; Dict[str, Any]</code> Abstract method for converting event to dictionary (must be implemented). <code>from_dict</code> <code>classmethod (data: Dict[str, Any]) -&gt; Event</code> Abstract method for creating event from dictionary (must be implemented)."},{"location":"user-guide/events/events/#event-types","title":"Event Types","text":"<p>The <code>EventType</code> enumeration defines all supported event types:</p>"},{"location":"user-guide/events/events/#component-events","title":"Component Events","text":"<ul> <li><code>NODE_INVOKE</code>, <code>NODE_RESPOND</code>, <code>NODE_FAILED</code></li> <li><code>TOOL_INVOKE</code>, <code>TOOL_RESPOND</code>, <code>TOOL_FAILED</code> </li> <li><code>WORKFLOW_INVOKE</code>, <code>WORKFLOW_RESPOND</code>, <code>WORKFLOW_FAILED</code></li> <li><code>ASSISTANT_INVOKE</code>, <code>ASSISTANT_RESPOND</code>, <code>ASSISTANT_FAILED</code></li> </ul>"},{"location":"user-guide/events/events/#topic-events","title":"Topic Events","text":"<ul> <li><code>TOPIC_EVENT</code>, <code>STREAM_TOPIC_EVENT</code></li> <li><code>PUBLISH_TO_TOPIC</code>, <code>CONSUME_FROM_TOPIC</code></li> <li><code>OUTPUT_TOPIC</code></li> </ul>"},{"location":"user-guide/events/events/#assistant-events","title":"Assistant Events","text":"<p>Assistant events track the lifecycle and operations of AI assistants within the system.</p>"},{"location":"user-guide/events/events/#assistantevent-base-class","title":"AssistantEvent (Base Class)","text":"<p>Base class for all assistant-related events.</p>"},{"location":"user-guide/events/events/#assistantevent-fields","title":"AssistantEvent Fields","text":"Field Type Description <code>assistant_id</code> <code>str</code> Unique identifier for the assistant instance. <code>assistant_name</code> <code>str</code> Human-readable name of the assistant. <code>assistant_type</code> <code>str</code> Type/category of the assistant."},{"location":"user-guide/events/events/#assistantinvokeevent","title":"AssistantInvokeEvent","text":"<p>Emitted when an assistant is invoked with input data.</p>"},{"location":"user-guide/events/events/#assistantinvokeevent-fields","title":"AssistantInvokeEvent Fields","text":"Field Type Description <code>input_data</code> <code>Messages</code> Input messages provided to the assistant."},{"location":"user-guide/events/events/#example-usage","title":"Example Usage","text":"<pre><code>from grafi.common.events.assistant_events.assistant_invoke_event import AssistantInvokeEvent\nfrom grafi.common.models.message import Message\n\nevent = AssistantInvokeEvent(\n    invoke_context=context,\n    assistant_name=\"Chat Assistant\",\n    assistant_type=\"ConversationalAssistant\",\n    input_data=[Message(role=\"user\", content=\"Hello!\")]\n)\n\n# Convert to dictionary for storage/transmission\nevent_dict = event.to_dict()\n\n# Reconstruct from dictionary\nreconstructed_event = AssistantInvokeEvent.from_dict(event_dict)\n</code></pre>"},{"location":"user-guide/events/events/#assistantrespondevent","title":"AssistantRespondEvent","text":"<p>Emitted when an assistant produces a response.</p>"},{"location":"user-guide/events/events/#assistantrespondevent-fields","title":"AssistantRespondEvent Fields","text":"Field Type Description <code>input_data</code> <code>Messages</code> Original input messages provided to the assistant. <code>output_data</code> <code>Messages</code> Response messages generated by the assistant."},{"location":"user-guide/events/events/#assistantfailedevent","title":"AssistantFailedEvent","text":"<p>Emitted when an assistant operation fails.</p>"},{"location":"user-guide/events/events/#assistantfailedevent-fields","title":"AssistantFailedEvent Fields","text":"Field Type Description <code>error_message</code> <code>str</code> Description of the error that occurred. <code>error_type</code> <code>str</code> Type/category of the error."},{"location":"user-guide/events/events/#node-events","title":"Node Events","text":"<p>Node events track the execution and status of individual nodes in workflows.</p>"},{"location":"user-guide/events/events/#nodeevent-base-class","title":"NodeEvent (Base Class)","text":"<p>Base class for all node-related events.</p>"},{"location":"user-guide/events/events/#nodeevent-fields","title":"NodeEvent Fields","text":"Field Type Description <code>node_id</code> <code>str</code> Unique identifier for the node instance. <code>node_name</code> <code>str</code> Human-readable name of the node. <code>node_type</code> <code>str</code> Type/category of the node. <code>subscribed_topics</code> <code>List[str]</code> Topics this node subscribes to. <code>publish_to_topics</code> <code>List[str]</code> Topics this node publishes to."},{"location":"user-guide/events/events/#nodeinvokeevent","title":"NodeInvokeEvent","text":"<p>Emitted when a node begins processing.</p>"},{"location":"user-guide/events/events/#nodeinvokeevent-fields","title":"NodeInvokeEvent Fields","text":"Field Type Description <code>input_data</code> <code>Messages</code> Input messages provided to the node."},{"location":"user-guide/events/events/#noderespondevent","title":"NodeRespondEvent","text":"<p>Emitted when a node completes processing successfully.</p>"},{"location":"user-guide/events/events/#noderespondevent-fields","title":"NodeRespondEvent Fields","text":"Field Type Description <code>output_data</code> <code>Messages</code> Output messages generated by the node."},{"location":"user-guide/events/events/#nodefailedevent","title":"NodeFailedEvent","text":"<p>Emitted when a node operation fails.</p>"},{"location":"user-guide/events/events/#nodefailedevent-fields","title":"NodeFailedEvent Fields","text":"Field Type Description <code>error_message</code> <code>str</code> Description of the error that occurred. <code>error_type</code> <code>str</code> Type/category of the error."},{"location":"user-guide/events/events/#tool-events","title":"Tool Events","text":"<p>Tool events track the execution and status of tools within the system.</p>"},{"location":"user-guide/events/events/#toolevent-base-class","title":"ToolEvent (Base Class)","text":"<p>Base class for all tool-related events.</p>"},{"location":"user-guide/events/events/#toolevent-fields","title":"ToolEvent Fields","text":"Field Type Description <code>tool_id</code> <code>str</code> Unique identifier for the tool instance. <code>tool_name</code> <code>str</code> Human-readable name of the tool. <code>tool_type</code> <code>str</code> Type/category of the tool."},{"location":"user-guide/events/events/#toolinvokeevent","title":"ToolInvokeEvent","text":"<p>Emitted when a tool is invoked.</p>"},{"location":"user-guide/events/events/#toolinvokeevent-fields","title":"ToolInvokeEvent Fields","text":"Field Type Description <code>input_data</code> <code>Messages</code> Input messages provided to the tool."},{"location":"user-guide/events/events/#toolrespondevent","title":"ToolRespondEvent","text":"<p>Emitted when a tool produces a response.</p>"},{"location":"user-guide/events/events/#toolrespondevent-fields","title":"ToolRespondEvent Fields","text":"Field Type Description <code>output_data</code> <code>Messages</code> Response messages generated by the tool."},{"location":"user-guide/events/events/#toolfailedevent","title":"ToolFailedEvent","text":"<p>Emitted when a tool operation fails.</p>"},{"location":"user-guide/events/events/#toolfailedevent-fields","title":"ToolFailedEvent Fields","text":"Field Type Description <code>error_message</code> <code>str</code> Description of the error that occurred. <code>error_type</code> <code>str</code> Type/category of the error."},{"location":"user-guide/events/events/#workflow-events","title":"Workflow Events","text":"<p>Workflow events track the execution and status of entire workflows.</p>"},{"location":"user-guide/events/events/#workflowevent-base-class","title":"WorkflowEvent (Base Class)","text":"<p>Base class for all workflow-related events.</p>"},{"location":"user-guide/events/events/#workflowevent-fields","title":"WorkflowEvent Fields","text":"Field Type Description <code>workflow_id</code> <code>str</code> Unique identifier for the workflow instance. <code>workflow_name</code> <code>str</code> Human-readable name of the workflow. <code>workflow_type</code> <code>str</code> Type/category of the workflow."},{"location":"user-guide/events/events/#workflowinvokeevent","title":"WorkflowInvokeEvent","text":"<p>Emitted when a workflow begins execution.</p>"},{"location":"user-guide/events/events/#workflowinvokeevent-fields","title":"WorkflowInvokeEvent Fields","text":"Field Type Description <code>input_data</code> <code>Messages</code> Input messages provided to the workflow."},{"location":"user-guide/events/events/#workflowrespondevent","title":"WorkflowRespondEvent","text":"<p>Emitted when a workflow completes successfully.</p>"},{"location":"user-guide/events/events/#workflowrespondevent-fields","title":"WorkflowRespondEvent Fields","text":"Field Type Description <code>output_data</code> <code>Messages</code> Final output messages from the workflow."},{"location":"user-guide/events/events/#workflowfailedevent","title":"WorkflowFailedEvent","text":"<p>Emitted when a workflow execution fails.</p>"},{"location":"user-guide/events/events/#workflowfailedevent-fields","title":"WorkflowFailedEvent Fields","text":"Field Type Description <code>error_message</code> <code>str</code> Description of the error that occurred. <code>error_type</code> <code>str</code> Type/category of the error."},{"location":"user-guide/events/events/#topic-events_1","title":"Topic Events","text":"<p>Topic events handle message passing and communication between components. These are the primary events used for component interaction in the current architecture.</p>"},{"location":"user-guide/events/events/#topicevent-base-class","title":"TopicEvent (Base Class)","text":"<p>Basic event for topic-related activities.</p>"},{"location":"user-guide/events/events/#topicevent-base-fields-extended-from-event","title":"TopicEvent Base Fields (Extended from Event)","text":"Field Type Description <code>name</code> <code>str</code> Name of the topic. <code>type</code> <code>TopicType</code> Type of the topic. <code>offset</code> <code>int</code> Position/offset in the topic stream. <code>data</code> <code>List[Message]</code> Data associated with the topic event."},{"location":"user-guide/events/events/#publishtotopicevent","title":"PublishToTopicEvent","text":"<p>The primary event used for publishing data to topics. This is the output format for nodes and the input format for workflows and assistants.</p>"},{"location":"user-guide/events/events/#publishtotopicevent-fields-extended-from-topicevent","title":"PublishToTopicEvent Fields (Extended from TopicEvent)","text":"Field Type Description <code>publisher_name</code> <code>str</code> Name of the component publishing the data. <code>publisher_type</code> <code>str</code> Type of the component publishing the data. <code>consumed_event_ids</code> <code>List[EventId]</code> IDs of events consumed before publishing."},{"location":"user-guide/events/events/#consumefromtopicevent","title":"ConsumeFromTopicEvent","text":"<p>The primary event used when consuming data from topics. This is the input format for nodes and the output format for workflows and assistants.</p>"},{"location":"user-guide/events/events/#consumefromtopicevent-fields-extended-from-topicevent","title":"ConsumeFromTopicEvent Fields  (Extended from TopicEvent)","text":"Field Type Description <code>consumer_name</code> <code>str</code> Name of the original publisher. <code>consumer_type</code> <code>str</code> Type of the original publisher."},{"location":"user-guide/events/events/#event-usage-patterns","title":"Event Usage Patterns","text":""},{"location":"user-guide/events/events/#creating-events","title":"Creating Events","text":"<pre><code>from grafi.common.events.tool_events.tool_invoke_event import ToolInvokeEvent\nfrom grafi.common.models.invoke_context import InvokeContext\nfrom grafi.common.models.message import Message\n\n# Create invoke context\ncontext = InvokeContext(assistant_request_id=\"req_123\")\n\n# Create tool invoke event\nevent = ToolInvokeEvent(\n    invoke_context=context,\n    tool_name=\"OpenAI Tool\",\n    tool_type=\"LLMTool\",\n    input_data=[Message(role=\"user\", content=\"Hello\")]\n)\n</code></pre>"},{"location":"user-guide/events/events/#serialization-and-deserialization","title":"Serialization and Deserialization","text":"<pre><code># Convert event to dictionary for storage/transmission\nevent_dict = event.to_dict()\n\n# Store or transmit event_dict...\n\n# Reconstruct event from dictionary\nreconstructed_event = ToolInvokeEvent.from_dict(event_dict)\n</code></pre>"},{"location":"user-guide/events/events/#event-context-and-metadata","title":"Event Context and Metadata","text":""},{"location":"user-guide/events/events/#invokecontext","title":"InvokeContext","text":"<p>All events carry an <code>InvokeContext</code> that provides:</p> <ul> <li>Request Tracking: <code>assistant_request_id</code> for correlating related events</li> <li>Workflow Context: Information about the current workflow execution  </li> <li>Metadata: Additional context-specific information</li> </ul>"},{"location":"user-guide/events/events/#event-context","title":"Event Context","text":"<p>Each event type includes structured context information:</p> <pre><code># Assistant events include:\n{\n    \"assistant_id\": \"asst_123\",\n    \"assistant_name\": \"Chat Assistant\",\n    \"assistant_type\": \"ConversationalAssistant\",\n    \"invoke_context\": {...}\n}\n\n# Tool events include:\n{\n    \"tool_id\": \"tool_456\",\n    \"tool_name\": \"OpenAI Tool\",\n    \"tool_type\": \"LLMTool\",\n    \"invoke_context\": {...}\n}\n\n# Topic events include:\n{\n    \"consumed_event_ids\": [\"event_1\", \"event_2\"],\n    \"publisher_name\": \"Data Processor\",\n    \"publisher_type\": \"ProcessingNode\",\n    \"name\": \"output_topic\",\n    \"type\": \"topic\",\n    \"offset\": 42,\n    \"invoke_context\": {...}\n}\n</code></pre>"},{"location":"user-guide/events/events/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/events/events/#event-design","title":"Event Design","text":"<ol> <li>Immutability: Events should be immutable once created</li> <li>Complete Information: Include all relevant context for debugging and monitoring</li> <li>Structured Data: Use typed fields rather than generic data blobs</li> <li>Correlation: Ensure proper <code>invoke_context</code> for request correlation</li> </ol>"},{"location":"user-guide/events/events/#event-handling-best-practices","title":"Event Handling Best Practices","text":"<ol> <li>Type Safety: Use <code>isinstance()</code> checks for type-specific handling</li> <li>Error Handling: Always handle potential deserialization errors</li> <li>Logging: Log important events for debugging and monitoring</li> <li>Performance: Avoid expensive operations in event handlers</li> </ol>"},{"location":"user-guide/events/events/#serialization","title":"Serialization","text":"<ol> <li>JSON Compatibility: Ensure all event data is JSON-serializable</li> <li>Schema Versioning: Use <code>event_version</code> for backward compatibility</li> <li>Validation: Validate events during deserialization</li> <li>Error Recovery: Handle malformed event data gracefully</li> </ol>"},{"location":"user-guide/events/events/#topic-event-best-practices","title":"Topic Event Best Practices","text":"<ol> <li>Event Correlation: Use <code>consumed_event_ids</code> to track event dependencies</li> <li>Publisher Information: Always include <code>publisher_name</code> and <code>publisher_type</code> for traceability</li> <li>Offset Management: Properly manage topic offsets for ordered processing</li> <li>Async Handling: Use <code>OutputAsyncEvent</code> for non-blocking output operations</li> </ol>"},{"location":"user-guide/events/events/#integration-with-graphite","title":"Integration with Graphite","text":""},{"location":"user-guide/events/events/#observability","title":"Observability","text":"<p>Events integrate with Graphite's observability system:</p> <ul> <li>Tracing: Events can be correlated with OpenInference traces</li> <li>Monitoring: Event streams provide real-time system monitoring</li> <li>Debugging: Event history helps diagnose issues</li> </ul>"},{"location":"user-guide/events/events/#workflow-coordination","title":"Workflow Coordination","text":"<p>Events enable loose coupling between workflow components:</p> <ul> <li>Asynchronous Processing: Components can react to events asynchronously</li> <li>Topic-Based Communication: Events flow through topic-based messaging</li> <li>State Management: Events provide a history of state changes</li> </ul>"},{"location":"user-guide/events/events/#error-handling","title":"Error Handling","text":"<p>Failed events provide structured error information:</p> <ul> <li>Error Classification: <code>error_type</code> enables categorized error handling</li> <li>Context Preservation: Failed events retain full context for debugging</li> <li>Recovery Strategies: Error events can trigger recovery mechanisms</li> </ul>"},{"location":"user-guide/events/events/#event-sourcing","title":"Event Sourcing","text":"<p>The events system supports event sourcing patterns:</p> <ul> <li>Event History: Complete history of system state changes</li> <li>Replay Capability: Events can be replayed to reconstruct system state</li> <li>Audit Trail: Events provide a complete audit trail of operations</li> </ul> <p>The events system forms the backbone of Graphite's event-driven architecture, enabling robust, observable, and loosely-coupled systems that can scale and adapt to complex workflow requirements.</p>"},{"location":"user-guide/tools/function-call/","title":"FunctionCallTool","text":"<p><code>FunctionCallTool</code> is designed to allow Language Models (LLMs) to invoke specific Python functions directly through JSON-formatted calls. When a message from the LLM references a particular function name along with arguments, <code>FunctionCallTool</code> checks if it has a function matching that name and, if so, invokes it.</p> <p>This design greatly reduces the complexity of integrating advanced logic: the LLM simply issues a request to invoke a function, and the tool handles the invocation details behind the scenes.</p>"},{"location":"user-guide/tools/function-call/#fields","title":"Fields","text":"Field Description <code>name</code> Descriptive identifier (defaults to <code>\"FunctionCallTool\"</code>). <code>type</code> Tool type (defaults to <code>\"FunctionCallTool\"</code>). <code>function_specs</code> List of <code>FunctionSpec</code> objects describing registered functions and their parameters. <code>functions</code> Dictionary mapping function names to their callable implementations. <code>oi_span_type</code> Semantic tracing attribute (<code>TOOL</code>) for observability."},{"location":"user-guide/tools/function-call/#methods","title":"Methods","text":"Method Description <code>function</code> (Builder Class) Builder method to register a function. Automatically applies <code>@llm_function</code> if not already decorated. <code>get_function_specs</code> Retrieves detailed metadata about registered functions (including parameter info), enabling structured LLM-based function calls. <code>invoke</code> Asynchronous method that evaluates incoming messages for tool calls, matches them to registered functions, and executes with JSON arguments, allowing concurrency and awaiting coroutine functions. <code>to_messages</code> Converts invoke results into <code>Message</code> objects with proper <code>tool_call_id</code> linkage. <code>to_dict</code> Serializes the <code>FunctionCallTool</code> instance, listing function specifications for debugging or persistence."},{"location":"user-guide/tools/function-call/#llm-function-decorator","title":"LLM Function Decorator","text":""},{"location":"user-guide/tools/function-call/#llm_function","title":"@llm_function","text":"<p>The <code>@llm_function</code> decorator exposes methods to Language Learning Models by automatically generating function specifications.</p> <p>Location: <code>grafi.common.decorators.llm_function</code></p> <p>Purpose:</p> <ul> <li>Extracts function metadata (name, docstring, parameters, type hints)</li> <li>Constructs a <code>FunctionSpec</code> object with JSON Schema-compatible parameter descriptions</li> <li>Stores the specification as a <code>_function_spec</code> attribute on the decorated function</li> </ul> <p>Usage:</p> <pre><code>from grafi.common.decorators.llm_function import llm_function\n\n@llm_function\ndef calculate_sum(x: int, y: int, precision: float = 0.1) -&gt; float:\n    \"\"\"\n    Calculate the sum of two numbers with optional precision.\n\n    Args:\n        x (int): The first number to add.\n        y (int): The second number to add.\n        precision (float, optional): Precision level. Defaults to 0.1.\n\n    Returns:\n        float: The sum of x and y.\n    \"\"\"\n    return float(x + y)\n</code></pre> <p>Features:</p> <ul> <li>Automatically maps Python types to JSON Schema types</li> <li>Extracts parameter descriptions from docstrings</li> <li>Marks parameters without defaults as required</li> <li>Supports type hints for comprehensive schema generation</li> </ul> <p>Type Mapping:</p> Python Type JSON Schema Type <code>str</code> <code>\"string\"</code> <code>int</code> <code>\"integer\"</code> <code>float</code> <code>\"number\"</code> <code>bool</code> <code>\"boolean\"</code> <code>list</code> <code>\"array\"</code> <code>dict</code> <code>\"object\"</code> Other <code>\"string\"</code> (default)"},{"location":"user-guide/tools/function-call/#how-it-works","title":"How It Works","text":"<ol> <li> <p>Function Registration: Functions are registered either via the builder pattern using <code>.function()</code> method or by inheriting from <code>FunctionCallTool</code> and decorating methods with <code>@llm_function</code>. This automatically generates <code>FunctionSpec</code> objects describing each function's metadata.</p> </li> <li> <p>Automatic Discovery: When inheriting from <code>FunctionCallTool</code>, the <code>__init_subclass__</code> method automatically discovers all methods decorated with <code>@llm_function</code> and adds them to the <code>functions</code> dictionary and <code>function_specs</code> list.</p> </li> <li> <p>Message Processing: When <code>invoke</code> receives messages, it examines the <code>tool_calls</code> field in the first message to find function calls that match registered function names.</p> </li> <li> <p>Function Execution: For each matching tool call:</p> </li> <li>Arguments are parsed from JSON in the <code>tool_call.function.arguments</code> field</li> <li>The corresponding function is called with the parsed arguments</li> <li> <p>Results are converted to <code>Message</code> objects with the appropriate <code>tool_call_id</code></p> </li> <li> <p>Response Handling: The <code>to_messages</code> method formats function results into proper <code>Message</code> objects, maintaining the link between function calls and responses through <code>tool_call_id</code>.</p> </li> </ol>"},{"location":"user-guide/tools/function-call/#usage-and-customization","title":"Usage and Customization","text":"<ul> <li>Builder Pattern: Use the builder's <code>.function(...)</code> method to assign the function you want to expose. This ensures your function is properly decorated if not already.</li> <li>Flexible: By simply swapping out the underlying callable, you can quickly adapt to new or updated logic without modifying the rest of the workflow.</li> <li>Observability: Because <code>FunctionCallTool</code> implements the <code>Tool</code> interface and integrates with the event-driven architecture, all invocations can be monitored and logged.</li> </ul> <p>With <code>FunctionCallTool</code>, you can integrate specialized Python functions into an LLM-driven workflow with minimal extra overhead. As your system grows and evolves, it provides a clean way to add or modify functionality while retaining a uniform interaction pattern with the LLM.</p>"},{"location":"user-guide/tools/function-call/#implementation-examples","title":"Implementation Examples","text":""},{"location":"user-guide/tools/function-call/#example-simple-weather-mock-tool","title":"Example - Simple Weather Mock Tool","text":"<p>A straightforward implementation that inherits from <code>FunctionCallTool</code>. This class provides a simple way to use <code>FunctionCallTool</code> by just instantiating and calling the method:</p> <pre><code>from grafi.common.decorators.llm_function import llm_function\nfrom grafi.tools.function_calls.function_call_tool import FunctionCallTool\n\nclass WeatherMock(FunctionCallTool):\n\n    @llm_function\n    async def get_weather_mock(self, postcode: str):\n        \"\"\"\n        Function to get weather information for a given postcode.\n\n        Args:\n            postcode (str): The postcode for which to retrieve weather information.\n\n        Returns:\n            str: A string containing a weather report for the given postcode.\n        \"\"\"\n        return f\"The weather of {postcode} is bad now.\"\n</code></pre> <p>Key Features:</p> <ul> <li>Uses <code>@llm_function</code> decorator for automatic function registration</li> <li>Simple implementation for basic use cases</li> <li>Inherits all FunctionCallTool capabilities</li> </ul>"},{"location":"user-guide/tools/function-call/#example-tavily-search-tool","title":"Example - Tavily Search Tool","text":"<p>TavilyTool extends FunctionCallTool to provide web search capabilities through the Tavily API. This example demonstrates more complex configuration and a builder pattern for reusable tools.</p> <p>Key Features:</p> <ul> <li>Configurable Search Depth: Supports both \"basic\" and \"advanced\" search modes</li> <li>Token Management: Limits response size to prevent overly large outputs</li> <li>Builder Pattern: Provides fluent configuration interface</li> </ul> <p>Fields:</p> Field Description <code>name</code> Descriptive identifier for the tool (default: <code>\"TavilyTool\"</code>). <code>type</code> Tool type indicator (default: <code>\"TavilyTool\"</code>). <code>client</code> Instance of the <code>TavilyClient</code> used for performing search queries. <code>search_depth</code> Defines the search mode (either <code>\"basic\"</code> or <code>\"advanced\"</code>) for Tavily. <code>max_tokens</code> Limits the total size (in tokens) of the returned JSON string, preventing overly large responses. <p>Usage Example:</p> <pre><code>tavily_tool = (\n    TavilyTool.builder()\n    .api_key(\"YOUR_API_KEY\")\n    .search_depth(\"advanced\")\n    .max_tokens(6000)\n    .build()\n)\n</code></pre> <p>The <code>@llm_function</code> decorated <code>web_search_using_tavily</code> method accepts a query and optional max_results parameter, returning JSON-formatted search results with token management.</p>"},{"location":"user-guide/tools/function-call/#additional-search-tool-examples","title":"Additional Search Tool Examples","text":""},{"location":"user-guide/tools/function-call/#duckduckgo-search-tool","title":"DuckDuckGo Search Tool","text":"<p>DuckDuckGoTool provides web search functionality using the DuckDuckGo Search API, offering a privacy-focused alternative to other search engines.</p> <p>Key Features:</p> <ul> <li>Privacy-focused Search: Uses DuckDuckGo's API for searches without tracking</li> <li>Configurable Parameters: Supports custom headers, proxy settings, and timeout configurations</li> <li>Flexible Result Limits: Allows both fixed and dynamic result count settings</li> </ul> <p>Fields:</p> Field Description <code>name</code> Tool identifier (default: <code>\"DuckDuckGoTool\"</code>). <code>type</code> Tool type (default: <code>\"DuckDuckGoTool\"</code>). <code>fixed_max_results</code> Optional fixed maximum number of results to return. <code>headers</code> Optional custom headers for requests. <code>proxy</code> Optional proxy server configuration. <code>timeout</code> Request timeout in seconds (default: 10). <p>Usage Example:</p> <pre><code>duckduckgo_tool = (\n    DuckDuckGoTool.builder()\n    .fixed_max_results(10)\n    .timeout(15)\n    .build()\n)\n</code></pre>"},{"location":"user-guide/tools/function-call/#google-search-tool","title":"Google Search Tool","text":"<p>GoogleSearchTool extends FunctionCallTool to provide web search functionality using the Google Search API with advanced configuration options.</p> <p>Key Features:</p> <ul> <li>Language Support: Configurable language settings for international searches</li> <li>Result Customization: Fixed or dynamic result count limits</li> <li>Advanced Configuration: Support for custom headers, proxy, and timeout settings</li> </ul> <p>Fields:</p> Field Description <code>name</code> Tool identifier (default: <code>\"GoogleSearchTool\"</code>). <code>type</code> Tool type (default: <code>\"GoogleSearchTool\"</code>). <code>fixed_max_results</code> Optional fixed maximum number of results. <code>fixed_language</code> Optional fixed language code for searches. <code>headers</code> Optional custom headers for requests. <code>proxy</code> Optional proxy server configuration. <code>timeout</code> Request timeout in seconds (default: 10). <p>Usage Example:</p> <pre><code>google_tool = (\n    GoogleSearchTool.builder()\n    .fixed_max_results(8)\n    .fixed_language(\"en\")\n    .timeout(20)\n    .build()\n)\n</code></pre>"},{"location":"user-guide/tools/function-call/#mcp-model-context-protocol-tool","title":"MCP (Model Context Protocol) Tool","text":"<p>MCPTool provides integration with Model Context Protocol servers, enabling access to external tools, resources, and prompts.</p> <p>Key Features:</p> <ul> <li>Dynamic Function Discovery: Automatically discovers and registers functions from MCP servers</li> <li>Resource Access: Provides access to MCP server resources and prompts</li> <li>Extensible Configuration: Supports custom MCP server configurations</li> </ul> <p>Fields:</p> Field Description <code>name</code> Tool identifier (default: <code>\"MCPTool\"</code>). <code>type</code> Tool type (default: <code>\"MCPTool\"</code>). <code>mcp_config</code> Configuration dictionary for MCP server connections. <code>resources</code> List of available MCP resources. <code>prompts</code> List of available MCP prompts. <p>This tool automatically discovers available functions from connected MCP servers and makes them available for LLM function calling.</p>"},{"location":"user-guide/tools/function-call/#agent-calling-tool","title":"Agent Calling Tool","text":"<p><code>AgentCallingTool</code> extends the <code>FunctionCallTool</code> concept to enable multi-agent systems, allowing an LLM to call another agent by name, pass relevant arguments (as a message prompt), and return the agent's response as part of the workflow.</p> <p>Fields:</p> Field Description <code>name</code> Descriptive identifier, defaults to <code>\"AgentCallingTool\"</code>. <code>type</code> Tool type indicator, defaults to <code>\"AgentCallingTool\"</code>. <code>agent_name</code> Name of the agent to call; also used as the tool's name. <code>agent_description</code> High-level explanation of what the agent does, used to generate function specs. <code>argument_description</code> Describes the argument required (e.g., <code>prompt</code>) for the agent call. <code>agent_call</code> A callable that takes <code>(invoke_context, Message)</code> and returns a dictionary (e.g., <code>{\"content\": ...}</code>). <code>oi_span_type</code> OpenInference semantic attribute (<code>TOOL</code>), enabling observability and traceability. <p>Methods:</p> Method Description <code>get_function_specs</code> Returns the function specification (name, description, parameters) for the agent call. <code>invoke</code> Asynchronous method that processes incoming tool calls matching <code>agent_name</code>, passing the <code>prompt</code> to the <code>agent_call</code> callable; yields messages in an async generator for real-time or concurrent agent calls. <code>to_messages</code> Creates a <code>Message</code> object from the agent's response, linking the output to <code>tool_call_id</code>. <code>to_dict</code> Serializes all relevant fields, including agent metadata and the assigned callable, for debugging or persistence. <p>Workflow Example:</p> <ol> <li>Tool Registration: An <code>AgentCallingTool</code> is constructed with details about the agent (<code>agent_name</code>, <code>agent_description</code>, etc.) and the callable (<code>agent_call</code>).</li> <li>Agent Invocation: When an LLM includes a tool call referencing this agent's name, <code>invoke</code> receives the <code>prompt</code> and calls the agent.</li> <li>Response Conversion: The agent's return value is formed into a new <code>Message</code>, which the workflow can then process or forward.</li> </ol> <p>Usage and Customization:</p> <ul> <li>Multi-Agent Systems: By configuring multiple <code>AgentCallingTool</code> instances, you can facilitate dynamic exchanges among multiple agents, each specializing in a different task.</li> <li>Runtime Flexibility: Changing or updating the underlying <code>agent_call</code> logic requires no changes to the rest of the workflow.</li> <li>Parameter Schemas: <code>argument_description</code> ensures the LLM knows which arguments are required and how they should be formatted.</li> </ul> <p>By integrating <code>AgentCallingTool</code> into your event-driven workflow, you can build sophisticated multi-agent systems where each agent can be invoked seamlessly via structured function calls. This approach maintains a clear separation between the LLM's orchestration and the agents' invoke details.</p>"},{"location":"user-guide/tools/function-call/#synthetic-tool","title":"Synthetic Tool","text":"<p><code>SyntheticTool</code> extends the <code>FunctionCallTool</code> that enables LLMs to generate synthetic or modeled data by leveraging another LLM as a data generator. Unlike traditional function call tools that execute predefined logic, <code>SyntheticTool</code> uses an LLM to produce plausible, schema-compliant, outputs based on input specifications\u2014perfect for testing, prototyping, or generating realistic mock data.</p> <p>Fields:</p> Field Description <code>name</code> Descriptive identifier, defaults to <code>\"SyntheticTool\"</code>. <code>type</code> Tool type indicator, defaults to <code>\"SyntheticTool\"</code>. <code>tool_name</code> Name used for function registration and LLM tool calls. <code>description</code> Explanation of what synthetic data this tool generates. <code>input_model</code> Pydantic <code>BaseModel</code> class or JSON schema dict defining expected input structure. <code>output_model</code> Pydantic <code>BaseModel</code> class or JSON schema dict defining generated output structure. <code>model</code> OpenAI model to use for data generation (e.g., <code>\"gpt-4o-mini\"</code>). <code>openai_api_key</code> API key for OpenAI authentication. <code>oi_span_type</code> OpenInference semantic attribute (<code>TOOL</code>), enabling observability and traceability. <p>Methods:</p> Method Description <code>get_function_specs</code> Returns the function specification (name, description, input schema) for the synthetic. tool <code>invoke</code> Processes tool calls, generates synthetic data via LLM, returns schema-compliant JSON responses. <code>ensure_strict_schema</code> Static method that recursively adds <code>additionalProperties: false</code> to JSON schemas for OpenAI strict mode compatibility. <code>to_dict</code> Serializes all relevant fields, including agent metadata and the assigned callable, for debugging or persistence. <p>Workflow Example:</p> <ol> <li>Schema Definition: Define input and output schemas using either Pydantic models (type-safe Python) or JSON Schema dicts (flexible).</li> <li>Function Registration: The tool automatically generates the <code>FunctionSpec</code>, enabling LLMs to discover and call the tool.</li> <li>Tool Invocation: When an LLM invokes the tool with arguments:<ul> <li>Arguments are validated against <code>input_model</code> schema</li> <li>A prompt is constructed with input/output schema specifications</li> <li>LLM generates synthetic data conforming to <code>output_model</code></li> </ul> </li> <li>Structured Output:<ul> <li>Pydantic Mode: Uses OpenAI's <code>beta.chat.completions.parse()</code> with type safety</li> <li>JSON Schema Mode: Uses <code>chat.completions.create()</code> with <code>strict: True</code> for schema validation</li> </ul> </li> <li>Response Handling: Generated data is returned as a <code>Message</code> object linked to the original <code>tool_call_id</code>.</li> </ol> <p>Usage and Customization:</p> <ul> <li>Flexible Schema Definition: By supporting both Pydantic models and JSON schemas, you can choose type-safe Python development or dynamic schema-based configuration without changing the rest of your workflow.</li> <li>Runtime Model Selection: Easily swap between OpenAI models (e.g., <code>gpt-5-mini</code> for cost, <code>gpt-5</code> for quality) to balance generation quality and API costs without modifying tool logic.</li> <li>Schema-Driven Generation: Input and output schemas guide the LLM's data generation, ensuring consistent, validated outputs that conform to your exact specifications.</li> <li>Composable Data Pipelines: Chain multiple <code>SyntheticTool</code> instances where one tool's output becomes another's input, creating sophisticated data generation workflows.</li> </ul> <p>With SyntheticTool, you can rapidly prototype data-driven workflows without building actual data sources, while maintaining full schema compliance and type safety through Pydantic or JSON Schema validation.</p>"},{"location":"user-guide/tools/function-call/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/tools/function-call/#function-design","title":"Function Design","text":"<ol> <li> <p>Clear Documentation: Always provide comprehensive docstrings for functions decorated with <code>@llm_function</code>. The LLM uses these descriptions to understand when and how to call your functions.</p> </li> <li> <p>Type Hints: Use proper type hints for all parameters. These are used to generate accurate JSON schemas for function specifications.</p> </li> <li> <p>Error Handling: Implement proper error handling in your functions, as exceptions will be propagated back through the tool invoke chain.</p> </li> </ol>"},{"location":"user-guide/tools/function-call/#tool-configuration","title":"Tool Configuration","text":"<ol> <li> <p>Builder Pattern: Use the builder pattern for complex tools that require multiple configuration options.</p> </li> <li> <p>Resource Management: For tools that use external APIs or resources, implement proper resource management and cleanup.</p> </li> <li> <p>Token Management: For tools that return large amounts of data, implement token or size limits to prevent overwhelming downstream components.</p> </li> </ol>"},{"location":"user-guide/tools/function-call/#integration","title":"Integration","text":"<ol> <li> <p>Command Registration: Tools that extend <code>FunctionCallTool</code> automatically use the <code>FunctionCallCommand</code> through the <code>@use_command</code> decorator.</p> </li> <li> <p>Observability: Leverage the built-in observability features by ensuring proper tool naming and type identification.</p> </li> <li> <p>Testing: Write comprehensive tests for your function implementations, as they will be called dynamically by LLMs.</p> </li> </ol> <p>With these patterns and examples, you can create robust, reusable function call tools that integrate seamlessly into Graphite's event-driven architecture while maintaining clean separation of concerns and excellent observability.</p>"},{"location":"user-guide/tools/function/","title":"FunctionTool","text":"<p>The FunctionTool class is a specialized <code>Tool</code> designed to invoke custom function-based operations within event-driven workflows. It provides a flexible framework for integrating arbitrary function logic, allowing developers to wrap any callable function and seamlessly integrate it into the workflow system. The tool handles both synchronous and asynchronous function execution while maintaining compatibility with the broader tool interface.</p>"},{"location":"user-guide/tools/function/#fields","title":"Fields","text":"Field Type Description <code>name</code> <code>str</code> Human-readable name identifying the function tool (defaults to <code>\"FunctionTool\"</code>). <code>type</code> <code>str</code> Specifies the type of the tool (set to <code>\"FunctionTool\"</code>). <code>function</code> <code>Callable[[Messages], OutputType]</code> The callable function that processes input messages and returns output data. <code>oi_span_type</code> <code>OpenInferenceSpanKindValues</code> Semantic attribute from OpenInference used for tracing (set to <code>TOOL</code>)."},{"location":"user-guide/tools/function/#methods","title":"Methods","text":"Method Signature Description <code>builder()</code> <code>classmethod -&gt; FunctionToolBuilder</code> Returns a builder instance for constructing FunctionTool objects. <code>invoke</code> <code>async (invoke_context: InvokeContext, input_data: Messages) -&gt; MsgsAGen</code> Asynchronously invokes the function, supporting both regular and await-able functions. <code>to_messages</code> <code>(response: OutputType) -&gt; Messages</code> Converts the function's raw response into standardized <code>Message</code> objects with appropriate formatting. <code>to_dict</code> <code>() -&gt; dict[str, Any]</code> Serializes the tool's configuration into a dictionary format for persistence or debugging."},{"location":"user-guide/tools/function/#usage-and-customization","title":"Usage and Customization","text":""},{"location":"user-guide/tools/function/#function-requirements","title":"Function Requirements","text":"<p>The <code>FunctionTool</code> can wrap any callable that accepts a <code>Messages</code> list as input and returns various output types. The function signature must be:</p> <pre><code>def your_function(messages: Messages) -&gt; OutputType\n</code></pre> <p>Where <code>OutputType</code> is defined as <code>Union[BaseModel, List[BaseModel]]</code> but the tool also supports additional types through its output handling.</p>"},{"location":"user-guide/tools/function/#output-handling","title":"Output Handling","text":"<p>The tool automatically handles different response types in its <code>to_messages</code> method:</p> <ul> <li><code>BaseModel</code> instances: Serialized to JSON using <code>model_dump_json()</code></li> <li>Lists of <code>BaseModel</code> objects: Converted to JSON arrays using <code>model_dump()</code> for each item</li> <li>String responses: Used directly as message content</li> <li>Other types: Encoded using <code>jsonpickle</code> for complex object serialization</li> </ul>"},{"location":"user-guide/tools/function/#async-support","title":"Async Support","text":"<p>The <code>invoke</code> method supports both synchronous and asynchronous functions:</p> <ul> <li>For regular functions: Executes the function normally</li> <li>For async functions: Automatically detects await-able responses using <code>inspect.isawaitable()</code> and awaits them</li> <li>Returns results as an async generator (<code>MsgsAGen</code>)</li> </ul>"},{"location":"user-guide/tools/function/#builder-pattern","title":"Builder Pattern","text":"<p>The <code>FunctionTool</code> uses a builder pattern for construction through the <code>FunctionToolBuilder</code> class:</p> <pre><code>function_tool = (\n    FunctionTool.builder()\n    .function(your_custom_function)\n    .build()\n)\n</code></pre>"},{"location":"user-guide/tools/function/#example-usage","title":"Example Usage","text":"<pre><code>from grafi.tools.functions.function_tool import FunctionTool\nfrom grafi.common.models.message import Messages\nfrom pydantic import BaseModel\n\nclass ProcessResult(BaseModel):\n    status: str\n    data: dict\n\ndef my_custom_function(messages: Messages) -&gt; ProcessResult:\n    # Process the input messages\n    content = messages[0].content if messages else \"\"\n\n    return ProcessResult(\n        status=\"processed\",\n        data={\"input_length\": len(content)}\n    )\n\n# Build the tool\nfunction_tool = (\n    FunctionTool.builder()\n    .function(my_custom_function)\n    .build()\n)\n</code></pre>"},{"location":"user-guide/tools/function/#decorators","title":"Decorators","text":"<p>The <code>FunctionTool</code> class uses the <code>@use_command(Command)</code> decorator, which enables integration with the command pattern for tool execution within the workflow system.</p> <p>By providing a consistent interface for function execution, the <code>FunctionTool</code> enables developers to integrate custom computational logic into event-driven workflows while maintaining clean separation between workflow orchestration and business logic implementation.</p>"},{"location":"user-guide/tools/llm/","title":"LLM (Base Class)","text":"<p>The <code>LLM</code> class is the abstract base class for all language model tools in Graphite. It provides a unified interface and common functionality for integrating various language model providers, including OpenAI, Ollama, and other LLM services. This base class handles function specifications, chat parameters, streaming configurations, and provides standardized patterns for LLM tool implementations.</p>"},{"location":"user-guide/tools/llm/#fields","title":"Fields","text":"Field Type Description <code>system_message</code> <code>Optional[str]</code> Optional system message to include in API calls (defaults to <code>None</code>). <code>oi_span_type</code> <code>OpenInferenceSpanKindValues</code> Semantic attribute for tracing (set to <code>LLM</code> for observability). <code>api_key</code> <code>Optional[str]</code> API key for the LLM service (defaults to <code>None</code>). <code>model</code> <code>str</code> The name of the LLM model to use (e.g., <code>'gpt-4o-mini'</code>, defaults to empty string). <code>chat_params</code> <code>Dict[str, Any]</code> Additional chat completion parameters specific to the LLM provider (defaults to empty dict). <code>is_streaming</code> <code>bool</code> Whether to enable streaming mode for responses (defaults to <code>False</code>). <code>structured_output</code> <code>bool</code> Whether the output is structured (e.g., JSON) or unstructured (e.g., plain text, defaults to <code>False</code>). <code>_function_specs</code> <code>FunctionSpecs</code> (private) Private attribute storing function specifications for function calling capabilities."},{"location":"user-guide/tools/llm/#methods","title":"Methods","text":"Method Signature Description <code>add_function_specs</code> <code>(function_spec: FunctionSpecs) -&gt; None</code> Adds function specifications to the LLM for function calling capabilities. <code>get_function_specs</code> <code>() -&gt; FunctionSpecs</code> Returns a copy of the function specifications currently registered with the LLM. <code>prepare_api_input</code> <code>(input_data: Messages) -&gt; Any</code> Abstract method that must be implemented by subclasses to prepare API input data. <code>to_dict</code> <code>() -&gt; dict[str, Any]</code> Serializes the LLM configuration into a dictionary format."},{"location":"user-guide/tools/llm/#builder-pattern","title":"Builder Pattern","text":"<p>The <code>LLM</code> class uses the <code>LLMBuilder</code> base class that provides common builder methods:</p> Builder Method Signature Description <code>model</code> <code>(model: str) -&gt; Self</code> Sets the model name for the LLM. <code>chat_params</code> <code>(params: Dict[str, Any]) -&gt; Self</code> Sets chat completion parameters. Automatically enables <code>structured_output</code> if <code>response_format</code> is present. <code>is_streaming</code> <code>(is_streaming: bool) -&gt; Self</code> Enables or disables streaming mode. <code>system_message</code> <code>(system_message: Optional[str]) -&gt; Self</code> Sets the system message to be included in API calls."},{"location":"user-guide/tools/llm/#usage-patterns","title":"Usage Patterns","text":""},{"location":"user-guide/tools/llm/#basic-llm-configuration","title":"Basic LLM Configuration","text":"<pre><code># Example with a concrete LLM implementation\nllm_tool = (\n    ConcreteLLMTool.builder()\n    .model(\"gpt-4o-mini\")\n    .system_message(\"You are a helpful assistant.\")\n    .chat_params({\"temperature\": 0.7, \"max_tokens\": 1000})\n    .build()\n)\n</code></pre>"},{"location":"user-guide/tools/llm/#function-calling-setup","title":"Function Calling Setup","text":"<pre><code>from grafi.common.models.function_spec import FunctionSpec\n\n# Create function specifications\nfunction_specs = [\n    FunctionSpec(\n        name=\"get_weather\",\n        description=\"Get current weather for a location\",\n        parameters={\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n            },\n            \"required\": [\"location\"]\n        }\n    )\n]\n\n# Add to LLM\nllm_tool.add_function_specs(function_specs)\n\n# Verify function specs are registered\nregistered_specs = llm_tool.get_function_specs()\nprint(f\"Registered {len(registered_specs)} function specifications\")\n</code></pre>"},{"location":"user-guide/tools/llm/#streaming-configuration","title":"Streaming Configuration","text":"<pre><code># Enable streaming for real-time responses\nstreaming_llm = (\n    ConcreteLLMTool.builder()\n    .model(\"gpt-4o\")\n    .is_streaming(True)\n    .build()\n)\n</code></pre>"},{"location":"user-guide/tools/llm/#structured-output-configuration","title":"Structured Output Configuration","text":"<pre><code># Configure for structured JSON output\nstructured_llm = (\n    ConcreteLLMTool.builder()\n    .model(\"gpt-4o\")\n    .chat_params({\n        \"response_format\": {\"type\": \"json_object\"},\n        \"temperature\": 0.1\n    })\n    .build()\n)\n# structured_output will automatically be set to True\n</code></pre>"},{"location":"user-guide/tools/llm/#function-specification-management","title":"Function Specification Management","text":"<p>The LLM base class provides built-in support for managing function specifications:</p>"},{"location":"user-guide/tools/llm/#adding-function-specifications","title":"Adding Function Specifications","text":"<pre><code># Single function spec\nfunction_spec = FunctionSpec(name=\"calculate\", description=\"Perform calculation\")\nllm_tool.add_function_specs([function_spec])\n\n# Multiple function specs\nfunction_specs = [spec1, spec2, spec3]\nllm_tool.add_function_specs(function_specs)\n\n# Empty specs are safely ignored\nllm_tool.add_function_specs([])  # No operation\nllm_tool.add_function_specs(None)  # No operation\n</code></pre>"},{"location":"user-guide/tools/llm/#retrieving-function-specifications","title":"Retrieving Function Specifications","text":"<pre><code># Get a copy of all registered function specs\nspecs = llm_tool.get_function_specs()\n\n# The returned list is a copy, so modifications don't affect the original\nspecs.append(new_spec)  # This won't affect the LLM's internal specs\n</code></pre>"},{"location":"user-guide/tools/llm/#chat-parameters","title":"Chat Parameters","text":"<p>Different LLM providers support various chat parameters. The <code>chat_params</code> field allows provider-specific customization:</p>"},{"location":"user-guide/tools/llm/#openai-parameters","title":"OpenAI Parameters","text":"<pre><code>openai_params = {\n    \"temperature\": 0.7,\n    \"max_tokens\": 1000,\n    \"top_p\": 0.9,\n    \"frequency_penalty\": 0.0,\n    \"presence_penalty\": 0.0,\n    \"response_format\": {\"type\": \"json_object\"}\n}\n</code></pre>"},{"location":"user-guide/tools/llm/#ollama-parameters","title":"Ollama Parameters","text":"<pre><code>ollama_params = {\n    \"temperature\": 0.7,\n    \"top_p\": 0.9,\n    \"top_k\": 40,\n    \"repeat_penalty\": 1.1\n}\n</code></pre>"},{"location":"user-guide/tools/llm/#observability-and-tracing","title":"Observability and Tracing","text":"<p>The LLM base class integrates with OpenInference for observability:</p> <ul> <li>Span Type: All LLM tools use <code>OpenInferenceSpanKindValues.LLM</code> for consistent tracing</li> <li>Command Integration: Uses <code>@use_command(LLMCommand)</code> for workflow integration</li> <li>Serialization: <code>to_dict()</code> method provides structured representation for logging and debugging</li> </ul>"},{"location":"user-guide/tools/llm/#implementation-requirements","title":"Implementation Requirements","text":"<p>When creating a concrete LLM implementation, you must:</p> <ol> <li>Inherit from LLM: Extend the <code>LLM</code> base class</li> <li>Implement <code>prepare_api_input</code>: Convert <code>Messages</code> to provider-specific format</li> <li>Implement <code>invoke</code>: Handle asynchronous API calls</li> <li>Implement response conversion: Convert provider responses back to <code>Messages</code></li> <li>Create a builder: Extend <code>LLMBuilder</code> with provider-specific configuration</li> </ol>"},{"location":"user-guide/tools/llm/#example-implementation-structure","title":"Example Implementation Structure","text":"<pre><code>from grafi.tools.llms.llm import LLM, LLMBuilder\n\nclass CustomLLM(LLM):\n    name: str = Field(default=\"CustomLLM\")\n    type: str = Field(default=\"CustomLLM\")\n\n    @classmethod\n    def builder(cls) -&gt; \"CustomLLMBuilder\":\n        return CustomLLMBuilder(cls)\n\n    def prepare_api_input(self, input_data: Messages) -&gt; Any:\n        # Convert Messages to provider-specific format\n        pass\n\n    async def invoke(self, invoke_context: InvokeContext, input_data: Messages) -&gt; MsgsAGen:\n        # Asynchronous API call implementation\n        pass\n\nclass CustomLLMBuilder(LLMBuilder[CustomLLM]):\n    def custom_param(self, value: str) -&gt; Self:\n        self.kwargs[\"custom_param\"] = value\n        return self\n</code></pre>"},{"location":"user-guide/tools/llm/#available-implementations","title":"Available Implementations","text":"<p>Graphite provides several concrete LLM implementations:</p> <ul> <li>OpenAITool: Integration with OpenAI's GPT models</li> <li>OllamaTool: Integration with local Ollama deployments</li> <li>Other providers: Additional implementations for various LLM services</li> </ul>"},{"location":"user-guide/tools/llm/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/tools/llm/#configuration","title":"Configuration","text":"<ol> <li>Use builders: Always use the builder pattern for clean, readable configuration</li> <li>Set system messages: Provide clear system instructions for consistent behavior</li> <li>Configure parameters: Tune temperature, max_tokens, and other parameters for your use case</li> </ol>"},{"location":"user-guide/tools/llm/#function-calling","title":"Function Calling","text":"<ol> <li>Clear descriptions: Provide detailed function descriptions for better LLM understanding</li> <li>Proper schemas: Use complete JSON schemas with required fields and descriptions</li> <li>Validation: Validate function specifications before adding them</li> </ol>"},{"location":"user-guide/tools/llm/#error-handling","title":"Error Handling","text":"<ol> <li>API key management: Handle missing or invalid API keys gracefully</li> <li>Network issues: Implement proper retry logic and timeout handling</li> <li>Rate limiting: Respect provider rate limits and implement back-off strategies</li> </ol>"},{"location":"user-guide/tools/llm/#performance","title":"Performance","text":"<ol> <li>Streaming: Use streaming for long responses to improve user experience</li> <li>Caching: Consider caching responses for repeated queries</li> <li>Batch processing: Group multiple requests when possible</li> </ol>"},{"location":"user-guide/tools/llm/#integration-with-graphite","title":"Integration with Graphite","text":"<p>The LLM base class integrates seamlessly with Graphite's architecture:</p> <ul> <li>Tool Interface: Implements the standard <code>Tool</code> interface</li> <li>Event-Driven: Works with Graphite's event-driven workflow system</li> <li>Observability: Built-in tracing and monitoring capabilities</li> <li>Command Pattern: Uses <code>LLMCommand</code> for consistent workflow integration</li> </ul> <p>By leveraging the LLM base class, you can create powerful, consistent, and observable language model integrations that work seamlessly within Graphite's event-driven architecture.</p>"},{"location":"user-guide/tools/ollama/","title":"OllamaTool","text":"<p><code>OllamaTool</code> is an implementation of the <code>LLM</code> interface designed to interface with Ollama's language model API. It supports asynchronous interaction patterns, streaming responses, and function calling, converting workflow <code>Message</code> objects into an Ollama-compatible format and translating API responses back into the workflow.</p>"},{"location":"user-guide/tools/ollama/#fields","title":"Fields","text":"Field Type Description <code>name</code> <code>str</code> Descriptive identifier for the tool (defaults to <code>\"OllamaTool\"</code>). <code>type</code> <code>str</code> Tool type indicator (defaults to <code>\"OllamaTool\"</code>). <code>api_url</code> <code>str</code> URL of the Ollama API endpoint (defaults to <code>\"http://localhost:11434\"</code>). <code>model</code> <code>str</code> Ollama model name (defaults to <code>\"qwen3\"</code>). <code>system_message</code> <code>Optional[str]</code> Optional system message to include in API calls (inherited from <code>LLM</code>). <code>chat_params</code> <code>Dict[str, Any]</code> Additional optional chat completion parameters (inherited from <code>LLM</code>). <code>is_streaming</code> <code>bool</code> Whether to enable streaming mode for responses (defaults to <code>False</code>, inherited from <code>LLM</code>). <code>structured_output</code> <code>bool</code> Whether to use structured output mode (defaults to <code>False</code>, inherited from <code>LLM</code>). <code>oi_span_type</code> <code>OpenInferenceSpanKindValues</code> Semantic attribute for tracing (set to <code>LLM</code>, inherited from <code>LLM</code>)."},{"location":"user-guide/tools/ollama/#methods","title":"Methods","text":"Method Signature Description <code>builder()</code> <code>classmethod -&gt; OllamaToolBuilder</code> Returns a builder instance for constructing OllamaTool objects. <code>prepare_api_input</code> <code>(input_data: Messages) -&gt; tuple[List[Dict[str, Any]], Optional[List[Dict[str, Any]]]]</code> Adapts Message objects to Ollama API format, including function specifications. <code>invoke</code> <code>async (invoke_context: InvokeContext, input_data: Messages) -&gt; MsgsAGen</code> Asynchronously calls the Ollama API, supporting both streaming and non-streaming modes. <code>to_stream_messages</code> <code>(chunk: ChatResponse \\| dict[str, Any]) -&gt; Messages</code> Converts streaming response chunks from Ollama's API into Message objects. <code>to_messages</code> <code>(response: ChatResponse) -&gt; Messages</code> Converts a complete response from Ollama's API into Message objects. <code>to_dict</code> <code>() -&gt; dict[str, Any]</code> Provides a dictionary representation of the OllamaTool configuration."},{"location":"user-guide/tools/ollama/#how-it-works","title":"How It Works","text":"<p>The Ollama tool processes requests through several key steps:</p> <ol> <li>Input Preparation: The <code>prepare_api_input</code> method converts incoming <code>Message</code> objects into the format expected by Ollama's API:</li> <li>Adds system message if configured</li> <li>Converts message fields, mapping <code>function</code> role to <code>tool</code> role for Ollama compatibility</li> <li>Extracts function specifications and converts them to Ollama tool format</li> <li> <p>Handles function calls by converting them to tool_calls format</p> </li> <li> <p>API Invocation:</p> </li> <li>Asynchronous (<code>invoke</code>): Uses AsyncClient for concurrent operations</li> <li>Streaming: When <code>is_streaming=True</code>, processes responses incrementally</li> <li> <p>Function Calling: Supports tool/function calling through Ollama's API</p> </li> <li> <p>Response Processing:</p> </li> <li>Streaming responses: Processed chunk by chunk via <code>to_stream_messages</code></li> <li>Complete responses: Processed via <code>to_messages</code></li> <li>Function calls: Converted to proper tool_call format with generated IDs</li> <li>All responses are converted to standardized <code>Message</code> objects</li> </ol>"},{"location":"user-guide/tools/ollama/#builder-pattern","title":"Builder Pattern","text":"<p>The <code>OllamaTool</code> uses a builder pattern through the <code>OllamaToolBuilder</code> class:</p> <pre><code>ollama_tool = (\n    OllamaTool.builder()\n    .api_url(\"http://localhost:11434\")\n    .model(\"llama3.2\")\n    .system_message(\"You are a helpful assistant.\")\n    .chat_params({\"temperature\": 0.7})\n    .is_streaming(True)\n    .build()\n)\n</code></pre>"},{"location":"user-guide/tools/ollama/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/tools/ollama/#basic-usage","title":"Basic Usage","text":"<pre><code>from grafi.tools.llms.impl.ollama_tool import OllamaTool\nfrom grafi.common.models.message import Message\n\n# Create the tool\nollama_tool = (\n    OllamaTool.builder()\n    .api_url(\"http://localhost:11434\")\n    .model(\"llama3.2\")\n    .build()\n)\n\n# Create input messages\nmessages = [Message(role=\"user\", content=\"Hello, how are you?\")]\n\n# Asynchronous invocation\nasync for response in ollama_tool.invoke(invoke_context, messages):\n    print(response[0].content)\n</code></pre>"},{"location":"user-guide/tools/ollama/#streaming-usage","title":"Streaming Usage","text":"<pre><code># Enable streaming\nollama_tool = (\n    OllamaTool.builder()\n    .api_url(\"http://localhost:11434\")\n    .model(\"qwen3\")\n    .is_streaming(True)\n    .build()\n)\n\n# Asynchronous streaming\nasync def stream_example():\n    messages = [Message(role=\"user\", content=\"Tell me a story\")]\n    async for message_batch in ollama_tool.invoke(invoke_context, messages):\n        for message in message_batch:\n            if message.content:\n                print(message.content, end=\"\", flush=True)\n</code></pre>"},{"location":"user-guide/tools/ollama/#function-calling","title":"Function Calling","text":"<pre><code>from grafi.common.models.function_spec import FunctionSpec\n\n# Add function specifications\nfunction_spec = FunctionSpec(\n    name=\"get_weather\",\n    description=\"Get current weather for a location\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n        },\n        \"required\": [\"location\"]\n    }\n)\n\nollama_tool.add_function_specs([function_spec])\n\n# The tool will now include function specifications in API calls\n</code></pre>"},{"location":"user-guide/tools/ollama/#configuration-options","title":"Configuration Options","text":""},{"location":"user-guide/tools/ollama/#api-configuration","title":"API Configuration","text":"<ul> <li><code>api_url</code>: Customize the Ollama server endpoint (default: <code>http://localhost:11434</code>)</li> <li><code>model</code>: Specify the Ollama model to use (default: <code>qwen3</code>)</li> </ul>"},{"location":"user-guide/tools/ollama/#common-models","title":"Common Models","text":"<p>Popular Ollama models you can use:</p> <ul> <li><code>llama3.2</code> - Meta's Llama 3.2 model</li> <li><code>qwen3</code> - Alibaba's Qwen3 model  </li> <li><code>mistral</code> - Mistral AI's model</li> <li><code>codellama</code> - Code-specialized Llama model</li> <li><code>phi3</code> - Microsoft's Phi-3 model</li> </ul>"},{"location":"user-guide/tools/ollama/#chat-parameters","title":"Chat Parameters","text":"<p>You can include additional parameters in <code>chat_params</code>:</p> <pre><code>chat_params = {\n    \"temperature\": 0.7,        # Creativity level\n    \"top_p\": 0.9,              # Nucleus sampling\n    \"top_k\": 40,               # Top-k sampling\n    \"repeat_penalty\": 1.1,     # Penalty for repetition\n}\n</code></pre>"},{"location":"user-guide/tools/ollama/#streaming-response-handling","title":"Streaming Response Handling","text":"<p>The <code>to_stream_messages</code> method handles both <code>ChatResponse</code> objects and plain dictionaries:</p> <ul> <li>Empty content filtering: Skips empty deltas to avoid blank messages</li> <li>Role mapping: Ensures proper role assignment with fallback to \"assistant\"</li> <li>Streaming flag: Sets <code>is_streaming=True</code> on streamed messages</li> <li>Incremental content: Provides delta content for real-time display</li> </ul>"},{"location":"user-guide/tools/ollama/#function-call-support","title":"Function Call Support","text":"<p>Ollama tool provides comprehensive function calling support:</p> <ul> <li>Input mapping: Converts <code>function_call</code> messages to Ollama's <code>tool_calls</code> format</li> <li>Role conversion: Maps <code>function</code> role to <code>tool</code> role for Ollama compatibility</li> <li>Tool specifications: Converts function specs to Ollama tool format</li> <li>Response handling: Generates proper tool_call IDs and formats responses</li> </ul>"},{"location":"user-guide/tools/ollama/#error-handling","title":"Error Handling","text":"<p>The Ollama tool provides robust error handling:</p> <ul> <li>Import errors: Clear error message if <code>ollama</code> package is not installed</li> <li>API errors: Ollama-specific errors are caught and re-raised as <code>RuntimeError</code> with descriptive messages</li> <li>Connection issues: Network and server connectivity issues are properly handled</li> </ul>"},{"location":"user-guide/tools/ollama/#installation-requirements","title":"Installation Requirements","text":"<p>The OllamaTool requires the <code>ollama</code> package:</p> <pre><code>pip install ollama\n</code></pre>"},{"location":"user-guide/tools/ollama/#integration","title":"Integration","text":"<p>The <code>OllamaTool</code> integrates seamlessly with Graphite's architecture:</p> <ul> <li>Command Pattern: Uses <code>@use_command(LLMCommand)</code> for workflow integration</li> <li>Observability: Includes OpenInference tracing with <code>LLM</code> span type</li> <li>Decorators: Uses <code>@record_tool_invoke</code> for monitoring</li> <li>Local Deployment: Perfect for local or on-premises LLM deployments</li> </ul> <p>By leveraging <code>OllamaTool</code> in your workflows, you can utilize powerful local language models through Ollama while maintaining consistency with Graphite's tool architecture and event-driven patterns. This enables privacy-focused, locally-hosted AI capabilities without external API dependencies.</p>"},{"location":"user-guide/tools/openai/","title":"OpenAITool","text":"<p><code>OpenAITool</code> is a concrete implementation of the <code>LLM</code> interface, integrating directly with OpenAI's language model APIs. It supports asynchronous interactions and streaming responses for real-time applications.</p>"},{"location":"user-guide/tools/openai/#fields","title":"Fields","text":"Field Type Description <code>name</code> <code>str</code> Name of the tool (defaults to <code>\"OpenAITool\"</code>). <code>type</code> <code>str</code> Type indicator for this tool (defaults to <code>\"OpenAITool\"</code>). <code>api_key</code> <code>Optional[str]</code> API key required to authenticate with OpenAI's services (defaults to <code>OPENAI_API_KEY</code> environment variable). <code>model</code> <code>str</code> Model name used for OpenAI API calls (defaults to <code>\"gpt-4o-mini\"</code>). <code>system_message</code> <code>Optional[str]</code> Optional system message to include in API calls (inherited from <code>LLM</code>). <code>chat_params</code> <code>Dict[str, Any]</code> Additional optional chat completion parameters (inherited from <code>LLM</code>). <code>is_streaming</code> <code>bool</code> Whether to enable streaming mode for responses (defaults to <code>False</code>, inherited from <code>LLM</code>). <code>structured_output</code> <code>bool</code> Whether to use structured output mode via OpenAI's beta API (defaults to <code>False</code>, inherited from <code>LLM</code>). <code>oi_span_type</code> <code>OpenInferenceSpanKindValues</code> Semantic attribute for tracing (set to <code>LLM</code>, inherited from <code>LLM</code>)."},{"location":"user-guide/tools/openai/#methods","title":"Methods","text":"Method Signature Description <code>builder()</code> <code>classmethod -&gt; OpenAIToolBuilder</code> Returns a builder instance for constructing OpenAITool objects. <code>prepare_api_input</code> <code>(input_data: Messages) -&gt; tuple[List[ChatCompletionMessageParam], Union[List[ChatCompletionToolParam], NotGiven]]</code> Adapts Message objects to OpenAI API format, including function specifications. <code>invoke</code> <code>async (invoke_context: InvokeContext, input_data: Messages) -&gt; MsgsAGen</code> Asynchronously calls the OpenAI API, supporting both streaming and non-streaming modes. <code>to_stream_messages</code> <code>(chunk: ChatCompletionChunk) -&gt; Messages</code> Converts streaming response chunks from OpenAI's API into Message objects. <code>to_messages</code> <code>(response: ChatCompletion) -&gt; Messages</code> Converts a complete response from OpenAI's API into Message objects. <code>to_dict</code> <code>() -&gt; Dict[str, Any]</code> Serializes OpenAITool configuration, masking the API key for security."},{"location":"user-guide/tools/openai/#how-it-works","title":"How It Works","text":"<p>The OpenAI tool processes requests through several key steps:</p> <ol> <li>Input Preparation: The <code>prepare_api_input</code> method converts incoming <code>Message</code> objects into the format expected by OpenAI's API:</li> <li>Adds system message if configured</li> <li>Converts message fields (role, content, tool_calls, etc.) to OpenAI format</li> <li> <p>Extracts function specifications and converts them to OpenAI tool format</p> </li> <li> <p>API Invocation:</p> </li> <li>Asynchronous (<code>invoke</code>): Uses AsyncClient for concurrent operations</li> <li>Streaming: When <code>is_streaming=True</code>, processes responses incrementally</li> <li> <p>Structured Output: When <code>structured_output=True</code>, uses OpenAI's beta parsing API</p> </li> <li> <p>Response Processing:</p> </li> <li>Streaming responses: Processed chunk by chunk via <code>to_stream_messages</code></li> <li>Complete responses: Processed via <code>to_messages</code></li> <li>All responses are converted to standardized <code>Message</code> objects</li> </ol>"},{"location":"user-guide/tools/openai/#builder-pattern","title":"Builder Pattern","text":"<p>The <code>OpenAITool</code> uses a builder pattern through the <code>OpenAIToolBuilder</code> class:</p> <pre><code>openai_tool = (\n    OpenAITool.builder()\n    .api_key(\"your-api-key\")\n    .model(\"gpt-4o\")\n    .system_message(\"You are a helpful assistant.\")\n    .chat_params({\"temperature\": 0.7, \"max_tokens\": 1000})\n    .is_streaming(True)\n    .build()\n)\n</code></pre>"},{"location":"user-guide/tools/openai/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/tools/openai/#basic-usage","title":"Basic Usage","text":"<pre><code>from grafi.tools.llms.impl.openai_tool import OpenAITool\nfrom grafi.common.models.message import Message\n\n# Create the tool\nopenai_tool = (\n    OpenAITool.builder()\n    .api_key(\"your-openai-api-key\")\n    .model(\"gpt-4o-mini\")\n    .build()\n)\n\n# Create input messages\nmessages = [Message(role=\"user\", content=\"Hello, how are you?\")]\n\n# Asynchronous invocation\nasync for response in openai_tool.invoke(invoke_context, messages):\n    print(response[0].content)\n</code></pre>"},{"location":"user-guide/tools/openai/#streaming-usage","title":"Streaming Usage","text":"<pre><code># Enable streaming\nopenai_tool = (\n    OpenAITool.builder()\n    .api_key(\"your-openai-api-key\")\n    .model(\"gpt-4o\")\n    .is_streaming(True)\n    .build()\n)\n\n# Asynchronous streaming\nasync def stream_example():\n    messages = [Message(role=\"user\", content=\"Tell me a story\")]\n    async for message_batch in openai_tool.invoke(invoke_context, messages):\n        for message in message_batch:\n            if message.content:\n                print(message.content, end=\"\", flush=True)\n</code></pre>"},{"location":"user-guide/tools/openai/#function-calling","title":"Function Calling","text":"<pre><code>from grafi.common.models.function_spec import FunctionSpec\n\n# Add function specifications\nfunction_spec = FunctionSpec(\n    name=\"get_weather\",\n    description=\"Get current weather for a location\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n        },\n        \"required\": [\"location\"]\n    }\n)\n\nopenai_tool.add_function_specs([function_spec])\n\n# The tool will now include function specifications in API calls\n</code></pre>"},{"location":"user-guide/tools/openai/#configuration-options","title":"Configuration Options","text":""},{"location":"user-guide/tools/openai/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>OPENAI_API_KEY</code>: Default API key if not explicitly provided</li> </ul>"},{"location":"user-guide/tools/openai/#chat-parameters","title":"Chat Parameters","text":"<p>Common chat completion parameters you can include in <code>chat_params</code>:</p> <pre><code>chat_params = {\n    \"temperature\": 0.7,        # Creativity level (0.0-2.0)\n    \"max_tokens\": 1000,        # Maximum response length\n    \"top_p\": 0.9,              # Nucleus sampling\n    \"frequency_penalty\": 0.0,  # Penalty for repeated tokens\n    \"presence_penalty\": 0.0,   # Penalty for new topics\n    \"response_format\": {\"type\": \"json_object\"}  # For structured output\n}\n</code></pre>"},{"location":"user-guide/tools/openai/#structured-output","title":"Structured Output","text":"<p>When using structured output mode:</p> <pre><code>openai_tool = (\n    OpenAITool.builder()\n    .chat_params({\"response_format\": {\"type\": \"json_object\"}})\n    .build()\n)\n# structured_output will automatically be set to True\n</code></pre>"},{"location":"user-guide/tools/openai/#error-handling","title":"Error Handling","text":"<p>The OpenAI tool provides robust error handling:</p> <ul> <li>API Errors: OpenAI-specific errors are caught and re-raised as <code>RuntimeError</code> with descriptive messages</li> <li>Cancellation: Async operations properly handle <code>asyncio.CancelledError</code></li> <li>Network Issues: Connection and timeout issues are wrapped in appropriate exceptions</li> </ul>"},{"location":"user-guide/tools/openai/#integration","title":"Integration","text":"<p>The <code>OpenAITool</code> integrates seamlessly with Graphite's architecture:</p> <ul> <li>Command Pattern: Uses <code>@use_command(LLMCommand)</code> for workflow integration</li> <li>Observability: Includes OpenInference tracing with <code>LLM</code> span type</li> <li>Decorators: Uses <code>@record_tool_invoke</code> for monitoring</li> </ul> <p>By leveraging <code>OpenAITool</code> in your workflows, you gain access to OpenAI's powerful language models while maintaining consistency with Graphite's tool architecture and event-driven patterns.</p>"},{"location":"user-guide/tools/tool/","title":"Tool (Base Class)","text":"<p>The <code>Tool</code> class is the fundamental base class for all tools in the Graphite framework. It defines the core interface and common functionality that all tools must implement, providing a unified approach to tool development, execution, and integration within event-driven workflows. This abstract base class ensures consistency across different tool types while enabling powerful extensibility.</p>"},{"location":"user-guide/tools/tool/#fields","title":"Fields","text":"Field Type Description <code>tool_id</code> <code>str</code> Unique identifier for the tool instance (automatically generated using <code>default_id</code>). <code>name</code> <code>Optional[str]</code> Human-readable name for the tool (defaults to <code>None</code>). <code>type</code> <code>Optional[str]</code> Type identifier for the tool, typically the class name (defaults to <code>None</code>). <code>oi_span_type</code> <code>OpenInferenceSpanKindValues</code> Semantic attribute for observability tracing (required field for OpenInference integration)."},{"location":"user-guide/tools/tool/#methods","title":"Methods","text":"Method Signature Description <code>invoke</code> <code>async (invoke_context: InvokeContext, input_data: Messages) -&gt; MsgsAGen</code> Asynchronous method to process input data and yield response messages via async generator. <code>to_messages</code> <code>(response: Any) -&gt; Messages</code> Converts tool-specific response data into standardized Message objects. <code>to_dict</code> <code>() -&gt; Dict[str, Any]</code> Serializes the tool instance into a dictionary representation."},{"location":"user-guide/tools/tool/#builder-pattern","title":"Builder Pattern","text":"<p>The <code>Tool</code> class uses the <code>ToolBuilder</code> base class that provides common builder methods:</p> Builder Method Signature Description <code>name</code> <code>(name: str) -&gt; Self</code> Sets the human-readable name for the tool. <code>type</code> <code>(type_name: str) -&gt; Self</code> Sets the type identifier for the tool. <code>oi_span_type</code> <code>(oi_span_type: OpenInferenceSpanKindValues) -&gt; Self</code> Sets the OpenInference span type for observability."},{"location":"user-guide/tools/tool/#core-concepts","title":"Core Concepts","text":""},{"location":"user-guide/tools/tool/#tool-identification","title":"Tool Identification","text":"<p>Every tool instance has a unique identifier and optional metadata:</p> <pre><code># Automatic ID generation\ntool = SomeTool()\nprint(tool.tool_id)  # Automatically generated unique ID\n\n# Manual configuration via builder\ntool = (\n    SomeTool.builder()\n    .name(\"My Custom Tool\")\n    .type(\"CustomTool\")\n    .build()\n)\n</code></pre>"},{"location":"user-guide/tools/tool/#execution-pattern","title":"Execution Pattern","text":"<p>Tools use asynchronous execution patterns:</p>"},{"location":"user-guide/tools/tool/#asynchronous-execution","title":"Asynchronous Execution","text":"<pre><code># Asynchronous processing with generator\nasync for message_batch in tool.invoke(invoke_context, input_messages):\n    process_messages(message_batch)\n</code></pre>"},{"location":"user-guide/tools/tool/#message-conversion","title":"Message Conversion","text":"<p>All tools must handle conversion between internal data formats and standard Message objects:</p> <pre><code># Convert tool-specific response to Messages\ndef to_messages(self, response: Any) -&gt; Messages:\n    # Implementation-specific conversion logic\n    return [Message(role=\"assistant\", content=str(response))]\n</code></pre>"},{"location":"user-guide/tools/tool/#implementation-requirements","title":"Implementation Requirements","text":"<p>When creating a concrete tool implementation, you must:</p>"},{"location":"user-guide/tools/tool/#1-inherit-from-tool","title":"1. Inherit from Tool","text":"<pre><code>from grafi.tools.tool import Tool\n\nclass MyCustomTool(Tool):\n    # Tool-specific fields\n    oi_span_type: OpenInferenceSpanKindValues = OpenInferenceSpanKindValues.TOOL\n</code></pre>"},{"location":"user-guide/tools/tool/#2-implement-core-methods","title":"2. Implement Core Methods","text":"<pre><code>async def invoke(self, invoke_context: InvokeContext, input_data: Messages) -&gt; MsgsAGen:\n    \"\"\"Asynchronous processing implementation.\"\"\"\n    # Process input_data asynchronously\n    result = await self.async_process_data(input_data)\n    yield self.to_messages(result)\n\ndef to_messages(self, response: Any) -&gt; Messages:\n    \"\"\"Convert response to Messages.\"\"\"\n    return [Message(role=\"tool\", content=response)]\n</code></pre>"},{"location":"user-guide/tools/tool/#3-create-builder-class","title":"3. Create Builder Class","text":"<pre><code>from grafi.tools.tool import ToolBuilder\n\nclass MyCustomToolBuilder(ToolBuilder[MyCustomTool]):\n    \"\"\"Builder for MyCustomTool instances.\"\"\"\n\n    def custom_parameter(self, value: str) -&gt; Self:\n        self.kwargs[\"custom_parameter\"] = value\n        return self\n</code></pre>"},{"location":"user-guide/tools/tool/#4-add-builder-factory-method","title":"4. Add Builder Factory Method","text":"<pre><code>@classmethod\ndef builder(cls) -&gt; \"MyCustomToolBuilder\":\n    \"\"\"Return a builder for MyCustomTool.\"\"\"\n    return MyCustomToolBuilder(cls)\n</code></pre>"},{"location":"user-guide/tools/tool/#complete-implementation-example","title":"Complete Implementation Example","text":"<p>Here's a complete example of a custom tool implementation:</p> <pre><code>from typing import Any, Self\nfrom pydantic import Field\nfrom openinference.semconv.trace import OpenInferenceSpanKindValues\n\nfrom grafi.tools.tool import Tool, ToolBuilder\nfrom grafi.common.models.invoke_context import InvokeContext\nfrom grafi.common.models.message import Message, Messages, MsgsAGen\n\nclass TextProcessorTool(Tool):\n    \"\"\"A tool for processing text data.\"\"\"\n\n    name: str = Field(default=\"TextProcessorTool\")\n    type: str = Field(default=\"TextProcessorTool\")\n    oi_span_type: OpenInferenceSpanKindValues = OpenInferenceSpanKindValues.TOOL\n\n    # Tool-specific configuration\n    operation: str = Field(default=\"uppercase\")\n    prefix: str = Field(default=\"\")\n\n    @classmethod\n    def builder(cls) -&gt; \"TextProcessorToolBuilder\":\n        return TextProcessorToolBuilder(cls)\n\n    async def invoke(self, invoke_context: InvokeContext, input_data: Messages) -&gt; MsgsAGen:\n        \"\"\"Process text asynchronously.\"\"\"\n        if not input_data:\n            yield []\n            return\n\n        content = input_data[0].content or \"\"\n        processed = self._process_text(content)\n        yield self.to_messages(processed)\n\n    def _process_text(self, text: str) -&gt; str:\n        \"\"\"Internal text processing logic.\"\"\"\n        if self.operation == \"uppercase\":\n            result = text.upper()\n        elif self.operation == \"lowercase\":\n            result = text.lower()\n        else:\n            result = text\n\n        return f\"{self.prefix}{result}\"\n\n    def to_messages(self, response: str) -&gt; Messages:\n        \"\"\"Convert processed text to Messages.\"\"\"\n        return [Message(role=\"assistant\", content=response)]\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Serialize tool configuration.\"\"\"\n        return {\n            **super().to_dict(),\n            \"name\": self.name,\n            \"type\": self.type,\n            \"operation\": self.operation,\n            \"prefix\": self.prefix,\n        }\n\nclass TextProcessorToolBuilder(ToolBuilder[TextProcessorTool]):\n    \"\"\"Builder for TextProcessorTool instances.\"\"\"\n\n    def operation(self, operation: str) -&gt; Self:\n        self.kwargs[\"operation\"] = operation\n        return self\n\n    def prefix(self, prefix: str) -&gt; Self:\n        self.kwargs[\"prefix\"] = prefix\n        return self\n\n# Usage example\ntext_tool = (\n    TextProcessorTool.builder()\n    .name(\"Text Processor\")\n    .operation(\"uppercase\")\n    .prefix(\"PROCESSED: \")\n    .build()\n)\n</code></pre>"},{"location":"user-guide/tools/tool/#openinference-integration","title":"OpenInference Integration","text":"<p>All tools integrate with OpenInference for observability and tracing:</p>"},{"location":"user-guide/tools/tool/#span-types","title":"Span Types","text":"<p>Common span types used by different tool categories:</p> <pre><code>from openinference.semconv.trace import OpenInferenceSpanKindValues\n\n# General tools\noi_span_type = OpenInferenceSpanKindValues.TOOL\n\n# Language model tools\noi_span_type = OpenInferenceSpanKindValues.LLM\n\n# Retrieval tools\noi_span_type = OpenInferenceSpanKindValues.RETRIEVER\n\n# Embedding tools\noi_span_type = OpenInferenceSpanKindValues.EMBEDDING\n</code></pre>"},{"location":"user-guide/tools/tool/#tracing-configuration","title":"Tracing Configuration","text":"<pre><code>class MyTool(Tool):\n    oi_span_type: OpenInferenceSpanKindValues = OpenInferenceSpanKindValues.TOOL\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        return {\n            **super().to_dict(),\n            \"oi_span_type\": self.oi_span_type.value,\n            # Other fields...\n        }\n</code></pre>"},{"location":"user-guide/tools/tool/#tool-categories","title":"Tool Categories","text":"<p>Graphite provides several categories of tools that extend the base Tool class:</p>"},{"location":"user-guide/tools/tool/#llm-tools","title":"LLM Tools","text":"<ul> <li>OpenAITool: Integration with OpenAI's language models</li> <li>OllamaTool: Integration with local Ollama deployments</li> <li>Base LLM class: Abstract base for language model tools</li> </ul>"},{"location":"user-guide/tools/tool/#function-tools","title":"Function Tools","text":"<ul> <li>FunctionTool: Wrapper for custom Python functions</li> <li>FunctionCallTool: Enables LLM function calling capabilities</li> </ul>"},{"location":"user-guide/tools/tool/#specialized-tools","title":"Specialized Tools","text":"<ul> <li>Assistant Tools: High-level AI assistant implementations</li> <li>Retrieval Tools: Document and information retrieval</li> <li>Workflow Tools: Orchestration and flow control</li> </ul>"},{"location":"user-guide/tools/tool/#configuration-and-pydantic-integration","title":"Configuration and Pydantic Integration","text":"<p>Tools are built on Pydantic BaseModel, providing:</p>"},{"location":"user-guide/tools/tool/#type-safety","title":"Type Safety","text":"<pre><code>class MyTool(Tool):\n    # Pydantic field validation\n    max_tokens: int = Field(gt=0, le=4000, default=1000)\n    temperature: float = Field(ge=0.0, le=2.0, default=0.7)\n</code></pre>"},{"location":"user-guide/tools/tool/#configuration","title":"Configuration","text":"<pre><code>class MyTool(Tool):\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,  # Allow complex types\n        validate_assignment=True,      # Validate on assignment\n        extra=\"forbid\"                 # Strict field validation\n    )\n</code></pre>"},{"location":"user-guide/tools/tool/#error-handling-best-practices","title":"Error Handling Best Practices","text":""},{"location":"user-guide/tools/tool/#async-error-handling","title":"Async Error Handling","text":"<pre><code>async def invoke(self, invoke_context: InvokeContext, input_data: Messages) -&gt; MsgsAGen:\n    try:\n        result = await self.async_process(input_data)\n        yield self.to_messages(result)\n    except Exception as e:\n        logger.error(f\"Async tool {self.name} failed: {e}\")\n        yield [Message(role=\"assistant\", content=f\"Error: {str(e)}\")]\n</code></pre>"},{"location":"user-guide/tools/tool/#testing-tools","title":"Testing Tools","text":""},{"location":"user-guide/tools/tool/#unit-testing","title":"Unit Testing","text":"<pre><code>import pytest\nfrom grafi.common.models.message import Message\nfrom grafi.common.models.invoke_context import InvokeContext\n\n@pytest.mark.asyncio\nasync def test_tool_invoke():\n    tool = MyCustomTool()\n    context = InvokeContext()\n    input_messages = [Message(role=\"user\", content=\"test input\")]\n\n    results = []\n    async for batch in tool.invoke(context, input_messages):\n        results.extend(batch)\n\n    assert len(results) &gt; 0\n</code></pre>"},{"location":"user-guide/tools/tool/#integration-with-graphite","title":"Integration with Graphite","text":""},{"location":"user-guide/tools/tool/#command-pattern","title":"Command Pattern","text":"<p>Tools integrate with Graphite's command system:</p> <pre><code>from grafi.common.models.command import use_command\nfrom grafi.tools.tool_command import ToolCommand\n\n@use_command(ToolCommand)\nclass MyTool(Tool):\n    # Tool implementation\n    pass\n</code></pre>"},{"location":"user-guide/tools/tool/#event-driven-workflows","title":"Event-Driven Workflows","text":"<p>Tools participate in event-driven workflows through:</p> <ul> <li>InvokeContext: Carries workflow context and metadata</li> <li>Message passing: Standardized communication via Messages</li> <li>Async generators: Support for streaming and real-time processing</li> </ul>"},{"location":"user-guide/tools/tool/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/tools/tool/#design-principles","title":"Design Principles","text":"<ol> <li>Single Responsibility: Each tool should have a clear, focused purpose</li> <li>Immutability: Prefer immutable configurations where possible</li> <li>Error Resilience: Handle errors gracefully and provide meaningful feedback</li> <li>Observable: Use proper OpenInference span types for tracing</li> </ol>"},{"location":"user-guide/tools/tool/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Async Support: Implement both sync and async methods for flexibility</li> <li>Resource Management: Clean up resources in finally blocks or context managers</li> <li>Batching: Process multiple messages efficiently when possible</li> <li>Caching: Cache expensive operations when appropriate</li> </ol>"},{"location":"user-guide/tools/tool/#documentation","title":"Documentation","text":"<ol> <li>Clear Docstrings: Document purpose, parameters, and return values</li> <li>Type Hints: Use comprehensive type annotations</li> <li>Examples: Provide usage examples in docstrings</li> <li>Error Cases: Document expected exceptions and error conditions</li> </ol> <p>By following the Tool base class pattern, you can create powerful, consistent, and observable tools that integrate seamlessly with Graphite's event-driven architecture while maintaining clean separation of concerns and excellent developer experience.</p>"},{"location":"user-guide/topics/input_topics/","title":"Input Topics","text":"<p>The Graphite input topic system provides specialized topic implementations for handling workflow inputs, including initial user inputs and human-in-the-loop interactions. These topics serve as entry points for data flowing into the workflow system.</p>"},{"location":"user-guide/topics/input_topics/#overview","title":"Overview","text":"<p>The input topic system includes:</p> <ul> <li>InputTopic: Handles initial agent input from users or external systems</li> <li>InWorkflowInputTopic: Manages input within workflows, particularly for human-in-the-loop scenarios</li> <li>Event Publishing: Converts input data into publishable topic events</li> <li>Workflow Integration: Seamlessly integrates with the event-driven workflow system</li> </ul>"},{"location":"user-guide/topics/input_topics/#core-components","title":"Core Components","text":""},{"location":"user-guide/topics/input_topics/#inputtopic","title":"InputTopic","text":"<p>The foundational topic for receiving initial input into a workflow.</p>"},{"location":"user-guide/topics/input_topics/#inputtopic-fields","title":"InputTopic Fields","text":"Field Type Description <code>name</code> <code>str</code> Topic name (typically \"agent_input_topic\") <code>type</code> <code>str</code> Topic type identifier (\"AgentInput\") <code>condition</code> <code>Callable[[Messages], bool]</code> Function to filter publishable messages <code>event_cache</code> <code>TopicEventQueue</code> Manages event storage and consumer offsets <p>Inherits all fields from <code>TopicBase</code></p>"},{"location":"user-guide/topics/input_topics/#inputtopic-methods","title":"InputTopic Methods","text":"Method Signature Description <code>builder</code> <code>() -&gt; InputTopicBuilder</code> Class method returning builder instance <code>publish_data</code> <code>(invoke_context, publisher_name, publisher_type, data, consumed_events) -&gt; PublishToTopicEvent</code> Publish input messages to the topic <code>publish_data</code> <code>(invoke_context, publisher_name, publisher_type, data, consumed_events) -&gt; PublishToTopicEvent</code> Async version of publish_data"},{"location":"user-guide/topics/input_topics/#inworkflowinputtopic","title":"InWorkflowInputTopic","text":"<p>A specialized input topic for managing human-in-the-loop interactions within running workflows.</p>"},{"location":"user-guide/topics/input_topics/#inworkflowinputtopic-fields","title":"InWorkflowInputTopic Fields","text":"Field Type Description <code>name</code> <code>str</code> Topic name for workflow input <code>type</code> <code>str</code> Topic type identifier (\"InWorkflowInput\") <code>paired_in_workflow_output_topic_name</code> <code>str</code> Name of the paired output topic <code>condition</code> <code>Callable[[Messages], bool]</code> Function to filter publishable messages <code>event_cache</code> <code>TopicEventQueue</code> Manages event storage and consumer offsets <p>Inherits all fields from <code>TopicBase</code></p>"},{"location":"user-guide/topics/input_topics/#inworkflowinputtopic-methods","title":"InWorkflowInputTopic Methods","text":"Method Signature Description <code>builder</code> <code>() -&gt; InWorkflowInputTopicBuilder</code> Class method returning builder instance <code>publish_input_data</code> <code>(upstream_event: OutputTopicEvent, data: Messages) -&gt; PublishToTopicEvent</code> Publish input data based on upstream event <code>publish_input_data</code> <code>(upstream_event: OutputTopicEvent, data: Messages) -&gt; PublishToTopicEvent</code> Async version of publish_input_data"},{"location":"user-guide/topics/input_topics/#builders","title":"Builders","text":""},{"location":"user-guide/topics/input_topics/#inputtopicbuilder","title":"InputTopicBuilder","text":"<p>Builder for constructing InputTopic instances.</p> Method Signature Description <code>name</code> <code>(name: str) -&gt; Self</code> Set the topic name <code>condition</code> <code>(condition: Callable[[Messages], bool]) -&gt; Self</code> Set message filtering condition"},{"location":"user-guide/topics/input_topics/#inworkflowinputtopicbuilder","title":"InWorkflowInputTopicBuilder","text":"<p>Builder for constructing InWorkflowInputTopic instances.</p> Method Signature Description <code>name</code> <code>(name: str) -&gt; Self</code> Set the topic name <code>paired_in_workflow_output_topic_name</code> <code>(name: str) -&gt; Self</code> Set the paired output topic name <code>condition</code> <code>(condition: Callable[[Messages], bool]) -&gt; Self</code> Set message filtering condition"},{"location":"user-guide/topics/input_topics/#inputtopic-usage","title":"InputTopic Usage","text":""},{"location":"user-guide/topics/input_topics/#basic-input-publishing","title":"Basic Input Publishing","text":"<pre><code>from grafi.topics.topic import InputTopic\nfrom grafi.common.models.message import Message\nfrom grafi.common.models.invoke_context import InvokeContext\n\n# Create input topic\ninput_topic = InputTopic(name=\"agent_input_topic\")\n\n# Create context and messages\ncontext = InvokeContext(\n    conversation_id=\"conv_123\",\n    invoke_id=\"invoke_456\",\n    assistant_request_id=\"req_789\"\n)\nmessages = [Message(role=\"user\", content=\"Hello, assistant!\")]\n\n# Publish to input topic\nevent = input_topic.publish_data(\n    invoke_context=context,\n    publisher_name=\"user_interface\",\n    publisher_type=\"external\",\n    data=messages,\n    consumed_events=[]\n)\n\nprint(f\"Published input event: {event.event_id}\")\n</code></pre>"},{"location":"user-guide/topics/input_topics/#using-builder-pattern","title":"Using Builder Pattern","text":"<pre><code># Build input topic with custom condition\ninput_topic = (InputTopic.builder()\n    .name(\"custom_input_topic\")\n    .condition(lambda msgs: len(msgs) &gt; 0 and msgs[0].role == \"user\")\n    .build())\n</code></pre>"},{"location":"user-guide/topics/input_topics/#workflow-integration","title":"Workflow Integration","text":"<pre><code>from grafi.workflows.workflow import WorkflowBuilder\nfrom grafi.nodes.node import Node\n\n# Create workflow with input topic\nworkflow = (WorkflowBuilder()\n    .node(Node(\n        name=\"input_processor\",\n        tool=some_tool,\n        subscribed_expressions=[TopicExpr(topic=input_topic)],\n        publish_to=[some_output_topic]\n    ))\n    .build())\n</code></pre>"},{"location":"user-guide/topics/input_topics/#inworkflowinputtopic-usage","title":"InWorkflowInputTopic Usage","text":""},{"location":"user-guide/topics/input_topics/#creating-paired-topics","title":"Creating Paired Topics","text":"<pre><code>from grafi.topics.in_workflow_input_topic import InWorkflowInputTopic\nfrom grafi.topics.in_workflow_output_topic import InWorkflowOutputTopic\n\n# Create paired topics for human-in-the-loop\nworkflow_output_topic = InWorkflowOutputTopic(\n    name=\"approval_output\",\n    paired_in_workflow_input_topic_name=\"approval_input\"\n)\n\nworkflow_input_topic = InWorkflowInputTopic(\n    name=\"approval_input\",\n    paired_in_workflow_output_topic_name=\"approval_output\"\n)\n</code></pre>"},{"location":"user-guide/topics/input_topics/#publishing-user-responses","title":"Publishing User Responses","text":"<pre><code># When workflow needs human input, it publishes to output topic\noutput_event = workflow_output_topic.publish_data(\n    invoke_context=context,\n    publisher_name=\"approval_node\",\n    publisher_type=\"workflow\",\n    data=[Message(role=\"assistant\", content=\"Please approve this action\")],\n    consumed_events=[]\n)\n\n# When user responds, publish to paired input topic\nuser_response = [Message(role=\"user\", content=\"Approved\")]\ninput_data = workflow_input_topic.publish_input_data(\n    upstream_event=output_event,\n    data=user_response\n)\n</code></pre>"},{"location":"user-guide/topics/input_topics/#async-human-in-the-loop","title":"Async Human-in-the-Loop","text":"<pre><code>async def human_approval_workflow():\n    # Setup paired topics\n    output_topic = InWorkflowOutputTopic(\n        name=\"review_output\",\n        paired_in_workflow_input_topic_name=\"review_input\"\n    )\n    input_topic = InWorkflowInputTopic(\n        name=\"review_input\",\n        paired_in_workflow_output_topic_name=\"review_output\"\n    )\n\n    # Request human review\n    review_request = [Message(\n        role=\"assistant\",\n        content=\"Please review the following document...\"\n    )]\n\n    output_event = await output_topic.publish_data(\n        invoke_context=context,\n        publisher_name=\"review_system\",\n        publisher_type=\"workflow\",\n        data=review_request,\n        consumed_events=[]\n    )\n\n    # Wait for user response (in real system, this would be event-driven)\n    # ...\n\n    # Process user response\n    user_feedback = [Message(role=\"user\", content=\"Looks good, approved!\")]\n    input_data = await input_topic.publish_input_data(\n        upstream_event=output_event,\n        data=user_feedback\n    )\n\n    return input_data\n</code></pre>"},{"location":"user-guide/topics/input_topics/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/topics/input_topics/#inputtopic-design","title":"InputTopic Design","text":"<ol> <li>Entry Point: Use InputTopic as the primary entry point for workflows</li> <li>Validation: Implement condition functions to validate input data</li> <li>Context Preservation: Always include proper InvokeContext for traceability</li> <li>Error Handling: Handle invalid inputs gracefully</li> </ol>"},{"location":"user-guide/topics/input_topics/#inworkflowinputtopic-patterns","title":"InWorkflowInputTopic Patterns","text":"<ol> <li>Topic Pairing: Always create InWorkflowInputTopic with its paired InWorkflowOutputTopic</li> <li>Event Correlation: Use upstream_event to maintain event chain</li> <li>State Management: Track workflow state between output and input events</li> <li>Timeout Handling: Implement timeouts for human responses</li> </ol>"},{"location":"user-guide/topics/input_topics/#integration-guidelines","title":"Integration Guidelines","text":"<pre><code>class HumanInLoopNode(Node):\n    def __init__(self, approval_threshold: float = 0.8):\n        self.approval_threshold = approval_threshold\n\n        # Create paired topics\n        self.output_topic = InWorkflowOutputTopic(\n            name=f\"{self.name}_output\",\n            paired_in_workflow_input_topic_name=f\"{self.name}_input\"\n        )\n        self.input_topic = InWorkflowInputTopic(\n            name=f\"{self.name}_input\",\n            paired_in_workflow_output_topic_name=f\"{self.name}_output\"\n        )\n\n        super().__init__(\n            name=\"human_approval_node\",\n            tool=self.approval_tool,\n            subscribed_expressions=[\n                TopicExpr(topic=self.input_topic),\n                TopicExpr(topic=some_data_topic)\n            ],\n            publish_to=[self.output_topic, next_processing_topic]\n        )\n\n    async def approval_tool(self, context, events):\n        # Process incoming data\n        data_events = [e for e in events if e.topic_name == \"data_topic\"]\n\n        if self.needs_approval(data_events):\n            # Request human approval\n            approval_request = self.create_approval_request(data_events)\n            await self.output_topic.publish_data(\n                invoke_context=context,\n                publisher_name=self.name,\n                publisher_type=\"node\",\n                data=approval_request,\n                consumed_events=events\n            )\n\n            # Wait for human response via input_topic\n            # (handled by workflow event loop)\n</code></pre>"},{"location":"user-guide/topics/input_topics/#testing-strategies","title":"Testing Strategies","text":"<pre><code>async def test_input_topics():\n    \"\"\"Test input topic functionality.\"\"\"\n    # Test basic InputTopic\n    input_topic = InputTopic(name=\"test_input\")\n\n    messages = [Message(role=\"user\", content=\"test input\")]\n    event = await input_topic.publish_data(\n        invoke_context=InvokeContext(),\n        publisher_name=\"test\",\n        publisher_type=\"test\",\n        data=messages,\n        consumed_events=[]\n    )\n\n    assert event is not None\n    assert event.data == messages\n    assert len(input_topic.event_cache._records) == 1\n\n    # Test InWorkflowInputTopic pairing\n    output_topic = InWorkflowOutputTopic(\n        name=\"test_output\",\n        paired_in_workflow_input_topic_name=\"test_input\"\n    )\n    workflow_input_topic = InWorkflowInputTopic(\n        name=\"test_input\",\n        paired_in_workflow_output_topic_name=\"test_output\"\n    )\n\n    # Simulate workflow output\n    output_event = await output_topic.publish_data(\n        invoke_context=InvokeContext(),\n        publisher_name=\"workflow\",\n        publisher_type=\"node\",\n        data=[Message(role=\"assistant\", content=\"Need input\")],\n        consumed_events=[]\n    )\n\n    # Simulate user response\n    user_response = [Message(role=\"user\", content=\"Here's my input\")]\n    input_data = await workflow_input_topic.publish_input_data(\n        upstream_event=output_event,\n        data=user_response\n    )\n\n    assert input_data is not None\n    assert input_data.data == user_response\n    assert input_data.consumed_events == [output_event]\n</code></pre>"},{"location":"user-guide/topics/input_topics/#topic-type-constants","title":"Topic Type Constants","text":"<p>The system defines the following topic type constants:</p> <pre><code>AGENT_INPUT_TOPIC_TYPE = \"AgentInput\"\nIN_WORKFLOW_INPUT_TOPIC_TYPE = \"InWorkflowInput\"\n</code></pre> <p>These constants are used internally for topic type identification and validation within the workflow system.</p>"},{"location":"user-guide/topics/input_topics/#summary","title":"Summary","text":"<p>The input topic system provides essential entry points for data flowing into Graphite workflows. InputTopic serves as the primary entry point for initial user requests, while InWorkflowInputTopic enables sophisticated human-in-the-loop interactions within running workflows. Together with their output counterparts, they form a complete system for bidirectional communication in event-driven workflows.</p>"},{"location":"user-guide/topics/output_topics/","title":"Output Topics","text":"<p>The Graphite output topic system provides specialized topic implementations for handling output events from workflows and nodes. These topics support both synchronous and asynchronous message processing, streaming capabilities, and human-in-the-loop workflows.</p>"},{"location":"user-guide/topics/output_topics/#overview","title":"Overview","text":"<p>The output topic system includes:</p> <ul> <li>OutputTopic: Handles agent output with async generator support and streaming</li> <li>InWorkflowOutputTopic: Handles workflow output that requires human interaction</li> <li>Async Processing: Support for async generators and streaming responses</li> <li>Event Queuing: Queue-based event management for real-time processing</li> <li>Reserved Topics: Pre-configured topics for agent communication</li> </ul>"},{"location":"user-guide/topics/output_topics/#core-components","title":"Core Components","text":""},{"location":"user-guide/topics/output_topics/#outputtopic","title":"OutputTopic","text":"<p>A specialized topic for handling agent output with advanced async capabilities.</p>"},{"location":"user-guide/topics/output_topics/#outputtopic-fields","title":"OutputTopic Fields","text":"Field Type Description <code>name</code> <code>str</code> Topic name (defaults to \"agent_output_topic\") <code>event_queue</code> <code>asyncio.Queue[OutputAsyncEvent]</code> Queue for async events <code>active_generators</code> <code>List[asyncio.Task]</code> List of running generator tasks <code>publish_event_handler</code> <code>Optional[Callable[[OutputTopicEvent], None]]</code> Handler for publish events <p>Inherits all fields from <code>TopicBase</code>: <code>condition</code>, <code>consumption_offsets</code>, <code>topic_events</code></p>"},{"location":"user-guide/topics/output_topics/#outputtopic-methods","title":"OutputTopic Methods","text":"Method Signature Description <code>builder</code> <code>() -&gt; OutputTopicBuilder</code> Class method returning builder instance <code>publish_data</code> <code>(invoke_context, publisher_name, publisher_type, data, consumed_events) -&gt; Optional[OutputTopicEvent]</code> Publish messages synchronously <code>add_generator</code> <code>(generator, data, invoke_context, publisher_name, publisher_type, consumed_events) -&gt; None</code> Add async generator for streaming <code>get_events</code> <code>() -&gt; AsyncIterator[OutputAsyncEvent]</code> Get events as they become available <code>wait_for_completion</code> <code>() -&gt; None</code> Wait for all generators to complete <code>reset</code> <code>() -&gt; None</code> Reset topic and cancel generators"},{"location":"user-guide/topics/output_topics/#internal-methods","title":"Internal Methods","text":"Method Signature Description <code>_process_generator</code> <code>(generator, data, invoke_context, publisher_name, publisher_type, consumed_events) -&gt; None</code> Process async generator internally"},{"location":"user-guide/topics/output_topics/#inworkflowoutputtopic","title":"InWorkflowOutputTopic","text":"<p>A specialized topic for handling workflow output that requires human interaction.</p>"},{"location":"user-guide/topics/output_topics/#inworkflowoutputtopic-fields","title":"InWorkflowOutputTopic Fields","text":"Field Type Description <code>name</code> <code>str</code> Topic name for workflow output <code>paired_in_workflow_input_topic_name</code> <code>str</code> Name of the paired input topic <code>type</code> <code>str</code> Topic type (\"InWorkflowOutput\") <p>Inherits all fields from <code>TopicBase</code>: <code>condition</code>, <code>event_cache</code>, etc.</p>"},{"location":"user-guide/topics/output_topics/#inworkflowoutputtopic-methods","title":"InWorkflowOutputTopic Methods","text":"Method Signature Description <code>builder</code> <code>() -&gt; InWorkflowOutputTopicBuilder</code> Class method returning builder instance <code>publish_data</code> <code>(invoke_context, publisher_name, publisher_type, data, consumed_events) -&gt; OutputTopicEvent</code> Async version of publish_data"},{"location":"user-guide/topics/output_topics/#builders","title":"Builders","text":""},{"location":"user-guide/topics/output_topics/#outputtopicbuilder","title":"OutputTopicBuilder","text":"<p>Enhanced builder for OutputTopic instances.</p> Method Signature Description <code>publish_event_handler</code> <code>(handler: Callable[[OutputTopicEvent], None]) -&gt; Self</code> Set event handler for publish operations"},{"location":"user-guide/topics/output_topics/#inworkflowoutputtopicbuilder","title":"InWorkflowOutputTopicBuilder","text":"<p>Enhanced builder for InWorkflowOutputTopic instances.</p> Method Signature Description <code>paired_in_workflow_input_topic_name</code> <code>(name: str) -&gt; Self</code> Set the paired input topic name"},{"location":"user-guide/topics/output_topics/#reserved-topics","title":"Reserved Topics","text":"<p>The system includes pre-configured topic instances:</p> <pre><code>AGENT_OUTPUT_TOPIC = \"agent_output_topic\"\nIN_WORKFLOW_INPUT_TOPIC_TYPE = \"InWorkflowInput\"\nIN_WORKFLOW_OUTPUT_TOPIC_TYPE = \"InWorkflowOutput\"\n\n# Example topic instances\nagent_output_topic = OutputTopic(name=AGENT_OUTPUT_TOPIC)\n</code></pre> <p>Workflow topics are typically created as pairs for human-in-the-loop interactions.</p>"},{"location":"user-guide/topics/output_topics/#outputtopic-usage","title":"OutputTopic Usage","text":""},{"location":"user-guide/topics/output_topics/#basic-output-publishing","title":"Basic Output Publishing","text":"<pre><code>from grafi.topics.output_topic import OutputTopic, agent_output_topic\nfrom grafi.common.models.message import Message\nfrom grafi.common.models.invoke_context import InvokeContext\n\n# Create context and messages\ncontext = InvokeContext()\nmessages = [Message(role=\"assistant\", content=\"Hello, user!\")]\n\n# Publish to output topic\nevent = agent_output_topic.publish_data(\n    invoke_context=context,\n    publisher_name=\"chatbot\",\n    publisher_type=\"assistant\",\n    data=messages,\n    consumed_events=[]\n)\n\nif event:\n    print(f\"Published output: {event.event_id}\")\n</code></pre>"},{"location":"user-guide/topics/output_topics/#async-generator-support","title":"Async Generator Support","text":"<pre><code>import asyncio\nfrom typing import AsyncIterator\nfrom grafi.common.models.message import Messages\n\nasync def streaming_response() -&gt; AsyncIterator[Messages]:\n    \"\"\"Example async generator for streaming responses.\"\"\"\n    responses = [\n        [Message(role=\"assistant\", content=\"Let me think...\")],\n        [Message(role=\"assistant\", content=\"The answer is 42.\")],\n        [Message(role=\"assistant\", content=\"Is there anything else?\")]\n    ]\n\n    for response in responses:\n        await asyncio.sleep(0.1)  # Simulate processing delay\n        yield response\n\n# Add generator to output topic\ninitial_data = [Message(role=\"assistant\", content=\"Starting calculation...\")]\nagent_output_topic.add_generator(\n    generator=streaming_response(),\n    data=initial_data,\n    invoke_context=context,\n    publisher_name=\"calculator\",\n    publisher_type=\"tool\",\n    consumed_events=[]\n)\n</code></pre>"},{"location":"user-guide/topics/output_topics/#event-streaming","title":"Event Streaming","text":"<pre><code>async def consume_output_events():\n    \"\"\"Consume events as they become available.\"\"\"\n    async for event in agent_output_topic.get_events():\n        print(f\"Received event: {event.event_id}\")\n        for message in event.data:\n            print(f\"Content: {message.content}\")\n\n        # Process the event\n        await process_output_event(event)\n\n# Run the consumer\nasyncio.run(consume_output_events())\n</code></pre>"},{"location":"user-guide/topics/output_topics/#generator-management","title":"Generator Management","text":"<pre><code>async def managed_streaming():\n    \"\"\"Example of managing multiple generators.\"\"\"\n    # Add multiple generators\n    agent_output_topic.add_generator(\n        generator=stream1(),\n        data=initial_data1,\n        invoke_context=context,\n        publisher_name=\"stream1\",\n        publisher_type=\"generator\"\n    )\n\n    agent_output_topic.add_generator(\n        generator=stream2(),\n        data=initial_data2,\n        invoke_context=context,\n        publisher_name=\"stream2\",\n        publisher_type=\"generator\"\n    )\n\n    # Wait for all generators to complete\n    await agent_output_topic.wait_for_completion()\n\n    print(\"All generators completed\")\n</code></pre>"},{"location":"user-guide/topics/output_topics/#inworkflowoutputtopic-usage","title":"InWorkflowOutputTopic Usage","text":""},{"location":"user-guide/topics/output_topics/#publishing-workflow-output-for-human-interaction","title":"Publishing Workflow Output for Human Interaction","text":"<pre><code>from grafi.topics.in_workflow_output_topic import InWorkflowOutputTopic\nfrom grafi.common.models.message import Message\n\n# Create workflow output topic (paired with an input topic)\nworkflow_output_topic = InWorkflowOutputTopic(\n    name=\"review_output\",\n    paired_in_workflow_input_topic_name=\"review_input\"\n)\n\n# Create message for human review\nreview_message = [Message(role=\"assistant\", content=\"Please review this document.\")]\n\n# Publish to workflow output topic (triggers human interaction)\nevent = workflow_output_topic.publish_data(\n    invoke_context=context,\n    publisher_name=\"document_reviewer\",\n    publisher_type=\"agent\",\n    data=review_message,\n    consumed_events=[]\n)\n\nprint(f\"Sent request for review: {event.event_id}\")\n</code></pre>"},{"location":"user-guide/topics/output_topics/#integration-with-inworkflowinputtopic","title":"Integration with InWorkflowInputTopic","text":"<pre><code># InWorkflowOutputTopic works in tandem with InWorkflowInputTopic\n# When a human responds, the paired InWorkflowInputTopic receives the response\n# See input_topics.md for complete paired topic examples\n</code></pre>"},{"location":"user-guide/topics/output_topics/#human-in-the-loop-workflow-example","title":"Human-in-the-Loop Workflow Example","text":"<pre><code>class HumanApprovalWorkflow:\n    def __init__(self):\n        self.pending_approvals = {}\n\n        # Create workflow output topic for human interaction\n        self.output_topic = InWorkflowOutputTopic(\n            name=\"approval_output\",\n            paired_in_workflow_input_topic_name=\"approval_input\"\n        )\n\n    def handle_workflow_output(self, event: OutputTopicEvent):\n        \"\"\"Handle workflow output events (requests sent to human).\"\"\"\n        self.pending_approvals[event.event_id] = {\n            \"event\": event,\n            \"status\": \"pending\",\n            \"timestamp\": event.timestamp\n        }\n        print(f\"Approval request sent: {event.event_id}\")\n\n    async def request_approval(self, document: str) -&gt; OutputTopicEvent:\n        \"\"\"Request human approval for a document.\"\"\"\n        approval_message = [Message(\n            role=\"assistant\",\n            content=f\"Please approve this document: {document}\"\n        )]\n\n        # Publish to workflow output topic\n        event = await self.output_topic.publish_data(\n            invoke_context=InvokeContext(),\n            publisher_name=\"approval_system\",\n            publisher_type=\"workflow\",\n            data=approval_message,\n            consumed_events=[]\n        )\n\n        self.handle_workflow_output(event)\n        return event\n\n# Note: User responses are handled via the paired InWorkflowInputTopic\n# See input_topics.md for complete workflow examples\n</code></pre>"},{"location":"user-guide/topics/output_topics/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/topics/output_topics/#output-topic-design","title":"Output Topic Design","text":"<ol> <li>Generator Management: Always wait for generator completion or implement timeouts</li> <li>Memory Management: Monitor event queue size to prevent memory issues</li> <li>Error Handling: Implement proper error handling for async operations</li> <li>Resource Cleanup: Use reset() to properly clean up resources</li> </ol>"},{"location":"user-guide/topics/output_topics/#inworkflowoutputtopic-patterns","title":"InWorkflowOutputTopic Patterns","text":"<ol> <li>Topic Pairing: Always specify the paired InWorkflowInputTopic name</li> <li>Event Publishing: Use OutputTopicEvent for human-directed messages</li> <li>State Tracking: Track pending requests for human responses</li> <li>Integration: Coordinate with InWorkflowInputTopic for complete workflows</li> </ol>"},{"location":"user-guide/topics/output_topics/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Queue Management: Monitor and manage event queue sizes</li> <li>Generator Cleanup: Properly cancel and clean up completed generators</li> <li>Event Batching: Consider batching events for high-throughput scenarios</li> <li>Memory Monitoring: Track memory usage for long-running streams</li> </ol>"},{"location":"user-guide/topics/output_topics/#testing-strategies","title":"Testing Strategies","text":"<pre><code>async def test_output_topic():\n    \"\"\"Test output topic functionality.\"\"\"\n    topic = OutputTopic(name=\"test_output\")\n\n    # Test basic publishing\n    messages = [Message(role=\"assistant\", content=\"test\")]\n    event = await topic.publish_data(\n        invoke_context=InvokeContext(),\n        publisher_name=\"test\",\n        publisher_type=\"test\",\n        data=messages,\n        consumed_events=[]\n    )\n\n    assert event is not None\n    assert len(topic.topic_events) == 1\n\n    # Test generator addition\n    async def test_generator():\n        yield [Message(role=\"assistant\", content=\"stream1\")]\n        yield [Message(role=\"assistant\", content=\"stream2\")]\n\n    topic.add_generator(\n        generator=test_generator(),\n        data=[],\n        invoke_context=InvokeContext(),\n        publisher_name=\"test_gen\",\n        publisher_type=\"test\"\n    )\n\n    # Collect events\n    events = []\n    async for event in topic.get_events():\n        events.append(event)\n\n    assert len(events) &gt;= 2  # At least 2 streaming events\n\n    # Clean up\n    topic.reset()\n    assert len(topic.active_generators) == 0\n\ndef test_workflow_output_topic():\n    \"\"\"Test workflow output topic functionality.\"\"\"\n    # Create workflow output topic\n    output_topic = InWorkflowOutputTopic(\n        name=\"test_output\",\n        paired_in_workflow_input_topic_name=\"test_input\"\n    )\n\n    # Test publishing to workflow output\n    messages = [Message(role=\"assistant\", content=\"Please review\")]\n    output_event = output_topic.publish_data(\n        invoke_context=InvokeContext(),\n        publisher_name=\"test\",\n        publisher_type=\"test\",\n        data=messages,\n        consumed_events=[]\n    )\n\n    assert output_event is not None\n    assert output_event.topic_name == \"test_output\"\n    assert len(output_topic.event_cache._records) == 1\n\n    # Verify paired topic name is set\n    assert output_topic.paired_in_workflow_input_topic_name == \"test_input\"\n</code></pre> <p>The output topic system provides powerful capabilities for handling agent outputs, streaming responses, and workflow interactions in Graphite applications, supporting both real-time and batch processing scenarios with comprehensive error handling and monitoring capabilities. The new InWorkflow topics enable seamless human-in-the-loop workflows with proper event coordination and state management.</p>"},{"location":"user-guide/topics/subscription_expression/","title":"Topic Subscription Expressions","text":"<p>The Graphite topic subscription expression system provides a Domain Specific Language (DSL) for creating complex subscription patterns. It allows components to subscribe to multiple topics using logical operators, enabling sophisticated message routing and consumption patterns.</p>"},{"location":"user-guide/topics/subscription_expression/#overview","title":"Overview","text":"<p>The subscription expression system enables:</p> <ul> <li>Complex Subscriptions: Subscribe to multiple topics with logical combinations</li> <li>Expression Trees: Build hierarchical subscription logic using AND/OR operations</li> <li>Dynamic Evaluation: Evaluate subscriptions against available messages</li> <li>Topic Extraction: Extract all referenced topics from complex expressions</li> <li>Fluent API: Build expressions using a chainable builder pattern</li> </ul>"},{"location":"user-guide/topics/subscription_expression/#core-components","title":"Core Components","text":""},{"location":"user-guide/topics/subscription_expression/#expression-types","title":"Expression Types","text":""},{"location":"user-guide/topics/subscription_expression/#subexpr-base-class","title":"SubExpr (Base Class)","text":"<p>Abstract base class for all subscription expressions.</p> Method Signature Description <code>to_dict</code> <code>() -&gt; dict[str, Any]</code> Serialize expression to dictionary (abstract)"},{"location":"user-guide/topics/subscription_expression/#topicexpr","title":"TopicExpr","text":"<p>Represents a subscription to a single topic.</p> Field Type Description <code>topic</code> <code>TopicBase</code> The topic to subscribe to Method Signature Description <code>to_dict</code> <code>() -&gt; dict[str, Any]</code> Serialize topic expression to dictionary"},{"location":"user-guide/topics/subscription_expression/#combinedexpr","title":"CombinedExpr","text":"<p>Represents a logical combination of two expressions.</p> Field Type Description <code>op</code> <code>LogicalOp</code> Logical operator (AND/OR) <code>left</code> <code>SubExpr</code> Left expression operand <code>right</code> <code>SubExpr</code> Right expression operand Method Signature Description <code>to_dict</code> <code>() -&gt; dict[str, Any]</code> Serialize combined expression to dictionary"},{"location":"user-guide/topics/subscription_expression/#logical-operators","title":"Logical Operators","text":""},{"location":"user-guide/topics/subscription_expression/#logicalop-enum","title":"LogicalOp Enum","text":"<p>Defines available logical operators for combining expressions.</p> Value Description <code>AND</code> Both expressions must have new messages <code>OR</code> Either expression must have new messages"},{"location":"user-guide/topics/subscription_expression/#subscriptionbuilder","title":"SubscriptionBuilder","text":"<p>Fluent API builder for constructing subscription expressions.</p> Field Type Description <code>root_expr</code> <code>Optional[SubExpr]</code> Root expression being built <code>pending_op</code> <code>Optional[LogicalOp]</code> Pending logical operator"},{"location":"user-guide/topics/subscription_expression/#builder-methods","title":"Builder Methods","text":"Method Signature Description <code>subscribed_to</code> <code>(topic: TopicBase) -&gt; SubscriptionBuilder</code> Add topic to subscription <code>and_</code> <code>() -&gt; SubscriptionBuilder</code> Set AND operator for next topic <code>or_</code> <code>() -&gt; SubscriptionBuilder</code> Set OR operator for next topic <code>build</code> <code>() -&gt; SubExpr</code> Build final expression"},{"location":"user-guide/topics/subscription_expression/#expression-evaluation","title":"Expression Evaluation","text":""},{"location":"user-guide/topics/subscription_expression/#evaluation-function","title":"Evaluation Function","text":"<p>The system provides a function to evaluate subscription expressions against available messages:</p> <pre><code>def evaluate_subscription(expr: SubExpr, topics_with_new_msgs: List[str]) -&gt; bool:\n    \"\"\"\n    Evaluate the subscription expression given the list of topic names\n    that have new (unread) messages.\n    \"\"\"\n    if isinstance(expr, TopicExpr):\n        return expr.topic.name in topics_with_new_msgs\n    elif isinstance(expr, CombinedExpr):\n        left_val = evaluate_subscription(expr.left, topics_with_new_msgs)\n        right_val = evaluate_subscription(expr.right, topics_with_new_msgs)\n        if expr.op == LogicalOp.AND:\n            return left_val and right_val\n        else:  # expr.op == LogicalOp.OR\n            return left_val or right_val\n    else:\n        return False\n</code></pre>"},{"location":"user-guide/topics/subscription_expression/#evaluation-logic","title":"Evaluation Logic","text":"<ul> <li>TopicExpr: Returns <code>True</code> if the topic name is in the list of topics with new messages</li> <li>CombinedExpr with AND: Returns <code>True</code> if both left and right expressions evaluate to <code>True</code></li> <li>CombinedExpr with OR: Returns <code>True</code> if either left or right expression evaluates to <code>True</code></li> <li>Unknown Expression: Returns <code>False</code> for safety</li> </ul>"},{"location":"user-guide/topics/subscription_expression/#topic-extraction","title":"Topic Extraction","text":""},{"location":"user-guide/topics/subscription_expression/#extract-topics-function","title":"Extract Topics Function","text":"<p>Utility function to recursively extract all topics from an expression:</p> <pre><code>def extract_topics(expr: SubExpr) -&gt; List[TopicBase]:\n    \"\"\"Recursively collect topic names from a DSL expression tree.\"\"\"\n    if isinstance(expr, TopicExpr):\n        return [expr.topic]\n    elif isinstance(expr, CombinedExpr):\n        return extract_topics(expr.left) + extract_topics(expr.right)\n    return []\n</code></pre> <p>This function traverses the expression tree and collects all referenced topics, useful for:</p> <ul> <li>Setting up subscriptions</li> <li>Validating topic availability</li> <li>Dependency analysis</li> </ul>"},{"location":"user-guide/topics/subscription_expression/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/topics/subscription_expression/#simple-topic-subscription","title":"Simple Topic Subscription","text":"<pre><code>from grafi.topics.subscription_builder import SubscriptionBuilder\nfrom grafi.topics.topic import Topic\n\n# Create topics\nnotifications = Topic(name=\"notifications\")\nalerts = Topic(name=\"alerts\")\n\n# Simple subscription to one topic\nexpr = (SubscriptionBuilder()\n    .subscribed_to(notifications)\n    .build())\n</code></pre>"},{"location":"user-guide/topics/subscription_expression/#and-combination","title":"AND Combination","text":"<pre><code># Subscribe to both topics (both must have new messages)\nexpr = (SubscriptionBuilder()\n    .subscribed_to(notifications)\n    .and_()\n    .subscribed_to(alerts)\n    .build())\n</code></pre>"},{"location":"user-guide/topics/subscription_expression/#or-combination","title":"OR Combination","text":"<pre><code># Subscribe to either topic (at least one must have new messages)\nexpr = (SubscriptionBuilder()\n    .subscribed_to(notifications)\n    .or_()\n    .subscribed_to(alerts)\n    .build())\n</code></pre>"},{"location":"user-guide/topics/subscription_expression/#complex-expressions","title":"Complex Expressions","text":"<pre><code># Complex subscription: (notifications AND alerts) OR errors\nerrors = Topic(name=\"errors\")\n\nexpr = (SubscriptionBuilder()\n    .subscribed_to(notifications)\n    .and_()\n    .subscribed_to(alerts)\n    .or_()\n    .subscribed_to(errors)\n    .build())\n</code></pre>"},{"location":"user-guide/topics/subscription_expression/#multi-level-expressions","title":"Multi-level Expressions","text":"<pre><code># Create nested expressions manually for complex logic\n# (notifications OR alerts) AND (errors OR warnings)\nwarnings = Topic(name=\"warnings\")\n\n# First sub-expression: notifications OR alerts\nleft_expr = (SubscriptionBuilder()\n    .subscribed_to(notifications)\n    .or_()\n    .subscribed_to(alerts)\n    .build())\n\n# Second sub-expression: errors OR warnings  \nright_expr = (SubscriptionBuilder()\n    .subscribed_to(errors)\n    .or_()\n    .subscribed_to(warnings)\n    .build())\n\n# Combine manually\nfrom grafi.topics.topic_expression import CombinedExpr, LogicalOp\ncomplex_expr = CombinedExpr(\n    op=LogicalOp.AND,\n    left=left_expr,\n    right=right_expr\n)\n</code></pre>"},{"location":"user-guide/topics/subscription_expression/#evaluation-examples","title":"Evaluation Examples","text":""},{"location":"user-guide/topics/subscription_expression/#basic-evaluation","title":"Basic Evaluation","text":"<pre><code>from grafi.topics.topic_expression import evaluate_subscription\n\n# Topics with new messages\ntopics_with_msgs = [\"notifications\", \"errors\"]\n\n# Evaluate simple expression\nsimple_expr = (SubscriptionBuilder()\n    .subscribed_to(notifications)\n    .build())\n\nresult = evaluate_subscription(simple_expr, topics_with_msgs)\n# Returns True because \"notifications\" is in topics_with_msgs\n</code></pre>"},{"location":"user-guide/topics/subscription_expression/#and-evaluation","title":"AND Evaluation","text":"<pre><code># AND expression: both topics must have messages\nand_expr = (SubscriptionBuilder()\n    .subscribed_to(notifications)\n    .and_()\n    .subscribed_to(alerts)\n    .build())\n\n# Only notifications has new messages\nresult = evaluate_subscription(and_expr, [\"notifications\"])\n# Returns False because alerts doesn't have new messages\n\n# Both have new messages\nresult = evaluate_subscription(and_expr, [\"notifications\", \"alerts\"])\n# Returns True because both topics have new messages\n</code></pre>"},{"location":"user-guide/topics/subscription_expression/#or-evaluation","title":"OR Evaluation","text":"<pre><code># OR expression: either topic can have messages\nor_expr = (SubscriptionBuilder()\n    .subscribed_to(notifications)\n    .or_()\n    .subscribed_to(alerts)\n    .build())\n\n# Only notifications has new messages\nresult = evaluate_subscription(or_expr, [\"notifications\"])\n# Returns True because at least one topic has new messages\n\n# Neither has new messages\nresult = evaluate_subscription(or_expr, [\"other_topic\"])\n# Returns False because neither topic has new messages\n</code></pre>"},{"location":"user-guide/topics/subscription_expression/#serialization","title":"Serialization","text":""},{"location":"user-guide/topics/subscription_expression/#expression-serialization","title":"Expression Serialization","text":"<p>All expressions can be serialized to dictionaries for persistence or transport:</p> <pre><code># Simple topic expression\ntopic_expr = TopicExpr(topic=notifications)\nserialized = topic_expr.to_dict()\n# Returns: {\"topic\": {\"name\": \"notifications\", \"condition\": {...}}}\n\n# Combined expression\ncombined_expr = CombinedExpr(\n    op=LogicalOp.AND,\n    left=TopicExpr(topic=notifications),\n    right=TopicExpr(topic=alerts)\n)\nserialized = combined_expr.to_dict()\n# Returns: {\n#   \"op\": \"AND\",\n#   \"left\": {\"topic\": {\"name\": \"notifications\", ...}},\n#   \"right\": {\"topic\": {\"name\": \"alerts\", ...}}\n# }\n</code></pre>"},{"location":"user-guide/topics/subscription_expression/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/topics/subscription_expression/#builder-validation","title":"Builder Validation","text":"<pre><code>def safe_subscription_build():\n    \"\"\"Example of proper error handling in subscription building.\"\"\"\n    try:\n        builder = SubscriptionBuilder()\n\n        # This will raise ValueError if topic is not TopicBase\n        expr = builder.subscribed_to(\"invalid_topic\").build()\n\n    except ValueError as e:\n        print(f\"Invalid subscription: {e}\")\n        return None\n\n    try:\n        builder = SubscriptionBuilder()\n\n        # This will raise ValueError - missing operator\n        expr = (builder\n            .subscribed_to(topic1)\n            .subscribed_to(topic2)  # Missing .and_() or .or_()\n            .build())\n\n    except ValueError as e:\n        print(f\"Invalid subscription chain: {e}\")\n        return None\n</code></pre>"},{"location":"user-guide/topics/subscription_expression/#evaluation-safety","title":"Evaluation Safety","text":"<pre><code>def safe_evaluate(expr: SubExpr, topics_with_msgs: List[str]) -&gt; bool:\n    \"\"\"Safely evaluate subscription with error handling.\"\"\"\n    try:\n        return evaluate_subscription(expr, topics_with_msgs)\n    except Exception as e:\n        logger.error(f\"Error evaluating subscription: {e}\")\n        return False\n</code></pre>"},{"location":"user-guide/topics/subscription_expression/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/topics/subscription_expression/#subscription-design","title":"Subscription Design","text":"<ol> <li>Keep It Simple: Start with simple expressions and add complexity as needed</li> <li>Logical Grouping: Group related topics with appropriate operators</li> <li>Performance Consideration: Remember that AND operations are more restrictive</li> <li>Topic Dependencies: Consider message flow and dependencies between topics</li> </ol>"},{"location":"user-guide/topics/subscription_expression/#builder-usage","title":"Builder Usage","text":"<ol> <li>Operator Placement: Always place operators (<code>.and_()</code>, <code>.or_()</code>) between topics</li> <li>Error Handling: Wrap builder operations in try-catch blocks</li> <li>Validation: Validate topics exist before building subscriptions</li> <li>Reusability: Extract common subscription patterns into helper functions</li> </ol>"},{"location":"user-guide/topics/subscription_expression/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Topic Ordering: Place frequently updated topics first in OR expressions</li> <li>Expression Structure: Structure expressions to fail fast when possible</li> <li>Topic Extraction: Cache extracted topics to avoid repeated extraction</li> <li>Evaluation Frequency: Consider caching evaluation results for expensive expressions</li> </ol>"},{"location":"user-guide/topics/subscription_expression/#testing-strategies","title":"Testing Strategies","text":"<pre><code>def test_subscription_expression():\n    \"\"\"Test subscription expression building and evaluation.\"\"\"\n    # Create test topics\n    topic1 = Topic(name=\"test1\")\n    topic2 = Topic(name=\"test2\")\n\n    # Test AND expression\n    and_expr = (SubscriptionBuilder()\n        .subscribed_to(topic1)\n        .and_()\n        .subscribed_to(topic2)\n        .build())\n\n    # Test with no messages\n    assert not evaluate_subscription(and_expr, [])\n\n    # Test with one message\n    assert not evaluate_subscription(and_expr, [\"test1\"])\n\n    # Test with both messages\n    assert evaluate_subscription(and_expr, [\"test1\", \"test2\"])\n\n    # Test OR expression\n    or_expr = (SubscriptionBuilder()\n        .subscribed_to(topic1)\n        .or_()\n        .subscribed_to(topic2)\n        .build())\n\n    # Test with one message\n    assert evaluate_subscription(or_expr, [\"test1\"])\n    assert evaluate_subscription(or_expr, [\"test2\"])\n</code></pre> <p>The topic subscription expression system provides a powerful and flexible way to define complex message consumption patterns in Graphite applications, enabling sophisticated event-driven architectures with precise control over when components should process messages.</p>"},{"location":"user-guide/topics/topic/","title":"Topic","text":"<p>The <code>Topic</code> class is a concrete implementation of <code>TopicBase</code> that provides a complete message queue system for publishing and consuming messages within Graphite applications. It includes built-in logging, event handling, and condition-based message filtering.</p>"},{"location":"user-guide/topics/topic/#overview","title":"Overview","text":"<p>The <code>Topic</code> class extends <code>TopicBase</code> to provide:</p> <ul> <li>Conditional Publishing: Messages are only published if they meet the topic's condition</li> <li>Event Handling: Optional event handlers for publish operations</li> <li>Logging Integration: Automatic logging of publish operations</li> <li>Builder Pattern: Fluent API for topic configuration</li> </ul>"},{"location":"user-guide/topics/topic/#core-components","title":"Core Components","text":""},{"location":"user-guide/topics/topic/#topic-class","title":"Topic Class","text":"<p>A complete topic implementation with publishing logic and event handling.</p>"},{"location":"user-guide/topics/topic/#fields","title":"Fields","text":"Field Type Description <code>topic_events</code> <code>List[TopicEvent]</code> List of all topic events (overrides base) <code>publish_event_handler</code> <code>Optional[Callable[[PublishToTopicEvent], None]]</code> Optional handler called after successful publishing <p>Inherits all fields from <code>TopicBase</code>: <code>name</code>, <code>condition</code>, <code>consumption_offsets</code></p>"},{"location":"user-guide/topics/topic/#methods","title":"Methods","text":"Method Signature Description <code>builder</code> <code>() -&gt; TopicBuilder</code> Class method returning a TopicBuilder instance <code>publish_data</code> <code>(invoke_context, publisher_name, publisher_type, data, consumed_events) -&gt; PublishToTopicEvent</code> Publishes messages if condition is met <p>Inherits all methods from <code>TopicBase</code>: <code>can_consume</code>, <code>consume</code>, <code>reset</code>, <code>restore_topic</code>, etc.</p>"},{"location":"user-guide/topics/topic/#topicbuilder","title":"TopicBuilder","text":"<p>Enhanced builder for <code>Topic</code> instances with additional configuration options.</p>"},{"location":"user-guide/topics/topic/#builder-methods","title":"Builder Methods","text":"Method Signature Description <code>publish_event_handler</code> <code>(handler: Callable[[PublishToTopicEvent], None]) -&gt; Self</code> Set event handler for publish operations <p>Inherits all builder methods from <code>TopicBaseBuilder</code>: <code>name</code>, <code>condition</code></p>"},{"location":"user-guide/topics/topic/#publishing-logic","title":"Publishing Logic","text":""},{"location":"user-guide/topics/topic/#conditional-publishing","title":"Conditional Publishing","text":"<p>The <code>publish_data</code> method implements condition-based publishing:</p> <pre><code>def publish_data(\n    self,\n    invoke_context: InvokeContext,\n    publisher_name: str,\n    publisher_type: str,\n    data: Messages,\n    consumed_events: List[ConsumeFromTopicEvent],\n) -&gt; PublishToTopicEvent:\n    \"\"\"\n    Publishes a message's event ID to this topic if it meets the condition.\n    \"\"\"\n    if self.condition(data):\n        event = PublishToTopicEvent(\n            invoke_context=invoke_context,\n            name=self.name,\n            publisher_name=publisher_name,\n            publisher_type=publisher_type,\n            data=data,\n            consumed_event_ids=[\n                consumed_event.event_id for consumed_event in consumed_events\n            ],\n            offset=len(self.topic_events),\n        )\n        self.topic_events.append(event)\n        if self.publish_event_handler:\n            self.publish_event_handler(event)\n        logger.info(\n            f\"[{self.name}] Message published with event_id: {event.event_id}\"\n        )\n        return event\n    else:\n        logger.info(f\"[{self.name}] Message NOT published (condition not met)\")\n        return None\n</code></pre>"},{"location":"user-guide/topics/topic/#publishing-workflow","title":"Publishing Workflow","text":"<ol> <li>Condition Check: Evaluate if messages meet the topic's condition</li> <li>Event Creation: Create <code>PublishToTopicEvent</code> with metadata and offset</li> <li>Event Storage: Add event to topic's event list</li> <li>Handler Execution: Call publish event handler if configured</li> <li>Logging: Log success or failure with appropriate message</li> <li>Return Result: Return event on success, <code>None</code> on condition failure</li> </ol>"},{"location":"user-guide/topics/topic/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/topics/topic/#basic-topic-creation","title":"Basic Topic Creation","text":"<pre><code>from grafi.topics.topic import Topic\n\n# Create simple topic\ntopic = Topic(name=\"notifications\")\n\n# Or use builder pattern\ntopic = (Topic.builder()\n    .name(\"notifications\")\n    .condition(lambda msgs: len(msgs) &gt; 0)\n    .build())\n</code></pre>"},{"location":"user-guide/topics/topic/#topic-with-event-handler","title":"Topic with Event Handler","text":"<pre><code>def on_message_published(event: PublishToTopicEvent):\n    print(f\"Published message to {event.name}: {event.data}\")\n\ntopic = (Topic.builder()\n    .name(\"alerts\")\n    .condition(lambda msgs: any(\"error\" in msg.content.lower() for msg in msgs))\n    .publish_event_handler(on_message_published)\n    .build())\n</code></pre>"},{"location":"user-guide/topics/topic/#publishing-messages","title":"Publishing Messages","text":"<pre><code>from grafi.common.models.invoke_context import InvokeContext\nfrom grafi.common.models.message import Message\n\n# Create context and messages\ncontext = InvokeContext()\nmessages = [Message(role=\"user\", content=\"Hello world\")]\n\n# Publish to topic\nevent = await topic.publish_data(\n    invoke_context=context,\n    publisher_name=\"my_publisher\",\n    publisher_type=\"application\",\n    data=messages,\n    consumed_events=[]\n)\n\nif event:\n    print(f\"Published: {event.event_id}\")\nelse:\n    print(\"Message did not meet condition\")\n</code></pre>"},{"location":"user-guide/topics/topic/#message-consumption","title":"Message Consumption","text":"<pre><code># Check for new messages\nif topic.can_consume(\"consumer_1\"):\n    messages = topic.consume(\"consumer_1\")\n    for message in messages:\n        print(f\"Consumed: {message.data}\")\n</code></pre>"},{"location":"user-guide/topics/topic/#filtering-and-conditions","title":"Filtering and Conditions","text":""},{"location":"user-guide/topics/topic/#custom-conditions","title":"Custom Conditions","text":"<pre><code># Only publish error messages\nerror_topic = (Topic.builder()\n    .name(\"errors\")\n    .condition(lambda msgs: any(\"error\" in msg.content.lower() for msg in msgs))\n    .build())\n\n# Only publish messages from specific roles\nadmin_topic = (Topic.builder()\n    .name(\"admin_messages\")\n    .condition(lambda msgs: any(msg.role == \"admin\" for msg in msgs))\n    .build())\n\n# Complex business logic\nvalidated_topic = (Topic.builder()\n    .name(\"validated_messages\")\n    .condition(lambda msgs: all(\n        msg.metadata.get(\"validated\", False) for msg in msgs\n    ))\n    .build())\n</code></pre>"},{"location":"user-guide/topics/topic/#default-behavior","title":"Default Behavior","text":"<pre><code># Accept all messages (default condition)\nall_messages_topic = Topic(name=\"all_messages\")\n# Equivalent to: condition=lambda _: True\n</code></pre>"},{"location":"user-guide/topics/topic/#pre-configured-topics","title":"Pre-configured Topics","text":""},{"location":"user-guide/topics/topic/#agent-input-topic","title":"Agent Input Topic","text":"<p>The module provides a pre-configured topic for agent input:</p> <pre><code>from grafi.topics.topic import agent_input_topic\n\n# Use the predefined agent input topic\nevent = agent_input_topic.publish_data(\n    invoke_context=context,\n    publisher_name=\"user_interface\",\n    publisher_type=\"input_handler\",\n    data=user_messages,\n    consumed_events=[]\n)\n</code></pre>"},{"location":"user-guide/topics/topic/#logging-integration","title":"Logging Integration","text":""},{"location":"user-guide/topics/topic/#log-messages","title":"Log Messages","text":"<p>The Topic class automatically logs publishing operations:</p> <pre><code># Successful publishing\nlogger.info(f\"[{self.name}] Message published with event_id: {event.event_id}\")\n\n# Condition not met\nlogger.info(f\"[{self.name}] Message NOT published (condition not met)\")\n</code></pre>"},{"location":"user-guide/topics/topic/#log-format-examples","title":"Log Format Examples","text":"<pre><code>INFO: [notifications] Message published with event_id: evt_123456\nINFO: [errors] Message NOT published (condition not met)\nINFO: [alerts] Message published with event_id: evt_789012\n</code></pre>"},{"location":"user-guide/topics/topic/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/topics/topic/#topic-design","title":"Topic Design","text":"<ol> <li>Meaningful Names: Use descriptive names that indicate the topic's purpose</li> <li>Focused Conditions: Keep condition functions simple and focused</li> <li>Event Handlers: Use handlers for side effects, not primary logic</li> <li>Error Handling: Handle condition evaluation errors gracefully</li> </ol>"},{"location":"user-guide/topics/topic/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Efficient Conditions: Optimize condition functions for frequent evaluation</li> <li>Handler Performance: Keep event handlers lightweight and fast</li> <li>Memory Management: Monitor topic event accumulation</li> <li>Batch Processing: Consider batching for high-volume scenarios</li> </ol>"},{"location":"user-guide/topics/topic/#error-handling","title":"Error Handling","text":"<pre><code>def safe_condition(messages: Messages) -&gt; bool:\n    try:\n        return any(\"priority\" in msg.metadata for msg in messages)\n    except (AttributeError, KeyError):\n        return False\n\ndef safe_handler(event: PublishToTopicEvent):\n    try:\n        process_event(event)\n    except Exception as e:\n        logger.error(f\"Error in event handler: {e}\")\n</code></pre>"},{"location":"user-guide/topics/topic/#testing-strategies","title":"Testing Strategies","text":"<pre><code>def test_topic_publishing():\n    # Create test topic\n    topic = Topic(name=\"test_topic\")\n    messages = [Message(role=\"user\", content=\"test\")]\n\n    # Test successful publishing\n    event = await topic.publish_data(\n        invoke_context=InvokeContext(),\n        publisher_name=\"test\",\n        publisher_type=\"test\",\n        data=messages,\n        consumed_events=[]\n    )\n\n    assert event is not None\n    assert len(topic.topic_events) == 1\n\ndef test_condition_filtering():\n    # Create topic with condition\n    topic = (Topic.builder()\n        .name(\"filtered_topic\")\n        .condition(lambda msgs: len(msgs) &gt; 1)\n        .build())\n\n    # Test with single message (should be filtered)\n    single_message = [Message(role=\"user\", content=\"test\")]\n    event = await topic.publish_data(\n        invoke_context=InvokeContext(),\n        publisher_name=\"test\",\n        publisher_type=\"test\",\n        data=single_message,\n        consumed_events=[]\n    )\n\n    assert event is None\n    assert len(topic.topic_events) == 0\n</code></pre> <p>The <code>Topic</code> class provides a robust, production-ready implementation of the topic-based messaging pattern with built-in logging, event handling, and flexible configuration options for Graphite applications.</p>"},{"location":"user-guide/topics/topic_base/","title":"Topic Base","text":"<p>The Graphite topic base system provides the foundational components for implementing topic-based messaging patterns. It enables publishers to send messages to named topics and consumers to receive messages based on configurable conditions, supporting decoupled communication between system components.</p>"},{"location":"user-guide/topics/topic_base/#overview","title":"Overview","text":"<p>The topic base system implements a publish-subscribe messaging pattern where:</p> <ul> <li>Publishers: Send messages to named topics</li> <li>Consumers: Subscribe to topics and receive messages</li> <li>Conditions: Filter messages based on custom logic</li> <li>Offsets: Track consumption progress for each consumer</li> <li>Events: Maintain a complete audit trail of all operations</li> </ul>"},{"location":"user-guide/topics/topic_base/#core-components","title":"Core Components","text":""},{"location":"user-guide/topics/topic_base/#topicbase","title":"TopicBase","text":"<p>The base class for all topic implementations, providing core messaging functionality.</p>"},{"location":"user-guide/topics/topic_base/#fields","title":"Fields","text":"Field Type Description <code>name</code> <code>str</code> Unique identifier for the topic <code>type</code> <code>str</code> Topic type identifier <code>condition</code> <code>Callable[[Messages], bool]</code> Function to filter publishable messages <code>event_cache</code> <code>TopicEventQueue</code> Manages event storage and consumer offsets <code>publish_event_handler</code> <code>Optional[Callable]</code> Handler for publish events"},{"location":"user-guide/topics/topic_base/#core-methods","title":"Core Methods","text":"Method Signature Description <code>publish_data</code> <code>(invoke_context, publisher_name, publisher_type, data, consumed_events) -&gt; PublishToTopicEvent</code> Publish messages to the topic (abstract) <code>can_consume</code> <code>(consumer_name: str) -&gt; bool</code> Check if consumer has unread messages <code>consume</code> <code>(consumer_name: str) -&gt; List[PublishToTopicEvent]</code> Retrieve unread messages for consumer <code>consume</code> <code>async (consumer_name: str, timeout: Optional[float]) -&gt; List[TopicEvent]</code> Async version of consume with timeout <code>commit</code> <code>async (consumer_name: str, offset: int) -&gt; None</code> Commit processed messages up to offset <code>reset</code> <code>() -&gt; None</code> Reset topic to initial state <code>reset</code> <code>async () -&gt; None</code> Async version of reset <code>restore_topic</code> <code>(topic_event: TopicEvent) -&gt; None</code> Restore topic from event <code>restore_topic</code> <code>async (topic_event: TopicEvent) -&gt; None</code> Async version of restore_topic"},{"location":"user-guide/topics/topic_base/#utility-methods","title":"Utility Methods","text":"Method Signature Description <code>to_dict</code> <code>() -&gt; dict[str, Any]</code> Serialize topic to dictionary <code>serialize_callable</code> <code>() -&gt; dict</code> Serialize condition function"},{"location":"user-guide/topics/topic_base/#topicbasebuilder","title":"TopicBaseBuilder","text":"<p>Builder pattern implementation for constructing topic instances.</p>"},{"location":"user-guide/topics/topic_base/#builder-methods","title":"Builder Methods","text":"Method Signature Description <code>name</code> <code>(name: str) -&gt; Self</code> Set topic name with validation <code>condition</code> <code>(condition: Callable[[Messages], bool]) -&gt; Self</code> Set message filtering condition"},{"location":"user-guide/topics/topic_base/#reserved-topics","title":"Reserved Topics","text":"<p>The system includes reserved topic names for internal agent operations:</p> <pre><code>AGENT_RESERVED_TOPICS = [\n    \"agent_input_topic\",\n    \"agent_output_topic\"\n]\n</code></pre> <p>These topics cannot be used for custom topic names to avoid conflicts with system functionality. Additionally, the system supports workflow-specific topic types:</p> <ul> <li><code>IN_WORKFLOW_INPUT_TOPIC_TYPE = \"InWorkflowInput\"</code></li> <li><code>IN_WORKFLOW_OUTPUT_TOPIC_TYPE = \"InWorkflowOutput\"</code></li> </ul>"},{"location":"user-guide/topics/topic_base/#message-publishing","title":"Message Publishing","text":"<p>The publishing mechanism is abstract and must be implemented by subclasses:</p> <pre><code>def publish_data(\n    self,\n    invoke_context: InvokeContext,\n    publisher_name: str,\n    publisher_type: str,\n    data: Messages,\n    consumed_events: List[ConsumeFromTopicEvent],\n) -&gt; PublishToTopicEvent:\n    \"\"\"\n    Publish data to the topic if it meets the condition.\n    \"\"\"\n    raise NotImplementedError(\n        \"Method 'publish_data' must be implemented in subclasses.\"\n    )\n</code></pre>"},{"location":"user-guide/topics/topic_base/#message-consumption","title":"Message Consumption","text":"<p>The topic system uses a sophisticated caching mechanism (<code>TopicEventQueue</code>) that manages consumed and committed offsets separately for reliable message processing.</p>"},{"location":"user-guide/topics/topic_base/#consumption-check","title":"Consumption Check","text":"<pre><code>def can_consume(self, consumer_name: str) -&gt; bool:\n    \"\"\"Check if consumer has unread messages.\"\"\"\n    return self.event_cache.can_consume(consumer_name)\n</code></pre>"},{"location":"user-guide/topics/topic_base/#message-retrieval","title":"Message Retrieval","text":"<pre><code>def consume(self, consumer_name: str) -&gt; List[PublishToTopicEvent | OutputTopicEvent]:\n    \"\"\"Retrieve unread messages for consumer.\"\"\"\n    # Get new events using the offset range\n    new_events = self.event_cache.fetch(consumer_name)\n\n    # Filter to only return PublishToTopicEvent instances for backward compatibility\n    return [\n        event\n        for event in new_events\n        if isinstance(event, (PublishToTopicEvent, OutputTopicEvent))\n    ]\n</code></pre>"},{"location":"user-guide/topics/topic_base/#async-message-retrieval","title":"Async Message Retrieval","text":"<pre><code>async def consume(\n    self, consumer_name: str, timeout: Optional[float] = None\n) -&gt; List[TopicEvent]:\n    \"\"\"Asynchronously retrieve new/unconsumed messages for the given node.\"\"\"\n    return await self.event_cache.fetch(consumer_name, timeout=timeout)\n</code></pre>"},{"location":"user-guide/topics/topic_base/#offset-management","title":"Offset Management","text":"<p>The system maintains two types of offsets:</p> <ul> <li>Consumed Offset: Tracks what has been fetched (advanced immediately on fetch)</li> <li>Committed Offset: Tracks what has been fully processed (advanced after processing)</li> </ul> <pre><code>async def commit(self, consumer_name: str, offset: int) -&gt; None:\n    \"\"\"Commit processed messages up to the specified offset.\"\"\"\n    await self.event_cache.commit_to(consumer_name, offset)\n</code></pre>"},{"location":"user-guide/topics/topic_base/#message-filtering","title":"Message Filtering","text":"<p>Topics support flexible message filtering through condition functions:</p>"},{"location":"user-guide/topics/topic_base/#default-condition","title":"Default Condition","text":"<pre><code>condition: Callable[[Messages], bool] = Field(default=lambda _: True)\n</code></pre>"},{"location":"user-guide/topics/topic_base/#condition-serialization","title":"Condition Serialization","text":"<p>The system can serialize various types of condition functions:</p> <pre><code>def serialize_callable(self) -&gt; dict:\n    \"\"\"Serialize condition function for persistence.\"\"\"\n    if callable(self.condition):\n        if inspect.isfunction(self.condition):\n            if self.condition.__name__ == \"&lt;lambda&gt;\":\n                # Lambda function\n                try:\n                    source = inspect.getsource(self.condition).strip()\n                except (OSError, TypeError):\n                    source = \"&lt;unable to retrieve source&gt;\"\n                return {\"type\": \"lambda\", \"code\": source}\n            else:\n                # Named function\n                return {\"type\": \"function\", \"name\": self.condition.__name__}\n        elif inspect.isbuiltin(self.condition):\n            return {\"type\": \"builtin\", \"name\": self.condition.__name__}\n        elif hasattr(self.condition, \"__call__\"):\n            return {\n                \"type\": \"callable_object\",\n                \"class_name\": self.condition.__class__.__name__,\n            }\n    return {\"type\": \"unknown\"}\n</code></pre>"},{"location":"user-guide/topics/topic_base/#topic-state-management","title":"Topic State Management","text":""},{"location":"user-guide/topics/topic_base/#reset-topic","title":"Reset Topic","text":"<pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset the topic to its initial state.\"\"\"\n    self.event_cache = TopicEventQueue(self.name)\n\nasync def reset(self) -&gt; None:\n    \"\"\"Asynchronously reset the topic to its initial state.\"\"\"\n    self.event_cache.reset()\n    self.event_cache = TopicEventQueue(self.name)\n</code></pre>"},{"location":"user-guide/topics/topic_base/#restore-topic","title":"Restore Topic","text":"<pre><code>def restore_topic(self, topic_event: TopicEvent) -&gt; None:\n    \"\"\"Restore a topic from a topic event.\"\"\"\n    if isinstance(topic_event, PublishToTopicEvent) or isinstance(\n        topic_event, OutputTopicEvent\n    ):\n        self.event_cache.put(topic_event)\n    elif isinstance(topic_event, ConsumeFromTopicEvent):\n        self.event_cache.fetch(\n            consumer_id=topic_event.consumer_name, offset=topic_event.offset + 1\n        )\n        self.event_cache.commit_to(topic_event.consumer_name, topic_event.offset)\n\nasync def restore_topic(self, topic_event: TopicEvent) -&gt; None:\n    \"\"\"Asynchronously restore a topic from a topic event.\"\"\"\n    if isinstance(topic_event, PublishToTopicEvent) or isinstance(\n        topic_event, OutputTopicEvent\n    ):\n        await self.event_cache.put(topic_event)\n    elif isinstance(topic_event, ConsumeFromTopicEvent):\n        # Fetch the events for the consumer and commit the offset\n        await self.event_cache.fetch(\n            consumer_id=topic_event.consumer_name, offset=topic_event.offset + 1\n        )\n        await self.event_cache.commit_to(\n            topic_event.consumer_name, topic_event.offset\n        )\n</code></pre>"},{"location":"user-guide/topics/topic_base/#serialization","title":"Serialization","text":""},{"location":"user-guide/topics/topic_base/#topic-serialization","title":"Topic Serialization","text":"<pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    return {\"name\": self.name, \"condition\": self.serialize_callable()}\n</code></pre>"},{"location":"user-guide/topics/topic_base/#builder-pattern-usage","title":"Builder Pattern Usage","text":""},{"location":"user-guide/topics/topic_base/#basic-topic-creation","title":"Basic Topic Creation","text":"<pre><code>from grafi.topics.topic_base import TopicBaseBuilder\n\n# Create topic with builder\ntopic = (TopicBaseBuilder()\n    .name(\"processing_results\")\n    .condition(lambda msgs: len(msgs) &gt; 0)\n    .build())\n</code></pre>"},{"location":"user-guide/topics/topic_base/#validation","title":"Validation","text":"<p>The builder includes validation for reserved topic names:</p> <pre><code>def name(self, name: str) -&gt; Self:\n    if name in AGENT_RESERVED_TOPICS:\n        raise ValueError(f\"Topic name '{name}' is reserved for the agent.\")\n    self.kwargs[\"name\"] = name\n    return self\n</code></pre> <p>The topic base system provides the foundational structure for implementing topic-based messaging patterns in Graphite applications.</p>"},{"location":"user-guide/topics/topic_event_cache/","title":"Topic Event Cache","text":"<p>The TopicEventQueue is a sophisticated in-memory caching system that provides Kafka-like functionality for managing topic events, consumer offsets, and reliable message processing in Graphite workflows. It acts as a miniature message broker within each topic, supporting concurrent producers and consumers with proper offset management.</p>"},{"location":"user-guide/topics/topic_event_cache/#overview","title":"Overview","text":"<p>The TopicEventQueue implements:</p> <ul> <li>Event Storage: Contiguous log of topic events with offset-based indexing</li> <li>Consumer Tracking: Per-consumer offset management (consumed and committed)</li> <li>Async Operations: Full async/await support with condition variables</li> <li>Reliable Processing: Separate consumed/committed offsets prevent duplicate processing</li> <li>Backpressure: Built-in flow control and timeout handling</li> </ul>"},{"location":"user-guide/topics/topic_event_cache/#architecture","title":"Architecture","text":""},{"location":"user-guide/topics/topic_event_cache/#core-components","title":"Core Components","text":"<pre><code>class TopicEventQueue:\n    def __init__(self, name: str = \"\"):\n        self.name: str = name\n        self._records: List[TopicEvent] = []  # contiguous log\n\n        # Per-consumer cursors\n        self._consumed: Dict[str, int] = defaultdict(int)      # next offset to read\n        self._committed: Dict[str, int] = defaultdict(lambda: -1)  # last committed offset\n\n        # For asynchronous operations\n        self._cond: asyncio.Condition = asyncio.Condition()\n</code></pre>"},{"location":"user-guide/topics/topic_event_cache/#offset-management","title":"Offset Management","text":"<p>The cache maintains two types of offsets per consumer:</p> <ol> <li>Consumed Offset: Tracks the next message to fetch (advanced immediately on fetch)</li> <li>Committed Offset: Tracks messages that have been fully processed (advanced after processing)</li> </ol> <p>This dual-offset system prevents duplicate message processing in concurrent environments.</p>"},{"location":"user-guide/topics/topic_event_cache/#core-methods","title":"Core Methods","text":""},{"location":"user-guide/topics/topic_event_cache/#event-storage","title":"Event Storage","text":""},{"location":"user-guide/topics/topic_event_cache/#putevent-topicevent-topicevent","title":"put(event: TopicEvent) \u2192 TopicEvent","text":"<p>Synchronously append an event to the log.</p> <pre><code>def put(self, event: TopicEvent) -&gt; TopicEvent:\n    self._records.append(event)\n    return event\n</code></pre>"},{"location":"user-guide/topics/topic_event_cache/#async-putevent-topicevent-topicevent","title":"async put(event: TopicEvent) \u2192 TopicEvent","text":"<p>Asynchronously append an event and notify waiting consumers.</p> <pre><code>async def put(self, event: TopicEvent) -&gt; TopicEvent:\n    async with self._cond:\n        self._records.append(event)\n        self._cond.notify_all()  # wake waiting consumers\n        return event\n</code></pre>"},{"location":"user-guide/topics/topic_event_cache/#event-consumption","title":"Event Consumption","text":""},{"location":"user-guide/topics/topic_event_cache/#can_consumeconsumer_id-str-bool","title":"can_consume(consumer_id: str) \u2192 bool","text":"<p>Check if a consumer has unread messages.</p> <pre><code>def can_consume(self, consumer_id: str) -&gt; bool:\n    self._ensure_consumer(consumer_id)\n    # Can consume if there are records beyond the consumed offset\n    return self._consumed[consumer_id] &lt; len(self._records)\n</code></pre>"},{"location":"user-guide/topics/topic_event_cache/#fetchconsumer_id-str-offset-optionalint-none-listtopicevent","title":"fetch(consumer_id: str, offset: Optional[int] = None) \u2192 List[TopicEvent]","text":"<p>Synchronously fetch unread events and advance consumed offset.</p> <pre><code>def fetch(self, consumer_id: str, offset: Optional[int] = None) -&gt; List[TopicEvent]:\n    \"\"\"\n    Fetch records newer than the consumer's consumed offset.\n    Immediately advances consumed offset to prevent duplicate fetches.\n    \"\"\"\n    self._ensure_consumer(consumer_id)\n\n    if self.can_consume(consumer_id):\n        start = self._consumed[consumer_id]\n        if offset is not None:\n            end = min(len(self._records), offset + 1)\n            batch = self._records[start:end]\n        else:\n            batch = self._records[start:]\n\n        # Advance consumed offset immediately to prevent duplicate fetches\n        self._consumed[consumer_id] += len(batch)\n        return batch\n\n    return []\n</code></pre>"},{"location":"user-guide/topics/topic_event_cache/#async-fetchconsumer_id-str-offset-optionalint-none-timeout-optionalfloat-none-listtopicevent","title":"async fetch(consumer_id: str, offset: Optional[int] = None, timeout: Optional[float] = None) \u2192 List[TopicEvent]","text":"<p>Asynchronously fetch events with blocking and timeout support.</p> <pre><code>async def fetch(\n    self,\n    consumer_id: str,\n    offset: Optional[int] = None,\n    timeout: Optional[float] = None,\n) -&gt; List[TopicEvent]:\n    \"\"\"\n    Await fresh records newer than the consumer's consumed offset.\n    Immediately advances consumed offset to prevent duplicate fetches.\n    \"\"\"\n    self._ensure_consumer(consumer_id)\n\n    async with self._cond:\n        # Wait for data to become available\n        while not self.can_consume(consumer_id):\n            if timeout is None:\n                await self._cond.wait()\n            else:\n                try:\n                    await asyncio.wait_for(self._cond.wait(), timeout)\n                except asyncio.TimeoutError:\n                    return []\n\n        start = self._consumed[consumer_id]\n        if offset is not None:\n            end = min(len(self._records), offset + 1)\n            batch = self._records[start:end]\n        else:\n            batch = self._records[start:]\n\n        # Advance consumed offset immediately\n        self._consumed[consumer_id] += len(batch)\n        return batch\n</code></pre>"},{"location":"user-guide/topics/topic_event_cache/#offset-commitment","title":"Offset Commitment","text":""},{"location":"user-guide/topics/topic_event_cache/#commit_toconsumer_id-str-offset-int-int","title":"commit_to(consumer_id: str, offset: int) \u2192 int","text":"<p>Synchronously commit processed messages up to the specified offset.</p> <pre><code>def commit_to(self, consumer_id: str, offset: int) -&gt; int:\n    \"\"\"\n    Marks everything up to `offset` as processed/durable\n    for this consumer.\n    \"\"\"\n    self._ensure_consumer(consumer_id)\n    # Only commit if offset is greater than current committed\n    if offset &gt; self._committed[consumer_id]:\n        self._committed[consumer_id] = offset\n    return self._committed[consumer_id]\n</code></pre>"},{"location":"user-guide/topics/topic_event_cache/#async-commit_toconsumer_id-str-offset-int-none","title":"async commit_to(consumer_id: str, offset: int) \u2192 None","text":"<p>Asynchronously commit processed messages.</p> <pre><code>async def commit_to(self, consumer_id: str, offset: int) -&gt; None:\n    \"\"\"Commit all offsets up to and including the specified offset.\"\"\"\n    async with self._cond:\n        self._ensure_consumer(consumer_id)\n        if offset &gt; self._committed[consumer_id]:\n            self._committed[consumer_id] = offset\n</code></pre>"},{"location":"user-guide/topics/topic_event_cache/#usage-patterns","title":"Usage Patterns","text":""},{"location":"user-guide/topics/topic_event_cache/#basic-producer-consumer","title":"Basic Producer-Consumer","text":"<pre><code># Producer\ncache = TopicEventQueue(\"my_topic\")\nevent = PublishToTopicEvent(...)\ncache.put(event)\n\n# Consumer\nconsumer_id = \"consumer_1\"\nif cache.can_consume(consumer_id):\n    events = cache.fetch(consumer_id)\n\n    # Process events\n    for event in events:\n        process_event(event)\n\n    # Commit after successful processing\n    if events:\n        last_offset = events[-1].offset\n        cache.commit_to(consumer_id, last_offset)\n</code></pre>"},{"location":"user-guide/topics/topic_event_cache/#async-producer-consumer","title":"Async Producer-Consumer","text":"<pre><code>async def producer():\n    cache = TopicEventQueue(\"async_topic\")\n    for i in range(10):\n        event = create_event(i)\n        await cache.put(event)\n        await asyncio.sleep(0.1)\n\nasync def consumer():\n    cache = TopicEventQueue(\"async_topic\")\n    consumer_id = \"async_consumer\"\n\n    while True:\n        # Fetch with timeout\n        events = await cache.fetch(consumer_id, timeout=1.0)\n        if not events:\n            break  # Timeout occurred\n\n        # Process events\n        for event in events:\n            await process_event_async(event)\n\n        # Commit after processing\n        if events:\n            last_offset = events[-1].offset\n            await cache.commit_to(consumer_id, last_offset)\n</code></pre>"},{"location":"user-guide/topics/topic_event_cache/#multiple-consumers","title":"Multiple Consumers","text":"<pre><code>cache = TopicEventQueue(\"shared_topic\")\n\n# Each consumer tracks its own offsets\nconsumers = [\"consumer_1\", \"consumer_2\", \"consumer_3\"]\n\nfor consumer_id in consumers:\n    if cache.can_consume(consumer_id):\n        events = cache.fetch(consumer_id)\n        # Each consumer gets its own view of unprocessed events\n</code></pre>"},{"location":"user-guide/topics/topic_event_cache/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/topics/topic_event_cache/#offset-management_1","title":"Offset Management","text":"<ol> <li>Immediate Consumption Tracking: The consumed offset is advanced immediately on fetch to prevent duplicate fetches</li> <li>Commit After Processing: Only commit offsets after successful event processing</li> <li>Batch Commits: Commit the highest offset in a batch for efficiency</li> </ol>"},{"location":"user-guide/topics/topic_event_cache/#error-handling","title":"Error Handling","text":"<pre><code>async def robust_consumer():\n    cache = TopicEventQueue(\"robust_topic\")\n    consumer_id = \"robust_consumer\"\n\n    try:\n        events = await cache.fetch(consumer_id, timeout=5.0)\n        if not events:\n            return  # No events or timeout\n\n        processed_events = []\n        for event in events:\n            try:\n                await process_event(event)\n                processed_events.append(event)\n            except Exception as e:\n                logger.error(f\"Failed to process event {event.offset}: {e}\")\n                # Decide whether to skip or retry\n                break\n\n        # Only commit successfully processed events\n        if processed_events:\n            last_offset = processed_events[-1].offset\n            await cache.commit_to(consumer_id, last_offset)\n\n    except asyncio.TimeoutError:\n        logger.info(\"No events available within timeout\")\n    except Exception as e:\n        logger.error(f\"Consumer error: {e}\")\n</code></pre>"},{"location":"user-guide/topics/topic_event_cache/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Batch Processing: Process multiple events in batches when possible</li> <li>Timeout Management: Use appropriate timeouts to prevent indefinite blocking</li> <li>Memory Management: Monitor cache size for long-running topics</li> <li>Consumer Cleanup: Reset consumers that are no longer needed</li> </ol>"},{"location":"user-guide/topics/topic_event_cache/#testing","title":"Testing","text":"<pre><code>async def test_cache_behavior():\n    cache = TopicEventQueue(\"test_topic\")\n\n    # Test basic put/fetch\n    event = create_test_event()\n    await cache.put(event)\n\n    consumer_id = \"test_consumer\"\n    assert cache.can_consume(consumer_id)\n\n    events = await cache.fetch(consumer_id)\n    assert len(events) == 1\n    assert events[0] == event\n\n    # Test duplicate fetch prevention\n    events2 = await cache.fetch(consumer_id)\n    assert len(events2) == 0  # Should be empty due to consumed offset\n\n    # Test commit and reset\n    await cache.commit_to(consumer_id, 0)\n\n    cache.reset()\n    assert len(cache._records) == 0\n    assert cache._consumed[consumer_id] == 0\n    assert cache._committed[consumer_id] == -1\n</code></pre>"},{"location":"user-guide/topics/topic_event_cache/#integration-with-topics","title":"Integration with Topics","text":"<p>The TopicEventQueue is used internally by all TopicBase implementations:</p> <pre><code>class TopicBase(BaseModel):\n    event_cache: TopicEventQueue = Field(default_factory=TopicEventQueue)\n\n    def can_consume(self, consumer_name: str) -&gt; bool:\n        return self.event_cache.can_consume(consumer_name)\n\n    async def consume(self, consumer_name: str, timeout: Optional[float] = None) -&gt; List[TopicEvent]:\n        return await self.event_cache.fetch(consumer_name, timeout=timeout)\n\n    async def commit(self, consumer_name: str, offset: int) -&gt; None:\n        await self.event_cache.commit_to(consumer_name, offset)\n</code></pre> <p>This provides a consistent, reliable messaging substrate for all topic types in Graphite workflows, ensuring proper event ordering, delivery guarantees, and offset management across the entire system.</p>"}]}